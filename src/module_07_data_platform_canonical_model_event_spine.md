# Module 7: Data Platform, Canonical Data Model, and Event Spine

> **Disclaimer:** This module is operational guidance for learning purposes. Technology choices are illustrative — evaluate against your specific infrastructure, scale, and organizational maturity before adopting. This is not legal advice regarding data privacy or compliance. Validate all GDPR, data residency, and retention requirements with qualified counsel.

---

## Module Summary

Modules 1 through 6 taught you the operational domain — how workers get hired, paid, filed for, and terminated across dozens of countries. This module is where **your BI and lakehouse expertise becomes the connective tissue** that turns all of that operational knowledge into a unified, trustworthy data platform.

The central challenge: payroll data is generated by dozens of systems, in dozens of countries, in dozens of formats, with different schemas, different update frequencies, and different quality levels. A worker named "Priya Sharma" in the platform might be "SHARMA, PRIYA" in the Indian payroll engine and "P. Sharma" in the benefits system. A "payroll run" in Germany contains church tax and solidarity surcharge fields that simply do not exist in Singapore. A "termination" in Brazil triggers FGTS penalty calculations with no equivalent anywhere else. And every single one of these data points must be correct, auditable, and available for analytics — because this data directly determines whether people get paid the right amount on the right day.

**Why payroll data is harder than typical SaaS data:**

- **Correctness is non-negotiable.** A 0.1% error rate in e-commerce recommendations is invisible. A 0.1% error rate in payroll means hundreds of people received the wrong pay.
- **Schema diversity is structural, not cosmetic.** India's CTC breakdown (Basic + HRA + Special Allowance) and the UK's single gross salary are fundamentally different data structures representing the same concept.
- **Temporal precision matters.** "What was this worker's salary on March 15?" is not a nice-to-have question — it determines whether a payroll run was calculated correctly.
- **Multi-currency is unavoidable.** Every monetary amount exists in at least two currencies (local and reporting currency), and the exchange rate used matters for reconciliation.
- **Regulatory audit trails are mandatory.** You cannot delete data that a tax authority might request. Retention periods vary by country — 7 years in Germany, 10 years in France, 8 years in India.
- **PII density is extreme.** Every record contains names, tax IDs, bank accounts, salaries, and addresses — the most sensitive categories of personal data.

By the end of this module, you will understand how to architect a data platform that handles all of this complexity, model every core entity with production-grade schemas, build an event spine that captures every operational moment, and deliver analytics that your ops, finance, compliance, and executive stakeholders can trust.

---

## Topic 1: Data Platform Architecture Overview — Bronze/Silver/Gold Medallion for Payroll

### What It Is

The data platform architecture is the blueprint for how raw data from dozens of source systems gets ingested, cleaned, standardized, enriched, and served to analytics consumers. In a global payroll context, this platform must handle data from the core EOR platform, dozens of local payroll engines, banking and treasury systems, benefits providers, government filing portals, and client HRIS systems — each with its own format, latency, and quality characteristics.

The **medallion architecture** (Bronze, Silver, Gold) provides a clear layer model that separates concerns: raw data preservation, standardization and quality enforcement, and business-ready analytics. This is not a new concept if you come from the lakehouse world — but applying it to payroll data introduces specific challenges around multi-country schema diversity, PII handling at every layer, and the audit requirements that do not exist in most SaaS analytics platforms.

### Why It Matters

Without a structured data platform, analytics teams spend 80% of their time answering the question "Is this data correct?" instead of "What does this data tell us?" Every global payroll company that scales past a few thousand workers hits a wall where spreadsheet-based reporting and direct-query dashboards break down. The data platform is what gets you past that wall — and as an analytics leader, building it is likely the single highest-leverage thing you will do in your first year.

**Business impact:**
- Payroll ops teams making decisions on stale or inconsistent data leads to errors that cost $50-$500 per incident to remediate
- Finance cannot close monthly books when payroll data from partner countries arrives late or in inconsistent formats
- Compliance teams cannot prove filing accuracy to auditors without a reliable data trail
- Executive dashboards that show conflicting numbers destroy trust in the analytics function permanently

### Process Flow

```
SOURCE SYSTEMS                    INGESTION LAYER                   LAKEHOUSE / DATA PLATFORM
─────────────────────────────────────────────────────────────────────────────────────────────

┌────────────────┐   CDC Stream    ┌──────────────┐    ┌─────────────────────────────────┐
│ Platform Core  │ ═══════════════►│              │    │  BRONZE LAYER                   │
│ (EOR app DB)   │                 │              │    │  ┌───────────────────────────┐   │
└────────────────┘                 │              │    │  │ raw_platform_workers      │   │
                                   │              │    │  │ raw_platform_contracts    │   │
┌────────────────┐   API Polling   │   Ingestion  │    │  │ raw_de_payroll_runs       │   │
│ Local Payroll  │ ═══════════════►│   Engine     │══► │  │ raw_in_payroll_runs       │   │
│ Engines (DE,   │                 │              │    │  │ raw_uk_payroll_runs       │   │
│  IN, UK, BR)   │   Batch/SFTP   │  (Spark      │    │  │ raw_payment_transactions  │   │
│                │ ═══════════════►│   Streaming  │    │  │ raw_filing_statuses       │   │
└────────────────┘                 │   + Batch)   │    │  │ raw_benefits_enrollments  │   │
                                   │              │    │  │ raw_zendesk_tickets       │   │
┌────────────────┐   Webhook/API   │              │    │  └───────────────────────────┘   │
│ Client HRIS    │ ═══════════════►│              │    │         │                        │
│ (Workday,      │                 │              │    │         ▼  DQ checks + conform   │
│  BambooHR)     │                 │              │    │  SILVER LAYER                    │
└────────────────┘                 │              │    │  ┌───────────────────────────┐   │
                                   │              │    │  │ worker (canonical SCD2)   │   │
┌────────────────┐   API + Batch   │              │    │  │ contract (canonical)      │   │
│ Banking /      │ ═══════════════►│              │    │  │ payroll_run (canonical)   │   │
│ Treasury       │                 │              │    │  │ payslip (canonical)       │   │
└────────────────┘                 │              │    │  │ pay_item (canonical)      │   │
                                   │              │    │  │ payment (canonical)       │   │
┌────────────────┐   Event Stream  │              │    │  │ statutory_filing          │   │
│ Internal       │ ═══════════════►│              │    │  │ fx_rate (canonical)       │   │
│ Microservices  │                 └──────────────┘    │  └───────────────────────────┘   │
└────────────────┘                                     │         │                        │
                                                       │         ▼  Aggregate + enrich    │
┌────────────────┐   Batch Scrape                      │  GOLD LAYER                     │
│ Gov't Filing   │ ═══════════════►  (via ingestion)   │  ┌───────────────────────────┐   │
│ Portals        │                                     │  │ payroll_run_summary       │   │
└────────────────┘                                     │  │ country_monthly_metrics   │   │
                                                       │  │ client_health_scorecard   │   │
┌────────────────┐   API Polling                       │  │ compliance_risk_scores    │   │
│ Support /      │ ═══════════════►  (via ingestion)   │  │ treasury_cash_position    │   │
│ Ticketing      │                                     │  │ worker_cost_analysis      │   │
└────────────────┘                                     │  │ feature_store (ML)        │   │
                                                       │  └───────────────────────────┘   │
                                                       └─────────────────────────────────┘
                                                                    │
                                                       ┌────────────┴────────────────────┐
                                                       │  SEMANTIC LAYER                  │
                                                       │  Metric definitions, dimension   │
                                                       │  models, access policies, SLOs   │
                                                       └────────────┬────────────────────┘
                                                                    │
                                                       ┌────────────┴────────────────────┐
                                                       │  PRESENTATION LAYER              │
                                                       │  Dashboards, Self-service BI,    │
                                                       │  AI/ML Apps, Client Reporting,   │
                                                       │  API for downstream systems      │
                                                       └─────────────────────────────────┘
```

### The Medallion Layers in Detail

| Layer | Purpose | Data Characteristics | Payroll Example |
|-------|---------|---------------------|-----------------|
| **Bronze** | Raw data, as-received, immutable. The system of record for "what did the source actually send us?" | Original format, original schema, original field names. Append-only. Partitioned by source and ingestion date. | Raw CSV export from German payroll engine: columns like `Personalnummer`, `Bruttolohn`, `Kirchensteuer`, `Nettolohn` in German field names |
| **Silver** | Cleaned, standardized, deduplicated. Mapped to the canonical data model. DQ rules applied and scored. | Canonical field names, consistent types, SCD2 history for dimensions, null handling, dedup complete. PII tagged. | Worker record: `worker_id`, `full_name`, `country_code` (ISO 3166), `status`, `annual_salary`, `salary_currency`, `annual_salary_usd`, `valid_from`, `valid_to` |
| **Gold** | Aggregated, business-ready. Optimized for specific analytics use cases, dashboards, and ML features. | Pre-aggregated, denormalized for query performance. Business logic applied (e.g., error classification). | Monthly payroll summary: `country`, `period`, `total_gross_local`, `total_gross_usd`, `headcount`, `error_count`, `on_time_pay_rate`, `avg_processing_days` |

### Maturity Stages — What the Platform Looks Like at Different Scales

| Dimension | 500 Workers (Startup) | 5,000 Workers (Growth) | 50,000 Workers (Scale) |
|-----------|----------------------|----------------------|----------------------|
| **Source systems** | 5-10 (platform DB, 3-5 payroll engines, 1 banking partner) | 20-30 (platform DB, 15+ engines, multiple banking partners, benefits systems) | 50-100+ (full ecosystem including client HRIS integrations) |
| **Data volume** | ~50K rows/month across all tables | ~2M rows/month | ~50M+ rows/month |
| **Ingestion** | Mostly batch (daily SFTP/API pulls); maybe CDC on platform DB | CDC on platform DB, scheduled API polling, some streaming | Full CDC + streaming for real-time; batch only for legacy partners |
| **Bronze layer** | Single schema per source, stored in cloud storage (S3/GCS) | Partitioned by source + date, Delta/Iceberg format | Partitioned, compacted, lifecycle-managed with retention policies |
| **Silver layer** | Basic canonical mapping, manual quality checks | Full canonical model, automated DQ rules, SCD2 for key entities | Complete canonical model, automated DQ with anomaly detection, full SCD2 |
| **Gold layer** | 3-5 summary tables, manually maintained | 15-20 curated tables + feature store | 50+ data products with SLAs, self-service catalog |
| **Semantic layer** | Definitions in a wiki (or in people's heads) | Metric definitions in dbt or a metrics layer tool | Full semantic layer with governed metric store |
| **Team** | 1-2 data engineers + 1 analyst | 5-8 data engineers, 2-3 analysts, 1 analytics engineer | 15-25+ across data eng, analytics eng, ML eng, data governance |

### Data Artifacts

| Entity | Key Fields | Analytics Enabled |
|--------|-----------|-------------------|
| Source system registry | source_id, source_name, ingestion_pattern, latency_sla, owner, status | Pipeline monitoring, SLA tracking |
| Pipeline run log | pipeline_id, source_id, run_timestamp, rows_ingested, rows_failed, duration_seconds, status | Ingestion reliability metrics |
| Layer transition log | record_id, bronze_timestamp, silver_timestamp, gold_timestamp, dq_score | End-to-end data latency tracking |
| Data quality score card | table_name, check_name, dimension, pass_rate, last_run, severity | DQ dashboard, trend analysis |
| PII inventory | table_name, column_name, pii_category, masking_method, retention_days | Privacy compliance, audit readiness |

### Controls

| Control | Type | Description |
|---------|------|-------------|
| **Immutable bronze** | Preventive | Bronze tables are append-only; no updates or deletes allowed |
| **Schema validation on ingest** | Preventive | Every incoming file/record is validated against expected schema before landing in bronze |
| **DQ gates between layers** | Preventive | Data does not promote from bronze to silver without passing completeness and format checks |
| **Row count reconciliation** | Detective | Source row count vs. bronze row count vs. silver row count — alerts on discrepancies |
| **Freshness monitoring** | Detective | Each pipeline has a freshness SLA; alerts fire when data is stale |
| **PII access logging** | Detective | All queries accessing PII columns are logged with user, timestamp, and query text |
| **Backfill approval workflow** | Preventive | Any retroactive data load requires approval from data engineering lead and domain owner |

### Metrics

| Metric | Definition | Target | Frequency | Owner |
|--------|-----------|--------|-----------|-------|
| Pipeline success rate | % of scheduled pipeline runs that complete without error | > 99.5% | Daily | Data Engineering |
| Data freshness SLA adherence | % of pipelines delivering data within their defined SLA window | > 98% | Daily | Data Engineering |
| Bronze-to-silver latency (p95) | 95th percentile time from bronze landing to silver table availability | < 30 min (CDC), < 4 hr (batch) | Daily | Data Engineering |
| Silver-to-gold latency (p95) | 95th percentile time from silver availability to gold table refresh | < 2 hours | Daily | Analytics Engineering |
| Data quality pass rate (overall) | % of all DQ checks passing across all silver/gold tables | > 99% | Daily | Data Engineering |
| Schema drift incidents | Count of source systems that changed schema without notice, breaking ingestion | 0 | Weekly | Data Engineering |
| Storage cost per worker per month | Total data platform storage cost / active worker count | Track trend | Monthly | Data Engineering |
| Compute cost per pipeline run | Average compute cost for each scheduled pipeline execution | Track trend | Monthly | Data Engineering |
| PII access audit coverage | % of PII-containing tables with access logging enabled | 100% | Weekly | Data Governance |
| Data product SLA adherence | % of published data products meeting their refresh and quality SLAs | > 99% | Daily | Analytics Engineering |
| Backfill frequency | Number of retroactive data loads required per month | < 5 | Monthly | Data Engineering |
| Source system coverage | % of operational source systems integrated into the data platform | Track growth toward 100% | Quarterly | Data Engineering |

### Common Failure Modes

| Failure | Consequence | Real-World Example |
|---------|------------|--------------------|
| **Schema change in source without notification** | Bronze ingestion breaks; silver tables stale; dashboards show yesterday's data | German payroll engine vendor upgrades their export format, renaming `Bruttolohn` to `Brutto_Gehalt`. Pipeline fails silently because the column mapping is hard-coded. |
| **Bronze table grows unbounded** | Storage costs explode; queries against bronze become slow; backfill jobs OOM | Raw event table grows to 500M rows because nobody configured lifecycle policies. A single backfill query costs $200 in compute. |
| **DQ rules too strict on launch** | Every pipeline run triggers alerts; team ignores alerts; real issues get missed | Freshness SLA set to 1 hour for a partner payroll engine that realistically delivers data every 6 hours. The alert fires every day and nobody investigates. |
| **No SCD2 on worker table** | Cannot answer "what was this worker's salary during March payroll?" — the current salary overwrites history | Auditor asks for proof that a March payroll run used the correct salary. The silver table only has the current (April) salary. No audit trail. |
| **PII leaks to gold layer** | Individual salaries or tax IDs visible in dashboards accessible to unauthorized users | Gold table `payroll_run_summary` includes a `worker_details` JSON column that was supposed to be removed during aggregation. An intern discovers individual salaries. |

### AI Opportunities

| Opportunity | Inputs | Outputs | Guardrails |
|------------|--------|---------|------------|
| **Automated schema mapping** | New source system's raw schema + existing canonical model | Suggested column mappings (source field → canonical field) with confidence scores | Human review required for any mapping below 90% confidence; never auto-promote to silver without approval |
| **Anomaly detection on pipeline metrics** | Pipeline run history (duration, row counts, error rates) | Alerts for anomalous runs (e.g., row count dropped 50% vs. same day last month) | Suppress alerts during known events (country holidays, planned maintenance); require human ack before auto-remediation |
| **Intelligent data quality rules** | Historical data patterns for each column | Auto-suggested DQ rules (e.g., "salary_usd is always between 500 and 500,000 for this country") | All auto-generated rules go through human review; never block production pipelines on untested rules |
| **Cost optimization** | Pipeline compute/storage metrics, query patterns | Recommendations for table partitioning, compaction schedules, cold storage migration | Recommendations only; no auto-implementation; cost savings estimates must be validated |

### Discovery Questions

1. "How many source systems feed into our analytics today, and how many are still only accessible via spreadsheet exports or manual downloads?"
2. "When was the last time a dashboard showed wrong numbers because of a data pipeline issue? What happened, and how long did it take to detect?"
3. "Do we have a formal data freshness SLA for each source system, or does the team just 'know' when data should arrive?"
4. "What's our biggest bottleneck: getting data in (ingestion), cleaning it (transformation), or serving it (query performance)?"
5. "Has anyone mapped PII across all our data assets? Do we know which tables contain tax IDs, bank accounts, and salaries?"

### Exercises

1. **Architecture assessment exercise:** Draw the current-state architecture for your data platform (or a hypothetical one based on what you have learned). Identify: which source systems exist, what ingestion patterns are used, whether a medallion architecture is in place, and where the biggest gaps are. Then draw the target-state architecture for 12 months from now. Present both diagrams to a peer and explain the migration path.

2. **Cost model exercise:** Estimate the monthly data platform cost for an EOR with 10,000 workers across 25 countries. Assume: 5 source systems per country, average 2,000 rows per source per month, 3 layers of storage (bronze retained 7 years, silver retained 3 years, gold retained 1 year). Price this on AWS (S3 + Glue + Athena) and Databricks. Which cost drivers dominate?

3. **Analytics-leader exercise — Platform roadmap:** Write a one-page "Data Platform Roadmap" that you would present to the VP of Engineering in your first 60 days. It should include: current state assessment (with gaps), proposed target architecture, 3 phases of implementation (each 90 days), resource requirements, and the business outcomes each phase unlocks. Use the maturity stages table to anchor your phases.

---

## Topic 2: Canonical Data Model — Full Entity Definitions

### What It Is

A canonical data model is the **single, agreed-upon schema** for representing every core business entity across all systems and countries. Without it, "worker" means different things in different systems, "payroll run" has different fields in different countries, and cross-country analytics is impossible. The canonical model is not a physical database — it is an abstraction that every source system maps into during the bronze-to-silver transformation.

In a global payroll context, the canonical model must solve several problems that do not exist in typical SaaS data modeling:
- **Structural diversity:** India's salary has 5+ components; the UK has 1. Both must map to a unified `pay_item` model.
- **Country-specific fields:** Germany requires `tax_class` and `church_tax_indicator`. India requires `pan_number` and `pf_uan`. These fields exist in some countries and are meaningless in others.
- **Temporal precision:** Every entity that affects payroll calculations must support point-in-time queries via SCD Type 2 versioning.
- **Multi-currency:** Every monetary amount must be stored in both local currency and a reporting currency (typically USD), with the exchange rate recorded.

### Why It Matters

Without a canonical model, your analytics team will spend its time doing ad-hoc joins between inconsistently named and typed fields from different source systems. Every new dashboard requires re-discovering what each source system calls the same concept. When a stakeholder asks "How many active workers do we have globally?", three analysts will produce three different numbers because they each queried a different source with a different definition of "active."

**Business impact:**
- Canonical model enables single-query global reporting (headcount, payroll cost, error rates) across all countries
- Schema enforcement catches data issues at ingestion time, not at dashboard-render time
- Point-in-time capability (SCD2) is required for payroll audit compliance — regulators can request proof of calculations months or years after the fact
- Every AI/ML model you build depends on consistently structured training data

### Process Flow — Entity Relationships

```
                              ┌──────────────┐
                              │    Client     │
                              │  client_id    │
                              │  name         │
                              │  segment      │
                              │  tier         │
                              └──────┬───────┘
                                     │ 1:N
                              ┌──────▼───────┐          ┌──────────────┐
                              │   Worker      │          │   LegalEntity│
                              │  worker_id    │          │  entity_id   │
                              │  client_id    │          │  country     │
                              │  full_name    │          │  type        │
                              │  country_code │          │  legal_name  │
                              │  status       │          └──────┬───────┘
                              └──┬────┬───┬──┘                 │
                        1:N ┌───┘    │   └───┐ 1:N             │
                   ┌────────▼──┐     │  ┌────▼──────────┐      │
                   │ Contract   │     │  │  TimeOff      │      │
                   │contract_id │     │  │ timeoff_id    │      │
                   │entity_id───┼─────┼──┤ worker_id     │      │
                   │start_date  │     │  │ type          │      │
                   │end_date    │     │  │ start_date    │      │
                   │salary      │     │  │ end_date      │      │
                   │currency    │     │  │ days          │      │
                   └─────┬──────┘     │  └───────────────┘      │
                         │ 1:N        │                         │
                   ┌─────▼──────┐     │  1:N                    │
                   │PayrollRun  │     │  ┌───────────────┐      │
                   │ run_id     │     │  │ Benefit       │      │
                   │ entity_id──┼─────┼──┤ benefit_id    │      │
                   │ country    │     │  │ worker_id     │      │
                   │ period     │     └──┤ type          │      │
                   │ status     │        │ provider      │      │
                   │ pay_date   │        │ cost_employee │      │
                   └─────┬──────┘        │ cost_employer │      │
                         │ 1:N           └───────────────┘      │
                   ┌─────▼──────┐                               │
                   │  Payslip   │     ┌───────────────┐         │
                   │ payslip_id │     │StatutoryFiling│         │
                   │ run_id     │     │ filing_id     │         │
                   │ worker_id  │     │ entity_id─────┼─────────┘
                   │ gross      │     │ country       │
                   │ net        │     │ filing_type   │
                   └──┬────┬────┘     │ period        │
                 1:N ┌┘    └┐ 1:1    │ due_date      │
           ┌────────▼┐  ┌──▼──────┐  │ status        │
           │ PayItem  │  │Payment  │  └───────────────┘
           │payitem_id│  │payment_id│
           │category  │  │amount    │  ┌───────────────┐
           │type      │  │currency  │  │  FXRate       │
           │amount    │  │status    │  │ rate_id       │
           │currency  │  │rail      │  │ from_currency │
           └─────────┘  └─────────┘  │ to_currency   │
                                      │ rate          │
           ┌──────────┐               │ rate_date     │
           │Adjustment│               └───────────────┘
           │adjust_id │
           │payslip_id│  ┌───────────────┐
           │reason    │  │ Termination   │
           │amount    │  │ term_id       │
           │status    │  │ worker_id     │
           └──────────┘  │ reason        │
                         │ last_work_day │
                         │ severance_amt │
                         └───────────────┘
```

### DDL-Style Schema Definitions

Below are the canonical entity schemas. These are logical schemas — the physical implementation may vary (Delta Lake, Iceberg, Hive) but the fields and types are prescriptive.

**Worker**
```sql
CREATE TABLE silver.worker (
    worker_id           STRING        NOT NULL,   -- Platform-generated UUID (wrk_xxxxx)
    client_id           STRING        NOT NULL,   -- FK to client
    full_name           STRING        NOT NULL,
    first_name          STRING,
    last_name           STRING,
    email               STRING,
    country_code        STRING(2)     NOT NULL,   -- ISO 3166-1 alpha-2
    nationality         STRING(2),
    date_of_birth       DATE,
    gender              STRING,                    -- M/F/Other/Prefer not to say
    worker_type         STRING        NOT NULL,   -- 'eor_employee', 'cor_contractor', 'managed_payroll'
    status              STRING        NOT NULL,   -- 'onboarding','active','suspended','terminated','offboarded'
    start_date          DATE          NOT NULL,
    end_date            DATE,
    -- Country-specific identifiers (nullable — populated per country)
    tax_id              STRING,                    -- India: PAN, Germany: Steuer-IdNr, UK: UTR
    social_id           STRING,                    -- India: Aadhaar, Germany: SV-Nummer, UK: NINO
    pf_id               STRING,                    -- India: PF UAN (null for non-India)
    -- SCD2 tracking
    valid_from          TIMESTAMP     NOT NULL,
    valid_to            TIMESTAMP     NOT NULL,   -- '9999-12-31' for current record
    is_current          BOOLEAN       NOT NULL,
    -- Metadata
    source_system       STRING        NOT NULL,
    ingested_at         TIMESTAMP     NOT NULL,
    updated_at          TIMESTAMP     NOT NULL
);
```

**Contract**
```sql
CREATE TABLE silver.contract (
    contract_id         STRING        NOT NULL,
    worker_id           STRING        NOT NULL,   -- FK to worker
    entity_id           STRING        NOT NULL,   -- FK to legal_entity (employing entity)
    contract_type       STRING        NOT NULL,   -- 'permanent','fixed_term','part_time','internship'
    start_date          DATE          NOT NULL,
    end_date            DATE,                      -- NULL for permanent contracts
    probation_end_date  DATE,
    notice_period_days  INTEGER,
    annual_salary       DECIMAL(18,2) NOT NULL,
    salary_currency     STRING(3)     NOT NULL,   -- ISO 4217
    annual_salary_usd   DECIMAL(18,2),            -- Converted at month-end reference rate
    pay_frequency       STRING        NOT NULL,   -- 'monthly','semi_monthly','bi_weekly','weekly'
    working_hours_week  DECIMAL(4,1),             -- e.g., 40.0, 35.0 (France)
    job_title           STRING,
    department          STRING,
    -- Country-specific fields
    tax_class           STRING,                    -- Germany: Steuerklasse I-VI
    church_tax          BOOLEAN,                   -- Germany: Kirchensteuer applicable
    pf_opt_in           BOOLEAN,                   -- India: opted for higher PF contribution
    tax_regime          STRING,                    -- India: 'old_regime','new_regime'
    ni_category         STRING,                    -- UK: NI category letter (A, B, C, etc.)
    -- SCD2 tracking
    valid_from          TIMESTAMP     NOT NULL,
    valid_to            TIMESTAMP     NOT NULL,
    is_current          BOOLEAN       NOT NULL,
    source_system       STRING        NOT NULL,
    ingested_at         TIMESTAMP     NOT NULL
);
```

**Employment (linking Worker to Legal Entity with employment terms)**
```sql
CREATE TABLE silver.employment (
    employment_id       STRING        NOT NULL,
    worker_id           STRING        NOT NULL,
    contract_id         STRING        NOT NULL,
    entity_id           STRING        NOT NULL,
    country_code        STRING(2)     NOT NULL,
    employment_type     STRING        NOT NULL,   -- 'eor','cor','managed_payroll','direct'
    status              STRING        NOT NULL,   -- 'active','on_leave','suspended','terminated'
    effective_date      DATE          NOT NULL,
    end_date            DATE,
    cost_center         STRING,
    manager_worker_id   STRING,                    -- FK to worker (manager)
    valid_from          TIMESTAMP     NOT NULL,
    valid_to            TIMESTAMP     NOT NULL,
    is_current          BOOLEAN       NOT NULL,
    source_system       STRING        NOT NULL,
    ingested_at         TIMESTAMP     NOT NULL
);
```

**PayrollRun**
```sql
CREATE TABLE silver.payroll_run (
    run_id              STRING        NOT NULL,
    entity_id           STRING        NOT NULL,
    country_code        STRING(2)     NOT NULL,
    period_start        DATE          NOT NULL,   -- e.g., 2026-03-01
    period_end          DATE          NOT NULL,   -- e.g., 2026-03-31
    pay_date            DATE          NOT NULL,   -- e.g., 2026-03-28
    run_type            STRING        NOT NULL,   -- 'regular','off_cycle','correction','13th_month'
    status              STRING        NOT NULL,   -- 'draft','inputs_locked','processing','review',
                                                  --  'approved','locked','paid','closed'
    headcount           INTEGER       NOT NULL,
    total_gross_local   DECIMAL(18,2),
    total_net_local     DECIMAL(18,2),
    total_employer_cost DECIMAL(18,2),
    currency            STRING(3)     NOT NULL,
    total_gross_usd     DECIMAL(18,2),
    total_net_usd       DECIMAL(18,2),
    error_count         INTEGER       DEFAULT 0,
    created_at          TIMESTAMP     NOT NULL,
    locked_at           TIMESTAMP,
    paid_at             TIMESTAMP,
    source_system       STRING        NOT NULL,
    ingested_at         TIMESTAMP     NOT NULL
);
```

**Payslip**
```sql
CREATE TABLE silver.payslip (
    payslip_id          STRING        NOT NULL,
    run_id              STRING        NOT NULL,   -- FK to payroll_run
    worker_id           STRING        NOT NULL,   -- FK to worker
    period_start        DATE          NOT NULL,
    period_end          DATE          NOT NULL,
    gross_pay           DECIMAL(18,2) NOT NULL,
    total_deductions    DECIMAL(18,2) NOT NULL,
    net_pay             DECIMAL(18,2) NOT NULL,
    employer_cost       DECIMAL(18,2) NOT NULL,
    currency            STRING(3)     NOT NULL,
    gross_pay_usd       DECIMAL(18,2),
    net_pay_usd         DECIMAL(18,2),
    has_error           BOOLEAN       DEFAULT FALSE,
    error_details       STRING,                    -- JSON array of error codes
    source_system       STRING        NOT NULL,
    ingested_at         TIMESTAMP     NOT NULL
);
```

**PayItem (line items within a payslip)**
```sql
CREATE TABLE silver.pay_item (
    payitem_id          STRING        NOT NULL,
    payslip_id          STRING        NOT NULL,   -- FK to payslip
    category            STRING        NOT NULL,   -- 'earning','deduction','employer_cost'
    subcategory         STRING        NOT NULL,   -- 'base_salary','overtime','bonus','income_tax',
                                                  --  'social_security','pension','health_insurance',
                                                  --  'church_tax','professional_tax','pf_employee',
                                                  --  'pf_employer','esi','gratuity','ni_employee',
                                                  --  'ni_employer','apprenticeship_levy', etc.
    description         STRING        NOT NULL,   -- Human-readable: "Income Tax (PAYE)"
    amount              DECIMAL(18,2) NOT NULL,   -- Positive for earnings, negative for deductions
    currency            STRING(3)     NOT NULL,
    amount_usd          DECIMAL(18,2),
    is_taxable          BOOLEAN,
    is_statutory        BOOLEAN       NOT NULL,   -- TRUE for government-mandated items
    country_code        STRING(2)     NOT NULL,
    source_system       STRING        NOT NULL,
    ingested_at         TIMESTAMP     NOT NULL
);
```

**Deduction (a view/subset of PayItem, but sometimes tracked separately for reporting)**
```sql
CREATE TABLE silver.deduction (
    deduction_id        STRING        NOT NULL,
    payslip_id          STRING        NOT NULL,
    worker_id           STRING        NOT NULL,
    deduction_type      STRING        NOT NULL,   -- 'income_tax','social_security','pension',
                                                  --  'health_insurance','union_dues','garnishment',
                                                  --  'loan_repayment','voluntary_pension'
    statutory_flag      BOOLEAN       NOT NULL,
    amount              DECIMAL(18,2) NOT NULL,
    currency            STRING(3)     NOT NULL,
    amount_usd          DECIMAL(18,2),
    authority_name      STRING,                    -- e.g., 'HMRC', 'Finanzamt', 'EPFO'
    country_code        STRING(2)     NOT NULL,
    period_start        DATE          NOT NULL,
    period_end          DATE          NOT NULL,
    source_system       STRING        NOT NULL,
    ingested_at         TIMESTAMP     NOT NULL
);
```

**EmployerCost**
```sql
CREATE TABLE silver.employer_cost (
    cost_id             STRING        NOT NULL,
    payslip_id          STRING        NOT NULL,
    worker_id           STRING        NOT NULL,
    cost_type           STRING        NOT NULL,   -- 'social_security_employer','pension_employer',
                                                  --  'health_insurance_employer','workers_comp',
                                                  --  'payroll_tax','apprenticeship_levy',
                                                  --  'severance_provision','gratuity_provision'
    amount              DECIMAL(18,2) NOT NULL,
    currency            STRING(3)     NOT NULL,
    amount_usd          DECIMAL(18,2),
    is_statutory        BOOLEAN       NOT NULL,
    authority_name      STRING,
    country_code        STRING(2)     NOT NULL,
    period_start        DATE          NOT NULL,
    period_end          DATE          NOT NULL,
    source_system       STRING        NOT NULL,
    ingested_at         TIMESTAMP     NOT NULL
);
```

**StatutoryFiling**
```sql
CREATE TABLE silver.statutory_filing (
    filing_id           STRING        NOT NULL,
    entity_id           STRING        NOT NULL,
    country_code        STRING(2)     NOT NULL,
    filing_type         STRING        NOT NULL,   -- 'income_tax_withholding','social_security',
                                                  --  'pension_contribution','annual_return',
                                                  --  'pf_challan','rti_submission','lohnsteuer'
    period_start        DATE          NOT NULL,
    period_end          DATE          NOT NULL,
    due_date            DATE          NOT NULL,
    submitted_date      DATE,
    accepted_date       DATE,
    status              STRING        NOT NULL,   -- 'pending','submitted','accepted','rejected',
                                                  --  'corrected','overdue'
    amount              DECIMAL(18,2),
    currency            STRING(3),
    authority_name      STRING        NOT NULL,   -- 'HMRC','Finanzamt Berlin','EPFO','Receita Federal'
    reference_number    STRING,
    rejection_reason    STRING,
    source_system       STRING        NOT NULL,
    ingested_at         TIMESTAMP     NOT NULL
);
```

**Payment**
```sql
CREATE TABLE silver.payment (
    payment_id          STRING        NOT NULL,
    payslip_id          STRING,                    -- FK to payslip (NULL for non-payslip payments)
    worker_id           STRING        NOT NULL,
    payment_type        STRING        NOT NULL,   -- 'net_salary','expense_reimbursement',
                                                  --  'bonus','severance','off_cycle'
    amount              DECIMAL(18,2) NOT NULL,
    currency            STRING(3)     NOT NULL,
    amount_usd          DECIMAL(18,2),
    payment_rail        STRING        NOT NULL,   -- 'bank_transfer','bacs','sepa','swift',
                                                  --  'ach','upi','pix','faster_payments'
    status              STRING        NOT NULL,   -- 'initiated','submitted','settled',
                                                  --  'failed','reversed','retried'
    initiated_at        TIMESTAMP,
    submitted_at        TIMESTAMP,
    settled_at          TIMESTAMP,
    failure_reason      STRING,
    bank_reference      STRING,
    source_system       STRING        NOT NULL,
    ingested_at         TIMESTAMP     NOT NULL
);
```

**FXRate**
```sql
CREATE TABLE silver.fx_rate (
    rate_id             STRING        NOT NULL,
    from_currency       STRING(3)     NOT NULL,   -- ISO 4217
    to_currency         STRING(3)     NOT NULL,
    rate                DECIMAL(18,8) NOT NULL,   -- 1 from_currency = rate * to_currency
    rate_type           STRING        NOT NULL,   -- 'mid_market','platform_rate','client_rate'
    rate_date           DATE          NOT NULL,
    rate_timestamp      TIMESTAMP,
    source              STRING        NOT NULL,   -- 'reuters','xe','internal'
    ingested_at         TIMESTAMP     NOT NULL
);
```

**Benefit**
```sql
CREATE TABLE silver.benefit (
    benefit_id          STRING        NOT NULL,
    worker_id           STRING        NOT NULL,
    benefit_type        STRING        NOT NULL,   -- 'health_insurance','life_insurance','dental',
                                                  --  'vision','pension_supplementary','meal_voucher',
                                                  --  'transport_voucher','gym','stock_options'
    provider_name       STRING,
    plan_name           STRING,
    coverage_level      STRING,                    -- 'employee_only','employee_spouse','family'
    employee_cost       DECIMAL(18,2),
    employer_cost       DECIMAL(18,2),
    currency            STRING(3),
    enrollment_date     DATE          NOT NULL,
    termination_date    DATE,
    status              STRING        NOT NULL,   -- 'enrolled','pending','terminated','waived'
    is_statutory        BOOLEAN       NOT NULL,
    country_code        STRING(2)     NOT NULL,
    source_system       STRING        NOT NULL,
    ingested_at         TIMESTAMP     NOT NULL
);
```

**TimeOff**
```sql
CREATE TABLE silver.time_off (
    timeoff_id          STRING        NOT NULL,
    worker_id           STRING        NOT NULL,
    leave_type          STRING        NOT NULL,   -- 'annual','sick','maternity','paternity',
                                                  --  'parental','bereavement','public_holiday',
                                                  --  'unpaid','compensatory'
    start_date          DATE          NOT NULL,
    end_date            DATE          NOT NULL,
    days_taken          DECIMAL(4,1)  NOT NULL,   -- Supports half-days
    status              STRING        NOT NULL,   -- 'requested','approved','rejected','taken','cancelled'
    approved_by         STRING,
    country_code        STRING(2)     NOT NULL,
    affects_payroll     BOOLEAN       NOT NULL,   -- TRUE if unpaid leave reduces pay
    source_system       STRING        NOT NULL,
    ingested_at         TIMESTAMP     NOT NULL
);
```

**Termination**
```sql
CREATE TABLE silver.termination (
    termination_id      STRING        NOT NULL,
    worker_id           STRING        NOT NULL,
    contract_id         STRING        NOT NULL,
    reason              STRING        NOT NULL,   -- 'voluntary_resignation','involuntary_dismissal',
                                                  --  'mutual_agreement','end_of_contract',
                                                  --  'redundancy','probation_failure','retirement'
    notice_date         DATE          NOT NULL,
    last_working_day    DATE          NOT NULL,
    effective_date      DATE          NOT NULL,   -- May differ from last_working_day (garden leave)
    notice_period_days  INTEGER,
    severance_amount    DECIMAL(18,2),
    severance_currency  STRING(3),
    gardening_leave     BOOLEAN       DEFAULT FALSE,
    country_code        STRING(2)     NOT NULL,
    legal_review_status STRING,                    -- 'not_required','pending','approved','blocked'
    source_system       STRING        NOT NULL,
    ingested_at         TIMESTAMP     NOT NULL
);
```

**Adjustment**
```sql
CREATE TABLE silver.adjustment (
    adjustment_id       STRING        NOT NULL,
    original_payslip_id STRING        NOT NULL,   -- The payslip being corrected
    correction_run_id   STRING,                    -- The payroll run containing the correction
    worker_id           STRING        NOT NULL,
    adjustment_type     STRING        NOT NULL,   -- 'salary_correction','tax_recalculation',
                                                  --  'retro_pay','bonus_correction',
                                                  --  'deduction_reversal','overpayment_recovery'
    reason              STRING        NOT NULL,
    amount              DECIMAL(18,2) NOT NULL,   -- Positive = additional pay, negative = recovery
    currency            STRING(3)     NOT NULL,
    amount_usd          DECIMAL(18,2),
    status              STRING        NOT NULL,   -- 'pending','approved','applied','rejected'
    approved_by         STRING,
    applied_in_period   STRING,                    -- The pay period where the adjustment was applied
    country_code        STRING(2)     NOT NULL,
    source_system       STRING        NOT NULL,
    ingested_at         TIMESTAMP     NOT NULL
);
```

### Multi-Country Field Handling

| Concept | India | Germany | UK | Canonical Approach |
|---------|-------|---------|----|--------------------|
| **Tax ID** | PAN (ABCDE1234F) | Steuer-IdNr (11 digits) | UTR (10 digits) or NINO for employment | Store in `worker.tax_id` with format validation per `country_code` |
| **Social insurance ID** | Aadhaar (12 digits) + PF UAN | Sozialversicherungsnummer (12 chars) | National Insurance Number (AB123456C) | Store in `worker.social_id`; India PF UAN goes in `worker.pf_id` |
| **Salary structure** | CTC split: Basic + HRA + Special Allowance | Single gross (Bruttogehalt) | Single gross | Each component maps to a `pay_item` row with `subcategory` distinguishing components |
| **Tax class** | Old regime vs. New regime | Steuerklasse I-VI | Tax code (1257L) | Store in `contract` country-specific fields; used in gross-to-net but does not change the canonical output schema |
| **Mandatory employer cost** | PF (12%), ESI (3.25%), Gratuity (4.81%) | Social insurance (~20%), Church tax pass-through | Employer NI (13.8%), Pension (3%), Apprenticeship Levy | Each maps to `employer_cost` rows with descriptive `cost_type` |

### Data Artifacts

| Entity | Key Fields | Analytics Enabled |
|--------|-----------|-------------------|
| Worker | worker_id, client_id, country_code, status, start_date, valid_from/valid_to | Headcount analytics, point-in-time reporting, worker lifecycle analysis |
| Contract | contract_id, worker_id, entity_id, salary, currency, contract_type | Compensation analytics, contract expiry monitoring, entity assignment |
| PayrollRun | run_id, entity_id, country, period, status, totals | Payroll cycle time, cost tracking, error rate monitoring |
| Payslip | payslip_id, run_id, worker_id, gross, net, has_error | Individual pay accuracy, variance analysis, error classification |
| PayItem | payitem_id, payslip_id, category, subcategory, amount | Pay component breakdown, tax analysis, deduction compliance |
| Payment | payment_id, worker_id, amount, status, rail, settled_at | Payment success rate, settlement time, rail performance |
| StatutoryFiling | filing_id, entity_id, filing_type, due_date, status | Filing compliance rate, overdue tracking, penalty risk |
| FXRate | from_currency, to_currency, rate, rate_type, rate_date | FX margin analysis, rate variance, currency exposure |

### Controls

| Control | Type | Description |
|---------|------|-------------|
| **Schema versioning** | Preventive | All canonical schema changes go through a review board; breaking changes require major version bump and consumer migration plan |
| **Referential integrity checks** | Detective | Automated checks that every `payslip.run_id` exists in `payroll_run`, every `pay_item.payslip_id` exists in `payslip`, etc. |
| **Cross-entity balance checks** | Detective | Sum of `pay_item` amounts for a payslip must equal `payslip.gross_pay` minus `payslip.total_deductions` equals `payslip.net_pay` |
| **SCD2 continuity checks** | Detective | For any SCD2 entity, `valid_to` of row N must equal `valid_from` of row N+1 — no gaps or overlaps |
| **Country-field validation** | Preventive | If `country_code = 'IN'`, then `tax_id` must match PAN format (5 letters + 4 digits + 1 letter); if `country_code = 'DE'`, must match Steuer-IdNr format |

### Metrics

| Metric | Definition | Target | Frequency | Owner |
|--------|-----------|--------|-----------|-------|
| Entity coverage | % of core entities (Worker through Adjustment) fully implemented in silver | 100% within 6 months | Monthly | Data Engineering |
| Schema conformance rate | % of silver records passing all canonical schema validation rules | > 99.5% | Daily | Data Engineering |
| SCD2 coverage | % of dimension entities with full SCD2 history implemented | 100% for Worker, Contract | Monthly | Data Engineering |
| Cross-entity referential integrity | % of foreign key relationships that resolve correctly | > 99.9% | Daily | Data Engineering |
| Balance check pass rate | % of payslips where sum(pay_items) = gross - deductions = net | > 99.95% | Per payroll run | Analytics Engineering |
| Country-specific field completeness | % of workers with all required country-specific fields populated | > 98% per country | Weekly | Data Engineering |
| Schema change frequency | Number of canonical schema changes per quarter | < 5 (stability indicator) | Quarterly | Data Architecture |
| Multi-currency coverage | % of monetary records with both local and USD amounts populated | > 99% | Daily | Data Engineering |
| Canonical mapping coverage | % of source system fields successfully mapped to canonical model | > 95% per source | Monthly | Data Engineering |
| Point-in-time query accuracy | % of historical salary lookups that return the correct value for a given date | 100% | Monthly (audit) | Analytics Engineering |

### Common Failure Modes

| Failure | Consequence | Real-World Example |
|---------|------------|--------------------|
| **Missing SCD2 on salary changes** | Cannot reconstruct what salary was used for a historical payroll run | Worker got a raise in March. April auditor asks: "Was the March payroll calculated on the old or new salary?" Without SCD2, you cannot answer. |
| **Country-specific fields in a single wide table** | Table grows to 200+ columns, most NULL for any given country; query performance degrades | Every new country adds 5-10 columns. After 50 countries, the worker table has 300 columns. Analysts do not know which columns apply to which country. |
| **Currency amount stored without rate** | Cannot reconcile USD totals; FX variance unexplained | Dashboard shows India payroll cost dropped 10% MoM. Is it a real cost reduction or just INR depreciation? Without the rate, you cannot tell. |
| **No versioning on canonical schema** | Breaking change in silver table breaks all gold tables and dashboards downstream | Data engineer adds a column and renames another. Three gold tables fail overnight. Executive dashboard blank for 6 hours. |

### AI Opportunities

| Opportunity | Inputs | Outputs | Guardrails |
|------------|--------|---------|------------|
| **Auto-mapping source to canonical** | New payroll engine's raw schema + canonical DDL | Column mapping suggestions with confidence and data-type compatibility assessment | Require human approval; run sample data through mapping before production |
| **Anomaly detection on pay items** | Historical pay_item distributions by country + subcategory | Flagged outlier pay items (e.g., a bonus 10x the usual amount) for human review | Flag only; never auto-correct payroll amounts; route to ops review queue |
| **Entity resolution suggestions** | Worker records from multiple sources with partial overlaps | Suggested matches (e.g., "Worker A in platform = Employee 789 in German engine") with confidence | Present as suggestions to ops team; require human confirmation for any match below 95% confidence |

### Discovery Questions

1. "Do we have a canonical data model today, or does every team query source systems directly? If a model exists, who owns it and when was it last updated?"
2. "How do we handle country-specific fields? Are they in a single wide table, a key-value extension table, or something else?"
3. "Can we answer the question 'What was this worker's salary on March 1st?' from our current data model? If not, why not?"
4. "When a payroll engine vendor changes their output format, what is the process for updating our mappings? How long does it typically take?"
5. "How many sources of truth do we have for 'active worker count'? Do they agree?"

### Exercises

1. **Schema implementation exercise:** Implement the Worker and Contract schemas in your preferred lakehouse (Databricks, Snowflake, BigQuery). Load synthetic data for 100 workers across 5 countries (India, Germany, UK, Brazil, Singapore). Include at least 10 workers with SCD2 salary history (salary changes with valid_from/valid_to). Write a query that answers: "What was worker W-042's salary on March 15, 2026?"

2. **Multi-country mapping exercise:** You receive a raw payroll export from a new German payroll engine with fields: `Personalnummer`, `Vorname`, `Nachname`, `Bruttolohn`, `Lohnsteuer`, `Kirchensteuer`, `Solidaritaetszuschlag`, `AN_SV_Beitrag` (employee social security), `AG_SV_Beitrag` (employer social security), `Nettolohn`. Write the mapping from these fields to the canonical `payslip`, `pay_item`, and `employer_cost` tables. For each mapping, specify the transformation logic.

3. **Analytics-leader exercise — Schema governance proposal:** Write a one-page proposal for establishing a canonical data model governance process. Include: who sits on the schema review board, how changes are proposed and approved, how breaking vs. non-breaking changes are classified, how downstream consumers are notified, and the rollback process if a schema change causes issues.

---

## Topic 3: Event Spine Architecture — Taxonomy, Sourcing Patterns, and Schema Design

### What It Is

An event is a structured record that something happened at a specific point in time. The **event spine** is the chronological stream of all business events across the global payroll platform — every worker onboarded, every contract signed, every payroll run processed, every payment settled, every filing submitted. It is the backbone for audit trails, real-time operational monitoring, ML feature engineering, process mining, and root cause analysis.

Unlike the canonical data model (which represents current and historical *state*), the event spine represents *transitions* — what changed, when, why, and who caused it. Together, state (canonical model) and transitions (event spine) give you complete observability into the platform.

### Why It Matters

**Business impact:**
- **Audit compliance:** Regulators can request a complete trail of every change to a worker's record. The event spine provides this without querying multiple transactional systems.
- **Process mining:** "How long does a payroll run take from draft to payment, and where does it get stuck?" requires event timestamps at every stage.
- **Real-time monitoring:** A dashboard showing that a payroll run has been in "processing" for 4 hours (vs. the usual 30 minutes) is only possible with event streaming.
- **ML features:** Event sequences are the richest input for predictive models — error prediction, churn prediction, SLA breach forecasting all depend on event patterns.
- **Root cause analysis:** When a payment fails, the event spine lets you trace back through every step to find exactly where things went wrong.

### Process Flow — Event Architecture

```
SOURCE SYSTEMS                    EVENT BUS                    CONSUMERS
──────────────────────────────────────────────────────────────────────────

┌──────────────┐                                    ┌─────────────────┐
│ Platform Core│──┐                                 │ Event Store     │
│ (app events) │  │                                 │ (Bronze: raw    │
└──────────────┘  │   ┌────────────────────────┐    │  append-only    │
                  ├──►│                        │───►│  events in      │
┌──────────────┐  │   │    Event Bus           │    │  Delta/Iceberg) │
│ Payroll      │──┤   │    (Kafka / EventBridge│    └─────────────────┘
│ Engine Events│  │   │     / Pub/Sub)         │
└──────────────┘  │   │                        │    ┌─────────────────┐
                  │   │  Topics:               │───►│ Real-time       │
┌──────────────┐  │   │   worker.*             │    │ Dashboards      │
│ Payment      │──┤   │   contract.*           │    │ (Streaming      │
│ System Events│  │   │   payroll_run.*        │    │  aggregation)   │
└──────────────┘  │   │   payment.*            │    └─────────────────┘
                  │   │   filing.*             │
┌──────────────┐  │   │   change.*             │    ┌─────────────────┐
│ Compliance   │──┤   │                        │───►│ ML Feature      │
│ System Events│  │   └────────────────────────┘    │ Engineering     │
└──────────────┘  │                                 │ (event sequence │
                  │                                 │  features)      │
┌──────────────┐  │                                 └─────────────────┘
│ User Actions │──┘
│ (UI events)  │                                    ┌─────────────────┐
└──────────────┘                                    │ Alerting Engine │
                                                    │ (SLA breach     │
                                                    │  detection)     │
                                                    └─────────────────┘
```

### The Complete Event Taxonomy

Events are organized by lifecycle domain. Each event type follows a namespace convention: `{domain}.{entity}.{action}`.

**Worker Lifecycle Events**
```
worker.created                -- New worker record created in platform
worker.onboarding_started     -- Onboarding workflow initiated
worker.document_uploaded      -- Worker uploaded a required document
worker.document_verified      -- Document verified (PAN, NINO, etc.)
worker.onboarding_completed   -- All onboarding steps done
worker.activated              -- Worker is active and eligible for payroll
worker.profile_updated        -- Non-payroll-affecting profile change
worker.suspended              -- Worker temporarily suspended
worker.reactivated            -- Worker un-suspended
worker.termination_initiated  -- Termination process started
worker.terminated             -- Worker officially terminated
worker.offboarded             -- Final pay complete, access revoked
```

**Contract Events**
```
contract.created              -- New employment contract drafted
contract.sent_for_signature   -- Contract sent to worker for signing
contract.signed               -- Worker signed the contract
contract.countersigned        -- Entity countersigned
contract.activated            -- Contract in effect
contract.amended              -- Contract terms changed (salary, role, etc.)
contract.renewal_triggered    -- Fixed-term contract renewal initiated
contract.terminated           -- Contract ended
contract.expired              -- Fixed-term contract reached end date
```

**Payroll Run Events**
```
payroll_run.created           -- New payroll run initiated for a country/period
payroll_run.inputs_opened     -- Input collection window opened
payroll_run.inputs_received   -- Client submitted payroll inputs
payroll_run.inputs_validated  -- Inputs passed validation checks
payroll_run.inputs_locked     -- Input window closed; no more changes
payroll_run.processing_started -- Gross-to-net calculation started
payroll_run.processing_completed -- Calculation finished
payroll_run.variance_flagged  -- Automated variance check found outliers
payroll_run.review_started    -- Human reviewer started checking
payroll_run.review_completed  -- Reviewer finished (may approve or reject)
payroll_run.approved          -- Payroll approved for payment
payroll_run.rejected          -- Payroll sent back for corrections
payroll_run.locked            -- Payroll locked; no more changes
payroll_run.payment_initiated -- Payment file generated and submitted
payroll_run.paid              -- All payments settled
payroll_run.filing_submitted  -- Statutory filings submitted
payroll_run.closed            -- Run complete; all downstream done
```

**Payment Events**
```
payment.initiated             -- Payment instruction created
payment.submitted_to_bank     -- Payment file sent to banking partner
payment.accepted_by_bank      -- Bank acknowledged receipt
payment.settled               -- Funds arrived in worker's account
payment.failed                -- Payment failed (bad account, insufficient funds, etc.)
payment.retried               -- Failed payment re-attempted
payment.reversed              -- Settled payment reversed (rare)
payment.fx_converted          -- Currency conversion executed
```

**Compliance / Filing Events**
```
filing.created                -- Filing record created for a period
filing.prepared               -- Filing data assembled
filing.submitted              -- Filed with government authority
filing.acknowledged           -- Authority acknowledged receipt
filing.accepted               -- Authority accepted filing
filing.rejected               -- Authority rejected filing
filing.corrected              -- Corrected filing submitted
filing.penalty_assessed       -- Penalty received for late/wrong filing
```

**Change Events (mid-cycle changes that affect payroll)**
```
change.compensation           -- Salary, bonus structure, or allowances changed
change.role                   -- Job title, department, or reporting line changed
change.location               -- Worker relocated (may change tax jurisdiction)
change.bank_details           -- Bank account updated
change.tax_profile            -- Tax regime, tax code, or withholding changed
change.benefits_enrollment    -- Benefit plan added, changed, or removed
change.working_hours          -- Part-time/full-time change or hours adjustment
```

### Event Schema (Contract)

Every event must conform to this standard envelope schema:

```json
{
  "event_id": "evt_7f3a2b1c-9d4e-4f5a-8b6c-1d2e3f4a5b6c",
  "event_type": "payroll_run.variance_flagged",
  "event_version": "2.3",
  "timestamp": "2026-03-15T14:30:22.451Z",
  "actor": {
    "actor_id": "sys_variance_checker",
    "actor_type": "system",
    "actor_name": "Automated Variance Engine"
  },
  "subject": {
    "entity_type": "payroll_run",
    "entity_id": "run_de_2026_03_001"
  },
  "context": {
    "country_code": "DE",
    "entity_id": "ent_de_gmbh_001",
    "client_id": "cli_techstart",
    "environment": "production"
  },
  "payload": {
    "flagged_payslips": 3,
    "total_payslips": 142,
    "flag_reasons": [
      {
        "payslip_id": "ps_de_089",
        "worker_id": "wrk_mueller",
        "reason": "net_pay_variance_exceeds_threshold",
        "variance_pct": -38.2,
        "details": "Tax class change from I to III detected"
      }
    ]
  },
  "metadata": {
    "source_system": "platform_core",
    "correlation_id": "corr_payrun_de_mar_2026",
    "trace_id": "trace_abc123",
    "schema_version": "2.3",
    "partition_key": "DE"
  }
}
```

### Event Schema DDL (for the event store)

```sql
CREATE TABLE bronze.event_store (
    event_id            STRING        NOT NULL,
    event_type          STRING        NOT NULL,
    event_version       STRING        NOT NULL,
    event_timestamp     TIMESTAMP     NOT NULL,
    actor_id            STRING        NOT NULL,
    actor_type          STRING        NOT NULL,   -- 'user','system','api','scheduler'
    subject_entity_type STRING        NOT NULL,
    subject_entity_id   STRING        NOT NULL,
    country_code        STRING(2),
    client_id           STRING,
    entity_id           STRING,
    payload             STRING        NOT NULL,   -- JSON blob (flexible per event_type)
    correlation_id      STRING,
    trace_id            STRING,
    source_system       STRING        NOT NULL,
    schema_version      STRING        NOT NULL,
    ingested_at         TIMESTAMP     NOT NULL
)
PARTITIONED BY (event_date DATE, country_code, event_type);
```

### Schema Versioning Rules

| Change Type | Version Impact | Consumer Impact | Example |
|-------------|---------------|-----------------|---------|
| New optional field added | Minor (2.3 → 2.4) | None — consumers ignore unknown fields | Adding `flag_severity` to variance_flagged payload |
| New event type added | Minor (2.3 → 2.4) | None — consumers only subscribe to known types | Adding `payroll_run.dry_run_completed` |
| Field renamed or removed | Major (2.x → 3.0) | Breaking — consumers must update | Renaming `worker_id` to `employee_id` |
| Field type changed | Major (2.x → 3.0) | Breaking — consumers must update | Changing `amount` from STRING to DECIMAL |
| Payload structure reorganized | Major (2.x → 3.0) | Breaking — consumers must update | Moving nested fields to top level |

### Data Artifacts

| Entity | Key Fields | Analytics Enabled |
|--------|-----------|-------------------|
| Event store (bronze) | event_id, event_type, timestamp, subject, payload | Full event replay, audit trail, process mining |
| Event aggregate (silver) | event_type, country, date, count, avg_payload_size | Event volume trends, anomaly detection |
| Process instance (gold) | process_id (e.g., run_id), event_sequence, timestamps, duration_per_step | Cycle time analysis, bottleneck identification |
| Event schema registry | event_type, current_version, fields, changelog | Schema governance, consumer compatibility |

### Controls

| Control | Type | Description |
|---------|------|-------------|
| **Schema validation on publish** | Preventive | Every event is validated against its registered schema before being accepted by the event bus |
| **Idempotency enforcement** | Preventive | Duplicate event_ids are rejected; consumers use event_id for dedup |
| **Ordering guarantees** | Preventive | Events for the same subject (e.g., same payroll_run) are delivered in order via partition key |
| **Dead letter queue** | Detective | Events that fail schema validation or delivery go to a DLQ for investigation |
| **Event replay capability** | Corrective | Any consumer can replay events from a given timestamp to rebuild state |
| **Retention policy enforcement** | Preventive | Event store retains events per country-specific retention policy (7-10 years for payroll events) |

### Metrics

| Metric | Definition | Target | Frequency | Owner |
|--------|-----------|--------|-----------|-------|
| Event throughput | Events published per second (peak and average) | Handle 10x current peak | Real-time | Platform Engineering |
| Event delivery latency (p99) | Time from event publish to consumer receipt (99th percentile) | < 5 seconds | Real-time | Platform Engineering |
| Dead letter queue depth | Number of events in DLQ awaiting investigation | 0 | Hourly | Data Engineering |
| Schema validation failure rate | % of events rejected due to schema non-compliance | < 0.01% | Daily | Data Engineering |
| Event coverage | % of defined event types that are actually being emitted by source systems | > 95% | Monthly | Data Engineering + Product |
| Duplicate event rate | % of events that are exact duplicates (same event_id) | < 0.1% | Daily | Platform Engineering |
| Process completion rate | % of processes (e.g., payroll runs) where all expected events in the sequence were observed | > 99% | Weekly | Analytics Engineering |
| Average process cycle time | Average time from first event to last event in a process (e.g., payroll_run.created to payroll_run.closed) | Track by country + run_type | Weekly | Payroll Ops |
| Event bus consumer lag | Maximum lag (in events or time) across all consumer groups | < 1 minute | Real-time | Platform Engineering |
| Event store query latency (p95) | 95th percentile query time against the event store for a single subject | < 2 seconds | Daily | Data Engineering |

### Common Failure Modes

| Failure | Consequence | Real-World Example |
|---------|------------|--------------------|
| **Missing events in sequence** | Process mining shows incomplete picture; cycle time metrics are wrong | The `payroll_run.review_started` event was never emitted because the code path for auto-approved runs skips the review events. Cycle time appears shorter than reality. |
| **Event ordering violation** | Consumers process events out of order; state reconstruction is wrong | A `worker.terminated` event arrives before `worker.termination_initiated` due to a race condition. Downstream systems think the worker was terminated without notice. |
| **Payload bloat** | Event store grows faster than projected; query performance degrades; costs spike | Someone adds the full payslip PDF (base64-encoded) to the payroll_run.paid event payload. A single event goes from 2KB to 2MB. |
| **No correlation ID** | Cannot link related events across domains (e.g., linking a payment failure back to its payroll run) | Payment.failed event has no reference to the originating payroll_run. Ops must manually investigate which run the payment belonged to. |
| **Schema version mismatch** | Consumer built for v2 receives v3 events and crashes | Event schema upgraded to v3 with a field rename. A downstream ML pipeline still expects v2 field names. Feature engineering breaks silently (NULL values). |

### AI Opportunities

| Opportunity | Inputs | Outputs | Guardrails |
|------------|--------|---------|------------|
| **Predictive process mining** | Historical event sequences for payroll runs by country | Predicted cycle time for active runs; flagged runs likely to miss SLA | Predictions are advisory; never auto-intervene in payroll processing based on prediction alone |
| **Anomaly detection on event patterns** | Event frequency and sequence patterns (baseline per country/event_type) | Alerts when event patterns deviate from baseline (e.g., unusually high variance_flagged count) | Require ops confirmation before escalation; suppress during known disruptions |
| **Root cause analysis assistant** | Event sequence for a failed process + historical similar failures | Suggested root cause with supporting evidence from similar past incidents | Present as suggestions with confidence score; human makes final determination |
| **Auto-generation of event taxonomy for new domains** | Existing event taxonomy + description of new business process | Suggested event types with schemas for the new process | All auto-generated events require product + engineering review before implementation |

### Discovery Questions

1. "Do we have a centralized event bus, or are events scattered across application logs, database triggers, and message queues?"
2. "Can we reconstruct the complete lifecycle of a payroll run from our current event data — from creation to closure, with timestamps at every step?"
3. "When a payment fails, how do we currently trace back to the root cause? How many systems do people have to check manually?"
4. "What is our event retention policy? Do we keep all events indefinitely, or is there a TTL? Does this align with regulatory requirements?"
5. "Are there business processes that we know are happening but are not emitting events? Where are the event coverage gaps?"

### Exercises

1. **Complete event stream design:** Map the full event sequence for a German payroll run from creation to closure. Include: every event type, the expected timestamp offset from the start (e.g., T+0h, T+2h, T+24h), the actor for each event, and the key payload fields. Then identify which events are currently missing in a typical implementation and propose how to add them.

2. **Process mining query:** Using the event store schema defined above, write a SQL query that calculates the average cycle time from `payroll_run.created` to `payroll_run.paid` by country for the last 6 months. Then write a second query that identifies the step (event pair) with the highest variance — this is where the process bottleneck is.

3. **Analytics-leader exercise — Event spine business case:** Write a one-page business case for investing in a centralized event spine. Include: current pain points (with estimated cost of manual investigation hours), proposed architecture, implementation cost and timeline, expected ROI (quantified in hours saved, incidents prevented, and compliance risk reduced). Target audience: VP of Engineering and VP of Payroll Ops.

---

## Topic 4: Data Ingestion Patterns — How Worker, Payroll, and Compliance Data Flows Into the Platform

### What It Is

Data ingestion is the process of moving data from operational source systems into the analytics platform. In a global payroll context, this is not a single pipeline — it is a network of 50-100+ distinct data flows, each with its own source format, delivery mechanism, latency characteristics, and failure modes. The ingestion layer is the first gate: if data does not arrive on time, in the right format, and with the right completeness, everything downstream — silver tables, gold aggregates, dashboards, ML models — is wrong or stale.

### Why It Matters

**Business impact:**
- A payroll run summary that arrives 48 hours late means the ops team is making decisions on two-day-old data — they cannot catch processing errors before payment
- A partner payroll engine that silently changes its export format breaks ingestion; if nobody notices for a week, an entire country's analytics goes dark
- Client HRIS integrations that fail intermittently create "ghost workers" (active in HRIS, missing in payroll) or "zombie workers" (terminated in HRIS, still being paid)
- Filing deadline monitoring depends on fresh data from government portals; stale data means missed deadlines and penalties

### Process Flow

```
INGESTION PATTERN DECISION TREE
────────────────────────────────────────────────────────────────────

Do you own/control the source database?
  │
  ├─ YES ──► Use CDC (Change Data Capture)
  │          Capture every insert/update/delete from DB transaction log
  │          Latency: seconds to minutes
  │          Tools: Debezium, AWS DMS, Fivetran, Airbyte
  │
  └─ NO ───► Does the source system have an API?
              │
              ├─ YES ──► Does it support webhooks/push notifications?
              │           │
              │           ├─ YES ──► Use Webhooks + API for bulk sync
              │           │          Latency: near real-time for changes
              │           │
              │           └─ NO ───► Use API Polling on schedule
              │                      Latency: minutes to hours (polling interval)
              │
              └─ NO ───► Does it export files (CSV, XML, JSON)?
                          │
                          ├─ YES ──► Use Batch/SFTP ingestion
                          │          Latency: hours to daily
                          │
                          └─ NO ───► Manual data entry or screen scraping
                                     Latency: days (last resort)
```

### Source System Ingestion Matrix

| Source System | Pattern | Format | Frequency | Latency SLA | Volume (10K workers) | Failure Mode |
|--------------|---------|--------|-----------|-------------|---------------------|--------------|
| Platform core DB | CDC (Debezium → Kafka) | Avro/JSON from WAL | Continuous | < 5 min | ~500K changes/day | DB failover breaks CDC slot |
| German payroll engine (DATEV) | Batch SFTP | CSV/XML, German field names | Daily at 02:00 CET | < 6 hours | ~200 files/month | File format change, encoding issues (UTF-8 vs Latin-1) |
| Indian payroll engine | API polling | JSON REST API | Every 4 hours | < 8 hours | ~5K records/poll | API rate limiting, token expiry |
| UK payroll engine | API + Webhook | JSON REST + event push | Webhook for changes, daily full sync | < 2 hours | ~3K records/day | Webhook delivery failures, API downtime |
| Client HRIS (Workday) | API polling | JSON (Workday REST API) | Every 6 hours | < 12 hours | Varies by client | Authentication renewal, schema changes per client |
| Client HRIS (BambooHR) | Webhook + API | JSON webhook + REST | Near real-time for changes | < 1 hour | Varies by client | Webhook payload incomplete; requires API follow-up |
| Banking / Treasury | Batch + API | MT940/CAMT.053 files + API | Files daily; API for status checks | < 24 hours | ~2K transactions/day | Bank holiday gaps, reconciliation file delays |
| Government filing portals | Batch (screen scrape or manual) | PDF, CSV, HTML | Weekly or per filing cycle | < 48 hours | ~100 filings/month | Portal downtime, format changes, CAPTCHA |
| Benefits providers | API or Batch | CSV or JSON | Monthly | < 48 hours | ~1K records/month | Provider-specific formats, no standard schema |
| Zendesk / Ticketing | API polling | JSON REST | Every 30 minutes | < 1 hour | ~500 tickets/day | API rate limits, pagination issues |

### Ingestion Pipeline for a Single Country (Example: Germany)

```
DATEV Payroll Engine (Germany)
    │
    │  SFTP export at 02:00 CET daily
    │  Files: payroll_results_YYYYMM.csv, employer_costs_YYYYMM.csv
    ▼
┌──────────────────────────────────────┐
│ Landing Zone (S3: raw/de/datev/)     │
│  - File arrives, checksum validated   │
│  - Manifest file checked (expected    │
│    file count, row counts)            │
│  - PII: encrypted at rest (SSE-S3)   │
└──────────────┬───────────────────────┘
               │
               ▼
┌──────────────────────────────────────┐
│ Schema Validation (Spark job)         │
│  - Column names match expected schema │
│  - Data types validate (dates, nums)  │
│  - Row count within expected range    │
│  - Encoding check (UTF-8)            │
│  IF FAIL → alert + move to quarantine │
└──────────────┬───────────────────────┘
               │
               ▼
┌──────────────────────────────────────┐
│ Bronze Landing (Delta: bronze.de_datev_payroll) │
│  - Append raw data with ingestion metadata      │
│  - Columns: _source_file, _ingested_at,         │
│    _row_number + all original columns            │
│  - Partitioned by ingestion_date                 │
└──────────────┬───────────────────────────────────┘
               │
               ▼
┌──────────────────────────────────────┐
│ Canonical Mapping (dbt model)         │
│  - Personalnummer → worker_id (via    │
│    mapping table)                     │
│  - Bruttolohn → gross_pay             │
│  - Kirchensteuer → pay_item           │
│    (subcategory='church_tax')         │
│  - DQ rules applied (nulls, ranges)  │
│  IF DQ FAIL → quarantine row +        │
│    alert; rest of batch proceeds      │
└──────────────┬───────────────────────┘
               │
               ▼
┌──────────────────────────────────────┐
│ Silver Tables                         │
│  silver.payroll_run (DE rows)         │
│  silver.payslip (DE rows)             │
│  silver.pay_item (DE rows)            │
│  silver.employer_cost (DE rows)       │
└──────────────────────────────────────┘
```

### Data Artifacts

| Entity | Key Fields | Analytics Enabled |
|--------|-----------|-------------------|
| Ingestion manifest | source_id, expected_files, actual_files, expected_rows, actual_rows, timestamp | Pipeline completeness monitoring |
| Source-to-canonical mapping | source_system, source_field, canonical_entity, canonical_field, transformation_logic | Mapping coverage, change tracking |
| Quarantine log | record_id, source, failure_reason, severity, resolution_status, resolved_at | DQ issue tracking, resolution time |
| Pipeline schedule | pipeline_id, source, cron_expression, next_run, last_success, last_failure | Schedule adherence, freshness SLA |
| Integration health log | source_id, check_timestamp, connectivity, auth_status, latency_ms | Integration reliability monitoring |

### Controls

| Control | Type | Description |
|---------|------|-------------|
| **File manifest validation** | Preventive | Every batch delivery must include a manifest (expected file count, row counts, checksums). Missing or mismatched manifests trigger alerts. |
| **Schema-on-read validation** | Preventive | Before writing to bronze, validate that the incoming schema matches the registered source schema. Reject and quarantine on mismatch. |
| **Deduplication on ingest** | Preventive | Use source-system primary keys + ingestion timestamp to detect and reject duplicate deliveries |
| **Freshness monitoring** | Detective | Every pipeline has a freshness SLA. If data has not arrived within the SLA window, an alert fires to the data engineering on-call. |
| **Volume anomaly detection** | Detective | If row count deviates more than 30% from the same-day-last-month baseline, alert for investigation |
| **PII encryption in transit and at rest** | Preventive | All data transfers use TLS 1.2+; all storage uses encryption at rest (AES-256) |
| **Retry with backoff** | Corrective | Failed API calls or file transfers retry up to 3 times with exponential backoff before alerting |

### Metrics

| Metric | Definition | Target | Frequency | Owner |
|--------|-----------|--------|-----------|-------|
| Ingestion success rate | % of scheduled ingestion runs completing without error | > 99% | Daily | Data Engineering |
| Freshness SLA adherence | % of source systems delivering data within their SLA | > 98% | Daily | Data Engineering |
| Row rejection rate | % of incoming rows quarantined due to validation failures | < 1% per source | Per run | Data Engineering |
| Schema drift detection rate | % of schema changes in source systems detected before pipeline failure | 100% (target) | Per incident | Data Engineering |
| Mean time to detect ingestion failure | Average time from pipeline failure to alert | < 15 minutes | Per incident | Data Engineering |
| Mean time to resolve ingestion failure | Average time from alert to pipeline restored | < 4 hours | Per incident | Data Engineering |
| Quarantine resolution time | Average time for quarantined records to be resolved (fixed or discarded) | < 24 hours | Daily | Data Engineering |
| Integration uptime per source | % of time each source system integration is functional | > 99.5% | Monthly | Data Engineering |
| Data volume trend | Row count per source per day — tracked for capacity planning | Within 2x baseline | Weekly | Data Engineering |
| End-to-end ingestion latency | Time from source system change to bronze table availability | Per pattern SLA | Daily | Data Engineering |

### Common Failure Modes

| Failure | Consequence | Real-World Example |
|---------|------------|--------------------|
| **SFTP credentials expire without notice** | Partner payroll data stops flowing; country goes dark | Indian payroll partner rotates SFTP credentials quarterly. Nobody on the data engineering team is on the distribution list for the rotation notice. Pipeline fails silently on a Saturday night. |
| **API pagination bug** | Only first page of results ingested; remaining records missing | Client HRIS API returns 100 records per page. Ingestion code fetches page 1 (100 workers) but does not follow `next_page` link. 400 workers in that client are invisible. |
| **Character encoding mismatch** | German umlauts (ö, ü, ä) corrupt to garbage characters | DATEV exports in ISO-8859-1 (Latin-1); ingestion assumes UTF-8. "Müller" becomes "M\xfcller" in bronze. Silver table name matching fails for all workers with umlauts. |
| **Timezone confusion** | Events appear to happen in the future or past; ordering is wrong | Indian payroll engine timestamps events in IST (UTC+5:30) without timezone indicator. Ingestion treats them as UTC. Events appear 5.5 hours in the future. |
| **Duplicate file delivery** | Same payroll data ingested twice; double-counting in aggregates | Partner sends the same SFTP file twice (once at 02:00, again at 02:15 due to a retry on their side). Without dedup, every payslip for that country appears twice. |

### AI Opportunities

| Opportunity | Inputs | Outputs | Guardrails |
|------------|--------|---------|------------|
| **Intelligent retry and routing** | Pipeline failure history, failure codes, time-of-day patterns | Recommended retry strategy (timing, method) and escalation routing | Auto-retry only for known transient failures (network timeout, rate limit); escalate unknown failures to humans |
| **Schema change detection** | Previous source schema + current incoming schema | Diff report showing added/removed/renamed columns with suggested mapping updates | Never auto-apply schema changes to production mappings; require human review and testing |
| **Data volume forecasting** | Historical ingestion volumes by source, worker count growth | Projected storage and compute needs for next 6 months | Forecasts are advisory for capacity planning; no auto-scaling based on forecasts alone |

### Discovery Questions

1. "How many distinct ingestion pipelines do we operate today? How many are CDC, API polling, and batch/SFTP?"
2. "Which source system is the most fragile — the one that breaks most often or requires the most manual intervention?"
3. "When a partner payroll engine changes their export format, what is our detection and recovery process? How long does it typically take?"
4. "Do we have a single ingestion framework, or does each source have a bespoke pipeline built by whoever happened to be available?"
5. "What is our current mean time to detect and resolve ingestion failures? Is this tracked?"

### Exercises

1. **Ingestion strategy for a new country:** You are launching payroll operations in Colombia via a local partner. The partner can deliver payroll results via SFTP (CSV) or API (REST, JSON). Design the complete ingestion pipeline: file/API specification, landing zone, schema validation, bronze table schema, canonical mapping to silver, DQ rules, freshness SLA, and alerting. Justify your pattern choice (SFTP vs. API).

2. **Ingestion failure postmortem:** Write a postmortem for this scenario: The Indian payroll engine API changed its authentication from API key to OAuth2 on March 1st. The change was communicated via email to a distribution list that did not include the data engineering team. The pipeline failed at the March 1st scheduled run. It was not detected until March 3rd when the ops team noticed the India payroll dashboard was blank. 4,500 workers' payroll data was 2 days late in the analytics platform. Include: timeline, root cause, impact, remediation, and prevention measures.

3. **Analytics-leader exercise — Ingestion SLA framework:** Design a comprehensive ingestion SLA framework for a platform with 30 source systems. Define: SLA tiers (critical/high/medium/low), latency targets per tier, monitoring and alerting rules, escalation paths, and a monthly SLA report template that you would present to the VP of Engineering.

---

## Topic 5: Data Quality Framework — Rules, Monitoring, Alerting, and Remediation

### What It Is

Data quality (DQ) in a global payroll analytics platform is the systematic practice of measuring, monitoring, and enforcing the trustworthiness of data across every layer — from raw ingestion through gold analytics tables. Unlike most SaaS analytics where poor data quality causes minor inconvenience (a slightly wrong dashboard number), poor payroll data quality can result in: incorrect pay, missed filings, regulatory penalties, failed audits, and lost client trust.

The DQ framework is not a tool — it is a combination of rules, processes, monitoring, alerting, and remediation workflows that together ensure the data platform is trustworthy enough for the decisions it supports.

### Why It Matters

**Business impact:**
- An executive dashboard showing headcount of 14,800 when the real number is 15,200 (because 400 workers were dropped by a faulty ingestion pipeline) leads to wrong capacity planning, wrong revenue forecasts, and wrong staffing decisions
- A compliance dashboard showing "all filings on time" when 3 filings were actually overdue (because the filing status data was stale) means penalties nobody anticipated
- An ML model trained on dirty payroll data produces unreliable predictions — a "payroll error risk" model that flags clean runs and misses dirty ones is worse than no model at all
- The moment a VP asks "Can I trust this number?" and the answer is "I think so" rather than "Yes, here's the DQ score," your analytics credibility is gone

### Process Flow — DQ Lifecycle

```
┌─────────────────────────────────────────────────────────────────┐
│                    DQ RULE LIFECYCLE                              │
│                                                                  │
│  ┌──────────┐    ┌──────────┐    ┌──────────┐    ┌──────────┐  │
│  │  DEFINE   │───►│ IMPLEMENT │───►│  MONITOR  │───►│ REMEDIATE │  │
│  │           │    │           │    │           │    │           │  │
│  │ - Business│    │ - SQL/dbt │    │ - Schedule│    │ - Alert   │  │
│  │   rules   │    │   checks  │    │   runs    │    │ - Triage  │  │
│  │ - Domain  │    │ - Great   │    │ - Score   │    │ - Fix     │  │
│  │   expert  │    │   Expect. │    │   compute │    │ - Verify  │  │
│  │   input   │    │ - Custom  │    │ - Dashboard│   │ - Close   │  │
│  │ - Severity│    │   validators│  │   update  │    │           │  │
│  └──────────┘    └──────────┘    └──────────┘    └──────────┘  │
│       ▲                                              │          │
│       └──────────────────────────────────────────────┘          │
│                    Feedback: new rules from incidents            │
└─────────────────────────────────────────────────────────────────┘
```

### The Five Dimensions of Data Quality for Payroll

| Dimension | Definition | Payroll Example | Measurement |
|-----------|-----------|-----------------|-------------|
| **Completeness** | All expected records are present | Every active worker has a payslip record for this period; no country is missing from the payroll run table | % of expected records present vs. actual |
| **Freshness** | Data reflects the most recent state within its SLA | Today's dashboard reflects yesterday's payroll runs, not last week's | Time since last successful ingestion vs. SLA |
| **Validity** | Values conform to expected formats, ranges, and business rules | Salary is positive; country code is ISO 3166; net pay <= gross pay; status is in the valid enum | % of records passing validation rules |
| **Consistency** | The same fact agrees across different tables and sources | Worker count in `silver.worker` = count in `silver.payslip` for the same period = count in `silver.payment` | Cross-table reconciliation pass rate |
| **Accuracy** | Values reflect the real-world truth | The salary in the analytics platform matches the signed employment contract amount | Spot-check audits against source-of-truth systems |

### DQ Rules Catalog for Payroll (Representative Set)

| Rule ID | Dimension | Entity | Rule Description | Check Logic (SQL-like) | Severity | Action on Failure |
|---------|-----------|--------|-----------------|----------------------|----------|-------------------|
| DQ-001 | Completeness | Worker | Every active worker has a current SCD2 record | `WHERE status='active' AND is_current=TRUE` — should match platform active count | Critical | Block gold refresh; alert ops |
| DQ-002 | Completeness | Payslip | Every worker in a payroll run has exactly one payslip | `GROUP BY run_id, worker_id HAVING COUNT(*) != 1` | Critical | Alert ops; flag run for review |
| DQ-003 | Validity | Payslip | Net pay is positive (or zero for fully deducted) | `WHERE net_pay < 0` | Critical | Quarantine record; alert ops |
| DQ-004 | Validity | Payslip | Net pay <= gross pay | `WHERE net_pay > gross_pay` | Critical | Quarantine record; alert ops |
| DQ-005 | Validity | Worker | Tax ID format matches country rules | PAN format for IN, Steuer-IdNr format for DE, NINO format for UK | High | Alert; allow proceed with warning |
| DQ-006 | Consistency | PayrollRun | Headcount in run matches count of payslips | `run.headcount != COUNT(payslips WHERE run_id = run.run_id)` | Critical | Block gold refresh; alert ops |
| DQ-007 | Consistency | PayrollRun | Total gross in run matches sum of payslip gross | `ABS(run.total_gross - SUM(payslip.gross_pay)) > 0.01` | Critical | Alert; investigate rounding vs. real error |
| DQ-008 | Freshness | Pipeline | Data arrived within SLA | `last_ingestion_time > NOW() - SLA_interval` | High | Alert data engineering on-call |
| DQ-009 | Validity | Payment | Payment amount matches payslip net pay | `ABS(payment.amount - payslip.net_pay) > 0.01` | High | Alert treasury; investigate |
| DQ-010 | Completeness | StatutoryFiling | Every entity-period has all required filing types | Cross-reference filing_type lookup per country against actual filings | High | Alert compliance team |
| DQ-011 | Validity | FXRate | FX rate is within 5% of previous day's rate | `ABS(today_rate - yesterday_rate) / yesterday_rate > 0.05` | Medium | Flag for review (may be legitimate volatility) |
| DQ-012 | Consistency | Worker | Worker count in silver matches worker count in platform DB | Silver count vs. CDC source count, reconciled daily | High | Investigate; potential ingestion gap |
| DQ-013 | Validity | Contract | End date is after start date (when not null) | `WHERE end_date IS NOT NULL AND end_date < start_date` | Critical | Quarantine record |
| DQ-014 | Validity | TimeOff | Days taken is positive and reasonable (< 365) | `WHERE days_taken <= 0 OR days_taken > 365` | Medium | Flag for review |
| DQ-015 | Accuracy | Payslip | Month-over-month net pay variance within 30% unless explained by change event | Join payslip to change events; flag if variance > 30% with no corresponding change event | Medium | Route to ops review queue |

### DQ Observability Dashboard

```
DATA QUALITY DASHBOARD — March 15, 2026, 08:00 UTC
══════════════════════════════════════════════════════════════════

OVERALL DQ SCORE:  97.8%  ⚠️  (target: >99%)
────────────────────────────────────────────────────────────────

BY DIMENSION:
  Completeness:    99.2%  ✅  (15,187 of 15,200 expected workers present)
  Freshness:       94.0%  ⚠️  (IN pipeline stale — last update 23h ago, SLA 12h)
  Validity:        99.8%  ✅  (14 records quarantined out of 72,000)
  Consistency:     98.5%  ⚠️  (3 country headcount mismatches: BR, CO, PH)
  Accuracy:        99.1%  ✅  (spot check: 12 of 12 samples match source)

BY COUNTRY (worst 5):
  India:           92.1%  🔴  Freshness failure — pipeline stale 23h
  Brazil:          95.3%  ⚠️  3 payslips with net_pay > gross_pay (DQ-004)
  Colombia:        96.0%  ⚠️  Headcount mismatch: silver=47, platform=49
  Philippines:     96.8%  ⚠️  2 workers missing tax_id (DQ-005)
  Germany:         99.9%  ✅

ACTIVE INCIDENTS:
  [INC-2341] IN pipeline stale — assigned to @data_eng_oncall — opened 11h ago
  [INC-2342] BR net_pay > gross_pay — assigned to @payroll_ops_br — opened 3h ago
  [INC-2343] CO headcount mismatch — assigned to @data_eng — opened 1h ago

TREND (last 30 days):
  Overall DQ Score: 97.8% ← 98.1% ← 99.0% ← 98.7% ← 99.2%
  Direction: ↓ declining — investigate IN freshness and BR validity issues
```

### Data Artifacts

| Entity | Key Fields | Analytics Enabled |
|--------|-----------|-------------------|
| DQ rule registry | rule_id, dimension, entity, description, check_logic, severity, owner | Rule coverage analysis, gap identification |
| DQ check results | rule_id, check_timestamp, pass_count, fail_count, pass_rate, table_name | DQ score computation, trend analysis |
| DQ incident log | incident_id, rule_id, severity, detected_at, resolved_at, root_cause, resolution | MTTR tracking, recurring issue identification |
| DQ score history | date, dimension, entity, country, score | Trend analysis, regression detection |
| Quarantine table | record_id, source_table, quarantine_reason, quarantined_at, resolution_status, resolved_at | Quarantine depth monitoring, resolution SLA |

### Controls

| Control | Type | Description |
|---------|------|-------------|
| **DQ gate between silver and gold** | Preventive | Gold tables do not refresh if the silver DQ score for any critical dimension drops below threshold (e.g., completeness < 98%) |
| **Quarantine isolation** | Preventive | Records failing critical DQ checks are moved to quarantine tables — never promoted to silver or gold until fixed |
| **Severity-based alerting** | Detective | Critical DQ failures page the on-call engineer immediately; high severity alerts go to Slack; medium generates a daily digest |
| **Weekly DQ review** | Detective | Weekly meeting with data engineering, analytics, and domain leads to review DQ trends, open incidents, and new rule proposals |
| **DQ score in data product metadata** | Transparency | Every data product in the catalog displays its current DQ score — consumers can see freshness, completeness, and validity before using |
| **Regression testing on DQ rules** | Preventive | When pipeline code changes, DQ rules are run against a test dataset before deployment to catch regressions |

### Metrics

| Metric | Definition | Target | Frequency | Owner |
|--------|-----------|--------|-----------|-------|
| Overall DQ score | Weighted average of all 5 dimension scores across all entities | > 99% | Daily | Data Engineering |
| DQ score by dimension | Individual score for completeness, freshness, validity, consistency, accuracy | > 98% each | Daily | Data Engineering |
| DQ score by country | Composite DQ score for all entities related to a specific country | > 97% per country | Daily | Data Engineering |
| Critical DQ incident count | Number of critical-severity DQ failures in the current period | 0 | Daily | Data Engineering |
| Mean time to detect DQ issue | Average time from data anomaly to alert firing | < 30 minutes | Per incident | Data Engineering |
| Mean time to resolve DQ issue | Average time from alert to issue resolved and data corrected | < 8 hours (critical), < 24 hours (high) | Per incident | Data Engineering |
| Quarantine depth | Number of records currently in quarantine across all entities | < 100 (trending toward 0) | Daily | Data Engineering |
| Quarantine resolution rate | % of quarantined records resolved within SLA | > 95% | Weekly | Data Engineering |
| DQ rule coverage | Number of DQ rules per entity — proxy for monitoring depth | > 10 rules per core entity | Monthly | Analytics Engineering |
| False positive rate | % of DQ alerts that turn out to be non-issues after investigation | < 10% | Monthly | Data Engineering |
| Downstream impact incidents | Number of times a DQ issue caused a dashboard to show wrong data | 0 (aspire) | Monthly | Analytics Engineering |

### Common Failure Modes

| Failure | Consequence | Real-World Example |
|---------|------------|--------------------|
| **Alert fatigue from noisy DQ rules** | Team ignores all DQ alerts; real critical issue gets missed | FX rate volatility rule (DQ-011) fires every time there is legitimate currency movement (e.g., after a central bank announcement). Team starts ignoring all DQ alerts. A genuine data corruption in the FX table goes unnoticed for 3 days. |
| **DQ rules not updated when business rules change** | Rules pass on data that is actually wrong by current standards | India PF ceiling increased from 15,000 to 18,000 in a regulatory update. The DQ rule still validates against the old ceiling. PF calculations are wrong, but DQ shows green. |
| **No DQ on gold layer** | Silver passes all checks, but the gold aggregation logic introduces errors | Gold table `payroll_run_summary` double-counts workers who appear in both a regular and a correction run. Silver is correct (both records are valid), but gold totals are inflated. |
| **Quarantine backlog grows unchecked** | Hundreds of records stuck in quarantine for weeks; nobody investigates | 200 Brazilian worker records quarantined due to a tax ID format change. Nobody resolves them. The Brazil headcount in analytics is consistently 200 lower than reality for 3 months. |
| **DQ checks only run on schedule, not on arrival** | Bad data sits in bronze/silver for hours before detection | A file with corrupt encoding lands in bronze at 02:00. DQ checks run at 06:00. For 4 hours, anyone querying silver gets garbled data. |

### AI Opportunities

| Opportunity | Inputs | Outputs | Guardrails |
|------------|--------|---------|------------|
| **Auto-generated DQ rules** | Historical data distributions for each column (min, max, mean, stddev, null rate, cardinality) | Suggested DQ rules with thresholds derived from data (e.g., "salary_usd should be between 3,000 and 300,000 based on historical distribution") | All auto-generated rules require human review; start in "advisory" mode (log but don't block) for 2 weeks before activating |
| **Root cause classification** | DQ failure details + pipeline metadata + recent changes | Likely root cause category (source schema change, network timeout, data corruption, business rule change) with confidence | Present ranked list of causes; human investigates and confirms |
| **DQ score forecasting** | DQ score time series by country and dimension | Predicted DQ score for next 7 days; early warning if score is trending toward threshold | Advisory only; no automated remediation based on forecasts |

### Discovery Questions

1. "Do we have a formal data quality framework today, or are quality checks ad-hoc and embedded in individual pipelines?"
2. "When was the last time a dashboard showed wrong data? How long did it take to detect, and what was the root cause?"
3. "Who is responsible for data quality — the data engineering team, the analytics team, or the domain teams? Is accountability clear?"
4. "How do we currently handle records that fail quality checks? Is there a quarantine process, or do bad records flow through to dashboards?"
5. "What is the single most common data quality issue you encounter? How often does it recur?"

### Exercises

1. **DQ rule design exercise:** Define 20 data quality rules across all five dimensions for the canonical payroll entities (Worker, Contract, PayrollRun, Payslip, PayItem, Payment, StatutoryFiling). For each rule: specify the dimension, entity, rule description, check logic (SQL), severity (critical/high/medium), and the action on failure (block/alert/log). Implement 5 of these as Great Expectations or dbt tests.

2. **DQ incident postmortem:** Write a postmortem for this scenario: The gold table `country_monthly_metrics` showed Germany's March headcount as 0 for 6 hours because the silver-to-gold aggregation job ran before the bronze-to-silver job completed (race condition). The VP of Ops saw the dashboard and escalated to the CTO. Include: timeline, root cause, impact, remediation, and the DQ controls that should have prevented this.

3. **Analytics-leader exercise — DQ SLO proposal:** Write a formal Data Quality Service Level Objective (SLO) document for the payroll data platform. Define: which DQ dimensions are measured, what the target scores are (per dimension, per country, overall), how scores are calculated, what the consequence of missing the SLO is (review process, resource allocation), and how the SLO will be reported (weekly report to VP Engineering, monthly to exec team).

---

## Topic 6: Multi-Tenant Data Architecture — Tenant Isolation, Data Partitioning, and Cross-Tenant Analytics

### What It Is

A multi-tenant data architecture ensures that data from different clients (tenants) is properly isolated — no client can ever see another client's data — while still enabling the platform to run cross-tenant analytics for internal operational intelligence. This is a fundamental architectural challenge in any SaaS platform, but it is especially critical in payroll because the data includes the most sensitive employee information: salaries, tax IDs, bank accounts, and personal details.

The term "tenant" in a global EOR platform has multiple dimensions:
- **Client tenant:** Each client company and its workers. Client A must never see Client B's data.
- **Country tenant:** Data may be subject to data residency requirements (e.g., German worker data must stay within the EU).
- **Entity tenant:** Each legal entity (owned or partner) has its own compliance boundaries.

### Why It Matters

**Business impact:**
- A cross-tenant data leak (Client A seeing Client B's worker salaries) is a catastrophic trust violation that can result in contract termination, regulatory fines (GDPR: up to 4% of global annual revenue), and reputational damage
- Data residency violations (German data stored outside the EU) can result in regulatory action from data protection authorities
- Without proper tenant isolation, a single "rogue query" from an internal analyst could accidentally expose one client's data in a report intended for another
- At the same time, the platform needs cross-tenant analytics to measure operational performance (average processing time across all clients, error rates by country, etc.)

### Process Flow — Multi-Tenant Isolation Architecture

```
OPTION A: LOGICAL ISOLATION (Shared Infrastructure, Tenant Column)
═══════════════════════════════════════════════════════════════════

┌───────────────────────────────────────────────────────────┐
│  Single Lakehouse / Warehouse                              │
│                                                            │
│  ┌─────────────────────────────────────────────────────┐  │
│  │  silver.worker                                       │  │
│  │  ┌────────────┬────────────┬───────┬──────────────┐ │  │
│  │  │ worker_id  │ client_id  │ name  │ salary       │ │  │
│  │  ├────────────┼────────────┼───────┼──────────────┤ │  │
│  │  │ wrk_001    │ cli_acme   │ Priya │ ₹30,00,000  │ │  │
│  │  │ wrk_002    │ cli_acme   │ James │ £60,000     │ │  │
│  │  │ wrk_003    │ cli_beta   │ Maria │ €85,000     │ │  │
│  │  │ wrk_004    │ cli_beta   │ Carlos│ $60,000     │ │  │
│  │  └────────────┴────────────┴───────┴──────────────┘ │  │
│  └─────────────────────────────────────────────────────┘  │
│                                                            │
│  Access Control Layer (row-level security):                │
│  ┌─────────────────────────────────────────────────────┐  │
│  │ Client Portal (cli_acme):                            │  │
│  │   SELECT * FROM silver.worker                        │  │
│  │   WHERE client_id = 'cli_acme'  ← enforced by RLS   │  │
│  └─────────────────────────────────────────────────────┘  │
│                                                            │
│  ┌─────────────────────────────────────────────────────┐  │
│  │ Internal Analytics:                                  │  │
│  │   SELECT country, COUNT(*), AVG(salary_usd)          │  │
│  │   FROM silver.worker  ← full access, aggregated only │  │
│  └─────────────────────────────────────────────────────┘  │
└───────────────────────────────────────────────────────────┘


OPTION B: PHYSICAL ISOLATION (Separate Schemas per Tenant)
═══════════════════════════════════════════════════════════════════

┌────────────────────┐  ┌────────────────────┐  ┌──────────────┐
│ schema: cli_acme   │  │ schema: cli_beta   │  │ schema:      │
│ ┌────────────────┐ │  │ ┌────────────────┐ │  │  internal    │
│ │ worker         │ │  │ │ worker         │ │  │ ┌──────────┐ │
│ │ contract       │ │  │ │ contract       │ │  │ │ agg_     │ │
│ │ payslip        │ │  │ │ payslip        │ │  │ │ metrics  │ │
│ └────────────────┘ │  │ └────────────────┘ │  │ └──────────┘ │
└────────────────────┘  └────────────────────┘  └──────────────┘

Each client has isolated schema.   Internal analytics
No RLS needed — access is          operates on pre-aggregated
by schema permission.              cross-tenant data only.
```

### Isolation Strategy Comparison

| Dimension | Logical Isolation (RLS) | Physical Isolation (Separate Schemas) | Hybrid |
|-----------|------------------------|--------------------------------------|--------|
| **Isolation strength** | Medium — depends on RLS implementation quality | High — schemas are separate by definition | High for sensitive data; medium for operational data |
| **Cross-tenant analytics** | Easy — single table, aggregate queries | Hard — must UNION across schemas or ETL into aggregate tables | Medium — operational tables unified; PII isolated |
| **Maintenance cost** | Low — single schema to maintain | High — schema changes must be applied N times | Medium |
| **Compliance** | Requires robust RLS testing and audit | Easier to demonstrate isolation to auditors | Depends on implementation |
| **Scale** | Scales to thousands of tenants | Becomes unwieldy beyond 50-100 tenants | Flexible |
| **Risk of misconfiguration** | Higher — a missing RLS filter exposes all data | Lower — access to wrong schema is harder | Medium |
| **Recommended for** | Internal analytics platform | Client-facing data access, regulated data | Most global payroll platforms |

### Data Residency Requirements

| Regulation | Requirement | Impact on Architecture |
|-----------|-------------|----------------------|
| **GDPR (EU)** | Personal data of EU residents should be processed with appropriate safeguards; data transfers outside EU require adequacy decisions or SCCs | EU worker data should be stored in EU region; or ensure appropriate transfer mechanisms |
| **India DPDP Act** | Personal data of Indian citizens may have cross-border restrictions under certain categories | Indian worker PII may need to stay in India region or have consent-based transfer |
| **China PIPL** | Personal data must be stored in China unless security assessment passed for cross-border transfer | Chinese worker data likely requires China-based storage |
| **Brazil LGPD** | Similar to GDPR; requires legal basis for processing and cross-border transfer mechanisms | Brazilian worker data should have appropriate safeguards |

### Multi-Tenant Partitioning Strategy

```
silver.worker
  ├── PARTITIONED BY (country_code)        -- First partition: country
  │   ├── country_code=DE/
  │   │   ├── client_id=cli_acme/          -- Second partition: client
  │   │   └── client_id=cli_beta/
  │   ├── country_code=IN/
  │   │   ├── client_id=cli_acme/
  │   │   └── client_id=cli_gamma/
  │   └── country_code=UK/
  │       └── client_id=cli_acme/

Benefits:
  - Country-level queries (common for ops) scan only relevant partitions
  - Client-level isolation is enforced by both partitioning and RLS
  - Data residency can be enforced by storing country partitions in region-specific storage
  - Partition pruning improves query performance significantly
```

### Data Artifacts

| Entity | Key Fields | Analytics Enabled |
|--------|-----------|-------------------|
| Tenant registry | client_id, tenant_type, data_residency_region, isolation_method, created_at | Tenant inventory, residency compliance |
| RLS policy registry | table_name, policy_name, filter_column, filter_logic, applied_roles | Access audit, policy coverage |
| Cross-tenant access log | user_id, query_text, tables_accessed, client_ids_touched, timestamp | Security audit, anomaly detection |
| Data residency map | country_code, storage_region, residency_requirement, compliance_status | Residency compliance monitoring |
| Tenant data volume | client_id, table_name, row_count, storage_bytes, last_updated | Capacity planning, cost allocation |

### Controls

| Control | Type | Description |
|---------|------|-------------|
| **Row-level security on all silver/gold tables** | Preventive | Every query from a client-facing context includes RLS filter on client_id |
| **Quarterly RLS penetration test** | Detective | Security team tests RLS by attempting cross-tenant data access with various role combinations |
| **Cross-tenant query alerting** | Detective | Any single query that accesses data from more than one client_id triggers an alert (except authorized internal analytics roles) |
| **Data residency validation** | Detective | Automated check that data physically resides in the region required by its country_code |
| **Client data access audit log** | Detective | All data access by client-facing applications is logged with client_id, user_id, and query details |
| **Tenant offboarding data purge** | Corrective | When a client churns, all their data is purged from silver/gold within 90 days per contractual and regulatory requirements |

### Metrics

| Metric | Definition | Target | Frequency | Owner |
|--------|-----------|--------|-----------|-------|
| RLS coverage | % of silver/gold tables with RLS policies applied and tested | 100% | Monthly | Data Engineering |
| Cross-tenant access incidents | Number of unauthorized cross-tenant data accesses detected | 0 | Monthly | Security |
| RLS policy test pass rate | % of RLS penetration test scenarios that correctly block unauthorized access | 100% | Quarterly | Security |
| Data residency compliance rate | % of records stored in the correct region per country requirements | 100% | Monthly | Data Governance |
| Tenant isolation audit findings | Number of audit findings related to tenant isolation in the most recent audit | 0 | Per audit | Data Governance |
| Tenant data purge completion rate | % of churned client data purged within contractual timeline | 100% | Per offboarding | Data Engineering |
| Cross-tenant query volume | Number of internal cross-tenant analytical queries per day (authorized) | Track trend | Daily | Analytics Engineering |
| Per-tenant query performance | p95 query latency for client-facing queries, per tenant | < 5 seconds | Daily | Data Engineering |
| Storage cost per tenant | Total storage cost allocated to each client (for cost attribution) | Track trend | Monthly | Finance / Data Engineering |
| Partition skew | Ratio of largest tenant partition to smallest — indicates imbalanced data distribution | < 10:1 | Monthly | Data Engineering |

### Common Failure Modes

| Failure | Consequence | Real-World Example |
|---------|------------|--------------------|
| **Missing RLS filter on new table** | Client-facing API serves unfiltered data; one client sees another's workers | New gold table `client_dashboard_metrics` is created without RLS. The client portal queries it and returns data for all clients. Discovered by a client who notices workers they did not hire. |
| **RLS bypass via raw SQL access** | Internal user with raw SQL access queries silver tables without RLS context | An analyst runs a query on `silver.payslip` without a WHERE clause, exports results to a spreadsheet, and accidentally shares it. All clients' payslip data is in the export. |
| **Data residency violation** | EU worker data replicated to US region for analytics | Platform uses a single US-based data warehouse for all analytics. German client's audit discovers their workers' PII is stored in Virginia. GDPR complaint filed. |
| **Tenant offboarding incomplete** | Churned client's data remains accessible months after contract ends | Client terminates contract. Their worker records are marked inactive in the platform but are never purged from silver/gold tables. A year later, an analyst runs a report that includes the churned client's data. |

### AI Opportunities

| Opportunity | Inputs | Outputs | Guardrails |
|------------|--------|---------|------------|
| **Automated RLS policy generation** | Table schema + tenant model definition | Suggested RLS policies for each table with the correct filter columns and logic | All policies require security team review and penetration testing before deployment |
| **Anomalous access pattern detection** | Query logs, user roles, historical access patterns | Alerts for unusual cross-tenant access patterns (e.g., analyst suddenly querying 50 different clients in one session) | Alert only; no auto-block; route to security team for investigation |
| **Data residency compliance monitoring** | Storage metadata, country-to-region mapping, regulatory database | Automated residency compliance report; flagged violations | Violations require human review and remediation plan; no auto-migration |

### Discovery Questions

1. "How is client data isolated in our current analytics platform? Is it RLS, separate schemas, separate databases, or something else?"
2. "Has there ever been a cross-tenant data exposure incident? If so, what happened and what was changed?"
3. "Where is our data physically stored? Do we have multi-region deployments for data residency compliance?"
4. "When a client churns, what is the process for purging their data from all analytics systems? How long does it take?"
5. "Can our internal analysts access individual client data, or only aggregated cross-tenant data? How is this controlled?"

### Exercises

1. **Multi-tenant design exercise:** Design the multi-tenant architecture for a payroll analytics platform that serves 500 clients across 40 countries. Choose between logical isolation (RLS), physical isolation (separate schemas), or hybrid. Justify your choice. Include: partitioning strategy, RLS implementation approach, data residency solution, and cross-tenant analytics pattern.

2. **RLS testing exercise:** Write 10 test scenarios for validating row-level security on the `silver.worker` table. Each scenario should specify: the user role, the expected behavior (which records should be visible/hidden), and the test query. Include both positive tests (authorized access works) and negative tests (unauthorized access is blocked).

3. **Analytics-leader exercise — Data residency roadmap:** A new regulation requires that all Indian worker PII must be stored in an India-based data center within 12 months. Currently, all data is in a single US-based warehouse. Write a roadmap for compliance: architecture changes needed, migration plan, timeline, cost estimate, and risk mitigation. Present this as a one-page executive brief for the CTO.

---

## Topic 7: Master Data Management — Golden Record, Entity Resolution, and Deduplication

### What It Is

Master Data Management (MDM) for a global payroll platform is the discipline of maintaining a single, authoritative "golden record" for every core entity — especially workers — across all source systems. The same worker may appear in 4-6 different systems: the platform core, the local payroll engine, the client's HRIS, the benefits provider, the banking system, and the support ticketing system. Each system may have slightly different identifiers, different name formats, and different data freshness. MDM answers the question: "Which records across all these systems refer to the same real-world person, and what is the single authoritative version of their data?"

### Why It Matters

**Business impact:**
- Without a golden record, the same worker appears as multiple people in analytics — headcount is overstated, payroll costs are double-counted, and error rates are miscalculated
- Duplicate worker records in the payroll system can result in double payments — paying the same person twice in one month
- When a worker changes their bank account in the platform, but the payment system still has the old account (because the records are not linked), the payment goes to the wrong account
- Regulatory filings that report duplicate workers result in penalties and audit findings
- Client-facing reports that show inconsistent worker counts across different views destroy trust in the platform

### Process Flow — Entity Resolution Pipeline

```
SOURCE SYSTEMS WITH WORKER RECORDS
────────────────────────────────────────────────────────────

Platform Core        Payroll Engine (DE)     Client HRIS (Workday)
┌──────────────┐    ┌──────────────────┐    ┌──────────────────┐
│ wrk_abc123   │    │ P-0789           │    │ EMP-456          │
│ John Smith   │    │ Smith, John      │    │ John A. Smith    │
│ DE           │    │ Steuer-IdNr:     │    │ john.smith@acme  │
│ john@acme.com│    │ 12345678901      │    │ DOB: 1990-03-15  │
└──────┬───────┘    └──────┬───────────┘    └──────┬───────────┘
       │                   │                       │
       ▼                   ▼                       ▼
┌──────────────────────────────────────────────────────────────┐
│                 ENTITY RESOLUTION ENGINE                       │
│                                                               │
│  Step 1: BLOCKING                                             │
│    Group candidates by country + approximate name to reduce   │
│    comparison pairs from N² to manageable clusters             │
│                                                               │
│  Step 2: MATCHING                                             │
│    Deterministic rules (highest priority):                    │
│    ├── Tax ID match (Steuer-IdNr / PAN / NINO) → 100% conf  │
│    ├── Platform ID cross-reference table match → 99% conf     │
│    └── Email + country match → 95% confidence                 │
│                                                               │
│    Probabilistic rules (when deterministic fails):            │
│    ├── Name similarity (Jaro-Winkler > 0.92) + DOB → 90%    │
│    ├── Name similarity + same client + same country → 85%     │
│    └── Address similarity + DOB + gender → 80%                │
│                                                               │
│  Step 3: DECISION                                             │
│    ├── Confidence ≥ 95% → Auto-merge                          │
│    ├── 80% ≤ Confidence < 95% → Human review queue            │
│    └── Confidence < 80% → Treat as separate entities          │
│                                                               │
│  Step 4: GOLDEN RECORD ASSEMBLY                               │
│    For each attribute, select the value from the highest-     │
│    priority source (Platform Core > Payroll Engine > HRIS)    │
│    Store all source values for audit trail                    │
└──────────────────────────────┬───────────────────────────────┘
                               │
                               ▼
                    ┌──────────────────────┐
                    │   GOLDEN RECORD       │
                    │                       │
                    │ golden_id: gld_001    │
                    │ name: John Smith      │  ← from Platform Core (priority 1)
                    │ tax_id: 12345678901   │  ← from Payroll Engine (only source)
                    │ dob: 1990-03-15       │  ← from HRIS (only source)
                    │ email: john@acme.com  │  ← from Platform Core (priority 1)
                    │                       │
                    │ Source links:          │
                    │  platform: wrk_abc123 │
                    │  de_payroll: P-0789   │
                    │  hris: EMP-456        │
                    └──────────────────────┘
```

### Data Artifacts

| Entity | Key Fields | Analytics Enabled |
|--------|-----------|-------------------|
| Golden record | golden_id, canonical attributes, confidence_score, source_links, last_resolved_at | Deduplicated headcount, single view of worker |
| Source-to-golden mapping | source_system, source_id, golden_id, match_method, confidence, matched_at | Cross-system traceability, reconciliation |
| Match candidate queue | candidate_pair_id, source_a, source_b, confidence, status (auto/pending/reviewed), reviewer | Human review backlog, resolution rate |
| Merge/split audit log | action (merge/split), golden_ids_affected, reason, performed_by, timestamp | Audit trail, undo capability |
| Duplicate detection report | potential_duplicates, confidence, affected_entities, created_at | Duplicate discovery, cleanup campaigns |

### Controls

| Control | Type | Description |
|---------|------|-------------|
| **Deterministic match threshold** | Preventive | Auto-merge only when confidence >= 95%; lower confidence requires human review |
| **Merge audit trail** | Detective | Every merge/split action is logged with before/after states, enabling undo |
| **Source priority hierarchy** | Preventive | Documented hierarchy (Platform > Payroll Engine > HRIS > Benefits > Ticketing) for resolving attribute conflicts |
| **Periodic full reconciliation** | Detective | Monthly full reconciliation of golden record count vs. source system counts; discrepancies investigated |
| **Duplicate detection scan** | Detective | Weekly scan for potential duplicates that may have been missed by real-time resolution |
| **Golden record completeness check** | Detective | Every golden record must have: name, country, tax_id (or explicit exception), at least one source link |

### Metrics

| Metric | Definition | Target | Frequency | Owner |
|--------|-----------|--------|-----------|-------|
| Golden record coverage | % of source system records linked to a golden record | > 99% | Daily | Data Engineering |
| Duplicate rate | % of workers with more than one golden record (false negatives in resolution) | < 0.1% | Monthly (audit) | Data Engineering |
| False merge rate | % of golden records that incorrectly merged two distinct workers | < 0.01% | Monthly (audit) | Data Engineering |
| Human review queue depth | Number of candidate pairs awaiting human review | < 50 at any time | Daily | Data Ops |
| Human review resolution time | Average time from queue entry to resolution | < 48 hours | Weekly | Data Ops |
| Source-to-golden match rate by source | % of records from each source system successfully matched to golden records | > 98% per source | Weekly | Data Engineering |
| Attribute conflict rate | % of golden records where source systems disagree on a key attribute (name, DOB, tax_id) | < 2% | Monthly | Data Engineering |
| Cross-system reconciliation pass rate | % of golden records where all linked source records agree on headcount-critical attributes | > 99% | Monthly | Data Engineering |
| Auto-merge rate | % of matches resolved automatically (confidence >= 95%) vs. requiring human review | > 90% | Monthly | Data Engineering |
| Golden record freshness | Age of the oldest unresolved source record not yet linked to a golden record | < 24 hours | Daily | Data Engineering |

### Common Failure Modes

| Failure | Consequence | Real-World Example |
|---------|------------|--------------------|
| **False merge — two different people merged** | Wrong person receives a payslip or payment; data corruption across all linked records | Two "John Smiths" in the same German entity, born in the same year. Probabilistic matcher merges them. One John Smith receives the other's payslip. |
| **Missed merge — same person stays as two records** | Double-counted in headcount; potentially double-paid; confusing analytics | Worker appears as "Priya Sharma" in the platform and "SHARMA PRIYA" (all caps, reversed) in the Indian payroll engine. Name similarity score is 0.78, below the threshold. Two golden records exist for the same person. |
| **Source priority misconfigured** | Wrong attribute value in golden record | HRIS has an outdated salary (pre-raise), but is configured as highest priority for salary. Golden record shows old salary even though the platform has the correct new salary. |
| **Stale golden record** | New hire added to payroll engine but not yet resolved to golden record | Worker onboarded in the Indian payroll engine on March 1. Entity resolution runs daily at midnight. For 24 hours, this worker has no golden record, so they are invisible in analytics. |

### AI Opportunities

| Opportunity | Inputs | Outputs | Guardrails |
|------------|--------|---------|------------|
| **ML-powered entity resolution** | Worker records from all sources, historical match outcomes (training data) | Match predictions with probability scores, feature importance for each match decision | Never auto-merge below 95% confidence; all ML matches must be explainable (which features drove the match) |
| **Name normalization** | Raw names from multiple systems in multiple formats and languages | Normalized name components (first, middle, last) across scripts and formats | Retain original name from each source; normalized name is for matching only, not display |
| **Proactive duplicate detection** | New worker records as they arrive, existing golden records | Real-time alerts when a new record looks like a potential duplicate of an existing golden record | Route to human review; never silently block a new worker record from entering the system |

### Discovery Questions

1. "How many source systems contain worker records? Is there a single worker ID that links them all, or do we rely on matching?"
2. "What is our current duplicate rate? How often do we discover that two records in the analytics platform are actually the same person?"
3. "When two systems disagree about a worker's attribute (e.g., different date of birth), how do we resolve it today?"
4. "Has a false merge ever caused an operational issue — like a payment going to the wrong person? What was the impact?"
5. "Is there a documented source priority hierarchy, or do different teams use different systems as 'source of truth'?"

### Exercises

1. **Entity resolution implementation:** Build a deterministic entity resolution process for workers appearing in 3 systems: Platform Core, Indian Payroll Engine, and Client HRIS. Define: blocking keys, matching rules (deterministic + probabilistic), confidence scoring, merge/manual-review thresholds, and golden record assembly logic. Test with 20 synthetic records (including 3 true duplicates and 2 near-misses that should not merge).

2. **Conflict resolution exercise:** Two systems disagree about a worker's date of birth: the platform says 1990-03-15, the HRIS says 1990-03-05. Design the investigation and resolution process. Who decides? What is the system of record for DOB? How do you prevent this from recurring?

3. **Analytics-leader exercise — MDM business case:** Write a one-page business case for investing in a formal MDM capability. Quantify the cost of not having it: estimated duplicate rate, impact on headcount accuracy, risk of double payments, audit findings. Propose: build vs. buy decision, implementation timeline, expected ROI.

---

## Topic 8: Analytics Layer — Semantic Models, Metrics Layer, and Self-Service BI

### What It Is

The analytics layer sits on top of the gold tables and provides the interface between the data platform and its human consumers — ops managers, finance teams, compliance officers, executives, and client success teams. It consists of three components: the **semantic model** (business-friendly definitions of entities, dimensions, and relationships), the **metrics layer** (formal, governed definitions of every business metric), and the **self-service BI tools** (dashboards, ad-hoc query interfaces, and scheduled reports).

The analytics layer answers the question: "How do we ensure that everyone in the company calculates the same metric the same way, and that non-technical users can access the data they need without filing a ticket every time?"

### Why It Matters

**Business impact:**
- Without a semantic layer, two analysts querying the same data will produce different numbers because they applied different filters, used different definitions of "active worker," or calculated "error rate" differently
- Without self-service BI, the analytics team becomes a bottleneck — every stakeholder request requires an analyst to write a query, build a chart, and deliver it, creating a backlog that frustrates everyone
- Inconsistent metrics undermine trust: when the CFO and the VP of Ops present different numbers in the same board meeting, the board loses confidence in the data
- The metrics layer is the contract between the analytics team and the business: "This is exactly what 'payslip error rate' means, how it is calculated, and where the data comes from."

### Process Flow — Analytics Layer Architecture

```
GOLD LAYER (curated tables)
─────────────────────────────────────────────────────────

 gold.payroll_run_summary    gold.worker_master    gold.payment_ledger
 gold.client_health          gold.compliance_risk   gold.country_metrics
         │                          │                       │
         └──────────────────────────┼───────────────────────┘
                                    │
                                    ▼
┌─────────────────────────────────────────────────────────────────┐
│                    SEMANTIC LAYER                                 │
│                                                                  │
│  ┌──────────────────────────────────────────────────────────┐   │
│  │  DIMENSIONAL MODEL                                        │   │
│  │  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌─────────┐ │   │
│  │  │ dim_worker│  │dim_country│  │dim_client │  │dim_time │ │   │
│  │  │ dim_entity│  │dim_payrun │  │dim_payment│  │         │ │   │
│  │  └──────────┘  └──────────┘  └──────────┘  └─────────┘ │   │
│  └──────────────────────────────────────────────────────────┘   │
│                                                                  │
│  ┌──────────────────────────────────────────────────────────┐   │
│  │  METRICS LAYER (governed metric definitions)              │   │
│  │  ┌───────────────────────────────────────────────────┐   │   │
│  │  │ payslip_error_rate  │ on_time_pay_rate  │ NRR      │   │   │
│  │  │ cost_per_payslip    │ filing_on_time    │ headcount│   │   │
│  │  │ revenue_per_worker  │ churn_risk_score  │ DQ_score │   │   │
│  │  └───────────────────────────────────────────────────┘   │   │
│  └──────────────────────────────────────────────────────────┘   │
│                                                                  │
│  ┌──────────────────────────────────────────────────────────┐   │
│  │  ACCESS POLICIES                                          │   │
│  │  Role-based: who can see what metrics and dimensions      │   │
│  │  Client Ops → their countries only                        │   │
│  │  Finance → all countries, no worker PII                   │   │
│  │  Exec → aggregated only                                   │   │
│  └──────────────────────────────────────────────────────────┘   │
└──────────────────────────────┬──────────────────────────────────┘
                               │
                               ▼
┌──────────────────────────────────────────────────────────────────┐
│                    PRESENTATION / BI LAYER                        │
│                                                                   │
│  ┌────────────┐  ┌────────────┐  ┌─────────────┐  ┌──────────┐ │
│  │ Executive  │  │ Ops        │  │ Client      │  │ Ad-hoc   │ │
│  │ Dashboard  │  │ Dashboard  │  │ Portal      │  │ Query    │ │
│  │ (Tableau / │  │ (Looker /  │  │ (embedded   │  │ (SQL Lab /│ │
│  │  Preset)   │  │  Superset) │  │  analytics) │  │  notebook)│ │
│  └────────────┘  └────────────┘  └─────────────┘  └──────────┘ │
│                                                                   │
│  ┌────────────┐  ┌────────────┐                                  │
│  │ Scheduled  │  │ Slack /    │                                  │
│  │ Reports    │  │ Email      │                                  │
│  │ (PDF/CSV)  │  │ Alerts     │                                  │
│  └────────────┘  └────────────┘                                  │
└──────────────────────────────────────────────────────────────────┘
```

### Metric Contract Template

Every metric in the metrics layer must have a formal contract. This eliminates ambiguity and ensures every consumer calculates the same number.

```yaml
metric_name: payslip_error_rate
display_name: "Payslip Error Rate"
description: >
  Percentage of payslips with at least one material error detected
  after payroll lock and before payment disbursement.
  Excludes: rounding differences (± $0.02 / local equivalent),
  client-input errors (e.g., client submitted wrong salary),
  and informational warnings.
  Includes: calculation errors, missed deductions, wrong tax code
  application, duplicate pay items.

formula: >
  COUNT(DISTINCT payslip_id WHERE has_error = TRUE
    AND error_source NOT IN ('client_input','rounding')
    AND error_severity IN ('critical','high'))
  / COUNT(DISTINCT payslip_id WHERE status IN ('locked','paid','closed'))

dimensions:
  - country_code
  - client_id
  - entity_type (owned / partner)
  - period (month)
  - worker_type (eor_employee / cor_contractor / managed_payroll)
  - run_type (regular / off_cycle / correction)

data_source: gold.payslip_quality

filters:
  exclude: payslips in 'draft' or 'processing' status
  time_range: rolling 12 months available, default = current month

refresh_frequency: daily (after all payroll runs for the day are complete)

owner: Analytics Engineering Team
approved_by: VP Payroll Ops + VP Analytics

thresholds:
  green: < 0.1%
  amber: 0.1% - 0.5%
  red: > 0.5%

history_available_from: 2024-01-01

changelog:
  - date: 2025-06-01
    change: "Excluded client_input errors per ops team request"
  - date: 2025-09-15
    change: "Added run_type dimension for off-cycle visibility"
```

### Core Metric Library

| Metric | Formula Summary | Primary Dimensions | Owner | Refresh |
|--------|----------------|-------------------|-------|---------|
| Payslip error rate | Payslips with error / total payslips (post-lock) | Country, Client, Entity type | Payroll Ops | Daily |
| On-time pay rate | Workers paid on/before scheduled pay date / total workers in run | Country, Pay frequency | Payroll Ops | Per run |
| Filing on-time rate | Filings submitted by due date / total filings due | Country, Filing type | Compliance | Daily |
| Cost per payslip | Total ops cost allocated to payroll / total payslips processed | Country, Entity type | Finance | Monthly |
| Revenue per worker (RWPM) | Total revenue (PEPM + FX + float) / active worker count | Model type, Country | Finance | Monthly |
| Client NPS | Net Promoter Score from client surveys | Client segment, Country | Customer Success | Quarterly |
| Worker NPS | Net Promoter Score from worker surveys | Country, Worker type | Customer Success | Quarterly |
| Gross margin by country | (Revenue - direct costs) / Revenue, per country | Country, Entity type | Finance | Monthly |
| Payroll cycle time | Time from payroll_run.created to payroll_run.paid | Country, Run type | Payroll Ops | Per run |
| Data quality score | Composite DQ score across all dimensions | Country, Entity | Data Engineering | Daily |

### Data Artifacts

| Entity | Key Fields | Analytics Enabled |
|--------|-----------|-------------------|
| Metric registry | metric_name, formula, dimensions, owner, refresh, thresholds | Metric governance, discrepancy investigation |
| Metric value history | metric_name, dimension_values, timestamp, value | Trend analysis, anomaly detection, SLA monitoring |
| Dashboard registry | dashboard_id, title, owner, audience, metrics_used, refresh_schedule | Dashboard inventory, usage tracking |
| Query audit log | user_id, query_text, tables_accessed, execution_time, row_count | Usage analytics, performance optimization |
| Self-service access log | user_id, role, data_accessed, timestamp | Adoption tracking, access audit |

### Controls

| Control | Type | Description |
|---------|------|-------------|
| **Metric contract approval** | Preventive | New metrics or changes to existing metric definitions require sign-off from metric owner and analytics lead |
| **Semantic layer as single source** | Preventive | All dashboards and reports must query through the semantic layer, not directly against gold tables, to enforce consistent definitions |
| **Dashboard certification** | Preventive | Dashboards accessible to executives or clients must be certified (reviewed for accuracy, correct metric definitions, appropriate access controls) |
| **Metric drift detection** | Detective | Automated comparison of metric values calculated via semantic layer vs. direct SQL — any discrepancy triggers investigation |
| **Usage monitoring** | Detective | Track which dashboards and metrics are actually used; sunset unused assets quarterly |

### Metrics

| Metric | Definition | Target | Frequency | Owner |
|--------|-----------|--------|-----------|-------|
| Metric contract coverage | % of KPIs on executive dashboards that have formal metric contracts | 100% | Monthly | Analytics Engineering |
| Metric discrepancy incidents | Number of times two reports showed different values for the same metric | 0 | Monthly | Analytics Engineering |
| Self-service adoption rate | % of data consumers who run at least one self-service query per month | > 60% | Monthly | Analytics Engineering |
| Dashboard certification rate | % of production dashboards that have been certified | > 90% | Monthly | Analytics Engineering |
| Mean time to answer (MTTA) | Average time from stakeholder data request to delivered answer | < 4 hours for standard; < 24 hours for custom | Weekly | Analytics Team |
| Query performance (p95) | 95th percentile query execution time in the semantic layer | < 10 seconds | Daily | Data Engineering |
| Semantic layer uptime | % of time the semantic layer is available for queries | > 99.9% | Monthly | Data Engineering |
| Active dashboard count | Number of dashboards accessed at least once in the past 30 days | Track trend | Monthly | Analytics Engineering |
| Metric refresh SLA adherence | % of metrics refreshed within their defined schedule | > 99% | Daily | Analytics Engineering |
| Data request backlog | Number of pending stakeholder data requests | < 10 | Weekly | Analytics Team |

### Common Failure Modes

| Failure | Consequence | Real-World Example |
|---------|------------|--------------------|
| **No metric contracts — same name, different definitions** | Two teams report different numbers for "error rate" in the same meeting; trust collapses | Ops calculates error rate including all payslips (including drafts). Finance calculates error rate excluding drafts. The CEO sees two charts that disagree by 3x. |
| **Dashboard proliferation without governance** | 200 dashboards exist; nobody knows which ones are current, accurate, or used | Every team has built their own version of the "payroll summary" dashboard. Some use last month's data model. Some use deprecated metric definitions. |
| **Self-service without guardrails** | Non-technical user writes a query that returns PII or incorrect results | A client success manager exports individual worker salaries while building an ad-hoc report. Or a marketing team member calculates headcount incorrectly and publishes the number externally. |
| **Semantic layer not adopted** | Teams bypass the semantic layer and query gold tables directly, reintroducing inconsistency | Data engineering built a semantic layer, but ops analysts find it "too slow" and query gold tables directly. Definitions drift within a month. |

### AI Opportunities

| Opportunity | Inputs | Outputs | Guardrails |
|------------|--------|---------|------------|
| **Natural language query interface** | User question in natural language + semantic model metadata | SQL query + visualization + natural language explanation of results | All queries must go through semantic layer (no direct table access); PII masking applies; results shown with data freshness timestamp |
| **Automated anomaly narration** | Metric values + historical baselines + correlated events | Natural language explanation of why a metric changed ("Germany error rate increased 2x because 3 new workers had incorrect tax class assignments") | Explanations are suggestions; link to source events for verification; never present as definitive root cause |
| **Dashboard recommendation engine** | User role, recent queries, stakeholder type | Suggested dashboards and metrics relevant to the user's role and recent activity | Recommendations only; no auto-creation of dashboards; respect access policies |

### Discovery Questions

1. "When two reports show different numbers for the same metric, what is the current process for resolving the discrepancy?"
2. "How many dashboards do we have in production? How many are actively used? Who certifies them for accuracy?"
3. "Can non-analysts (ops managers, client success) get the data they need without filing a request to the analytics team?"
4. "Is there a single place where all metric definitions are documented, or do different teams maintain their own definitions?"
5. "What is the most common data request you receive that should be self-service but currently is not?"

### Exercises

1. **Metric contract exercise:** Write formal metric contracts (using the template above) for 5 core metrics: payslip error rate, on-time pay rate, filing on-time rate, cost per payslip, and revenue per worker. For each, be precise about inclusions, exclusions, filters, and thresholds. Then have a colleague try to "break" your definition by finding an edge case the contract does not cover.

2. **Dashboard design exercise:** Design the "Payroll Operations Weekly Dashboard" for the VP of Payroll Ops. Include: 8 metrics (with exact definitions), the visualization type for each (line chart, bar chart, KPI card, table), drill-down paths (country → entity → payroll run → payslip), and the data freshness requirement. Mock up the layout as an ASCII diagram or wireframe.

3. **Analytics-leader exercise — Self-service BI strategy:** Write a strategy document for rolling out self-service BI to the ops and finance teams. Include: tool evaluation criteria (Tableau vs. Looker vs. Superset vs. Preset), training plan, governance model (who can publish dashboards, who certifies), semantic layer implementation plan, and success metrics (adoption rate, time-to-insight improvement, analyst backlog reduction).

---

## Topic 9: Business ROI — Quantifying the Return on Data Platform Investment

### What It Is

Business ROI for a data platform is the disciplined practice of measuring the financial return generated by investing in a medallion architecture, canonical data model, event spine, and analytics layer. It translates engineering effort — schema design, pipeline development, data quality rules, and semantic layer implementation — into business terms that CFOs, boards, and executive sponsors understand: cost savings, productivity gains, risk reduction, and revenue enablement. Without this translation, data platform teams struggle to secure ongoing funding and organizational support.

In a global payroll context, the ROI case is built on four pillars: **analyst productivity gains** (fewer hours spent hunting for data, reconciling conflicting numbers, and building one-off reports), **data quality improvement** (fewer payroll errors, fewer compliance violations, fewer client escalations caused by bad data), **self-service adoption** (reduced dependency on the analytics team for routine questions), and **time-to-insight reduction** (faster detection of operational problems and faster delivery of answers to stakeholders).

The challenge is that data platform benefits are often diffuse — they accrue across dozens of teams and hundreds of workflows. A well-built gold layer saves 20 minutes per analyst per day, but that saving is invisible unless you measure it. A data quality rule that catches 50 payroll errors per month prevents client escalations that would have cost $500 each, but nobody tracks the escalations that did not happen. ROI measurement makes the invisible visible.

ROI is not a one-time business case written to get budget approval. It is a continuous measurement practice — tracked monthly, reported quarterly, and used to prioritize the next wave of platform investment. The most effective data platform teams publish an internal "ROI scorecard" alongside their technical health dashboards.

### Why It Matters

**Business impact:**
- Data platform investments typically range from $500K to $2M annually (infrastructure, tooling, headcount). Without demonstrated ROI, these budgets are the first to be cut during cost optimization cycles, leaving operational teams stranded on degrading infrastructure
- Executive sponsors who funded the platform need quantifiable evidence that the investment is paying off — "the data is cleaner" is not sufficient; "we reduced payroll error remediation costs by $180K per year" is
- ROI measurement creates a feedback loop that drives better prioritization: when you know that building a self-service dashboard for client health saves 12 analyst-hours per week, you can compare that against competing priorities on the roadmap

**Operational impact:**
- Teams that cannot quantify their value get absorbed into other functions or lose headcount. Data platform teams that publish ROI scorecards retain budget, grow headcount, and gain organizational influence
- ROI frameworks force clarity about what "success" means for the data platform — not uptime or query speed, but business outcomes enabled by the platform

### Process Flow — Data Platform ROI Measurement Framework

```
DATA PLATFORM INVESTMENT                    BUSINESS VALUE REALIZATION
───────────────────────────                 ──────────────────────────

┌──────────────────────┐                   ┌──────────────────────────┐
│  COST INPUTS          │                   │  VALUE OUTPUTS            │
│                       │                   │                           │
│  Infrastructure       │                   │  Analyst productivity     │
│  ├─ Cloud compute     │                   │  ├─ Hours saved/analyst   │
│  ├─ Storage           │                   │  ├─ Ad-hoc requests       │
│  └─ Tooling licenses  │                   │  │   eliminated           │
│                       │                   │  └─ Self-service queries  │
│  Headcount            │                   │                           │
│  ├─ Data engineers    │     ────────►     │  Data quality gains       │
│  ├─ Analytics engrs   │                   │  ├─ Errors prevented      │
│  └─ Platform ops      │                   │  ├─ Escalations avoided   │
│                       │                   │  └─ Rework eliminated     │
│  Implementation       │                   │                           │
│  ├─ Migration effort  │                   │  Speed-to-insight         │
│  ├─ Training          │                   │  ├─ Time-to-answer        │
│  └─ Opportunity cost  │                   │  └─ Decision cycle time   │
│                       │                   │                           │
│  Ongoing operations   │                   │  Risk reduction           │
│  ├─ Maintenance       │                   │  ├─ Compliance penalties  │
│  ├─ Support           │                   │  │   avoided              │
│  └─ Upgrades          │                   │  └─ Audit readiness       │
└──────────────────────┘                   └──────────────────────────┘
         │                                            │
         ▼                                            ▼
┌──────────────────────────────────────────────────────────────────────┐
│                     ROI CALCULATION                                    │
│                                                                       │
│  3-Year ROI = (Total Quantified Benefits − Total Cost) / Total Cost  │
│  Payback Period = Total Cost / Annual Net Benefit                     │
│  NPV = Σ (Annual Net Benefit / (1 + discount_rate)^year) − Initial   │
└──────────────────────────────────────────────────────────────────────┘
```

### Worked example — ROI of building a medallion architecture data platform

**Scenario:** A mid-size EOR (3,000 workers, 18 countries, 15 analysts) is replacing a patchwork of spreadsheets, ad-hoc SQL scripts, and manual CSV exports with a proper medallion architecture data platform.

**Cost side (3-year total cost of ownership):**

| Cost category | Year 1 | Year 2 | Year 3 | 3-Year Total |
|--------------|--------|--------|--------|-------------|
| Cloud infrastructure (Databricks/Snowflake, storage, compute) | $120,000 | $130,000 | $140,000 | $390,000 |
| Data engineering headcount (2 FTEs × $150K loaded) | $300,000 | $300,000 | $300,000 | $900,000 |
| Analytics engineering (1 FTE × $140K loaded) | $140,000 | $140,000 | $140,000 | $420,000 |
| Tooling licenses (dbt, orchestration, BI tool) | $40,000 | $45,000 | $50,000 | $135,000 |
| Migration and implementation (one-time) | $150,000 | — | — | $150,000 |
| Training and change management | $30,000 | $10,000 | $10,000 | $50,000 |
| **Total cost** | **$780,000** | **$625,000** | **$640,000** | **$2,045,000** |

**Benefit side (3-year quantified benefits):**

| Benefit category | Calculation | Annual Value | 3-Year Total |
|-----------------|-------------|-------------|-------------|
| Analyst productivity (15 analysts × 6 hrs saved/week × 50 weeks × $70/hr) | Time previously spent finding data, reconciling numbers, building manual reports | $315,000 | $945,000 |
| Ad-hoc request reduction (200 requests/month reduced to 60 via self-service × $150 avg cost) | Ops and finance teams can self-serve routine questions | $252,000 | $756,000 |
| Data quality improvement (50 payroll errors prevented/month × $500 avg remediation cost) | Gold layer quality rules catch errors before payroll runs | $300,000 | $900,000 |
| Compliance risk reduction (estimated 2 audit findings avoided/year × $75K avg penalty) | Proper lineage, retention, and audit trails | $150,000 | $450,000 |
| Client escalation reduction (30 data-related escalations avoided/year × $2,000 avg cost) | Accurate, consistent reporting eliminates "your numbers are wrong" escalations | $60,000 | $180,000 |
| Faster decision-making (qualitative — not included in hard ROI) | Decisions made in hours instead of days | Not quantified | Not quantified |
| **Total quantified benefits** | | **$1,077,000** | **$3,231,000** |

**ROI summary:**

| Metric | Value |
|--------|-------|
| 3-Year Total Cost | $2,045,000 |
| 3-Year Total Benefit | $3,231,000 |
| 3-Year Net Benefit | $1,186,000 |
| 3-Year ROI | 58% |
| Annual Net Benefit (steady state, Year 2+) | $452,000 |
| Payback Period | 18.5 months |

### Data Artifacts

| Artifact | Source | Format | Grain | SLA |
|----------|--------|--------|-------|-----|
| `roi.platform_cost_ledger` | Finance system, cloud billing APIs | Iceberg/Delta | Monthly per cost category | Refreshed by 5th business day |
| `roi.analyst_time_survey` | Monthly analyst survey + query log analysis | Parquet | Per analyst per month | Collected monthly |
| `roi.self_service_adoption` | BI tool usage logs + semantic layer query logs | Delta | Per user per day | Daily refresh |
| `roi.dq_error_prevention_log` | Data quality framework alert history | Delta | Per rule per run | Real-time |
| `roi.escalation_tracker` | Client success CRM + support tickets | Parquet | Per escalation | Daily refresh |
| `roi.roi_scorecard_snapshot` | Calculated from above artifacts | Delta | Monthly snapshot | Published by 10th business day |

### Controls

| Control | Type | Implementation |
|---------|------|----------------|
| **Cost baseline validation** | Preventive | Before-and-after cost comparison must use consistent methodology; finance team reviews and signs off on baseline assumptions |
| **Benefit double-counting prevention** | Detective | Each benefit line item must map to exactly one measurement source; cross-check that analyst time savings and ad-hoc reduction are not counting the same hours |
| **Survey bias correction** | Detective | Analyst time-savings surveys are cross-validated against actual query log data; self-reported savings exceeding query log evidence are discounted by 50% |
| **Quarterly ROI review** | Governance | CFO and data platform lead review ROI scorecard quarterly; adjust assumptions based on actuals; reforecast remaining period |
| **Benefit realization tracking** | Detective | Each projected benefit has a named owner responsible for measuring and reporting actuals vs. forecast |

### Metrics

| Metric | Definition | Target | Frequency | Owner |
|--------|-----------|--------|-----------|-------|
| Analyst hours saved per week | Average hours saved per analyst per week (survey + query log proxy) | > 5 hours | Monthly | Analytics Engineering |
| Self-service adoption rate | % of routine data questions answered without analyst involvement | > 60% | Monthly | Analytics Engineering |
| Ad-hoc request volume | Number of ad-hoc data requests submitted to the analytics team per month | < 60 (from baseline of 200) | Monthly | Analytics Team |
| Data quality errors prevented | Number of payroll errors caught by gold layer quality rules before reaching production | > 50/month | Monthly | Data Engineering |
| Mean time to answer (MTTA) | Average time from stakeholder data request to delivered answer | < 4 hours (from baseline of 2 days) | Weekly | Analytics Team |
| Platform cost per worker | Total annual platform cost / number of active workers on platform | < $200 | Quarterly | Data Engineering |
| ROI realization rate | Actual quantified benefits / projected benefits (from business case) | > 80% | Quarterly | Data Platform Lead |
| Payback period tracking | Rolling calculation of months until cumulative benefits exceed cumulative costs | < 24 months | Quarterly | Finance + Data Platform Lead |

### Common Failure Modes

| Failure | Consequence | Real-World Example |
|---------|------------|--------------------|
| **Overstating benefits in the business case** | When actuals fall short, executive trust is lost and future investment is blocked | Business case projected 10 hours saved per analyst per week, but actual savings were 4 hours because analysts spent the freed-up time on new work that was not tracked |
| **Not tracking benefits after launch** | The platform is perceived as a cost center because nobody can prove it delivered value | Two years after launch, the CFO asks "what did we get for that $2M?" and nobody has data to answer |
| **Ignoring the cost of change management** | Analysts revert to old workflows (spreadsheets, direct SQL) because training was insufficient, negating projected benefits | Self-service adoption stays at 15% because ops managers were never trained on the BI tool |
| **Counting avoided costs that cannot be verified** | ROI includes "$500K in compliance penalties avoided" but there is no evidence the penalties would have actually occurred | The ROI scorecard claims credit for avoiding GDPR fines, but the company has never received a GDPR fine, making the claim unfalsifiable |
| **Platform cost creep without visibility** | Cloud costs grow 30% year-over-year while the team reports the same ROI numbers, eroding actual returns | Nobody monitors Databricks compute costs; a poorly optimized pipeline runs 10x more compute than necessary |

#### AI Opportunities

- **Automated ROI narration:** An LLM agent ingests the ROI scorecard data, compares actuals to forecasts, and generates a monthly narrative summary for executive consumption — highlighting where benefits are tracking ahead or behind plan, with root-cause hypotheses for variances
- **Predictive cost modeling:** ML models trained on historical cloud billing data predict next-quarter infrastructure costs based on worker count growth, query volume trends, and planned pipeline additions — enabling proactive budget management instead of reactive cost surprises
- **Benefit attribution engine:** A classification model analyzes analyst activity logs, query patterns, and ticket volumes to automatically attribute time savings and efficiency gains to specific platform features (e.g., "the new self-service dashboard for client health saved 8 analyst-hours this week")

### Discovery Questions

1. "Do we currently track the total cost of our data infrastructure — including headcount, tooling, and cloud spend — in a single view?"
2. "How many hours per week do our analysts spend finding, cleaning, or reconciling data rather than analyzing it?"
3. "What percentage of data requests from ops and finance are routine questions that could be answered by a self-service dashboard?"
4. "When was the last time we quantified the cost of a payroll error — including remediation, client communication, and compliance risk?"
5. "If the CFO asked today 'What is the ROI of our data platform?', could we answer with specific numbers?"

### Exercises

1. **ROI business case exercise:** Using the template above, build a 3-year ROI business case for your organization's data platform investment. Replace the example numbers with your actual costs (or best estimates). Interview 5 analysts to estimate time savings. Pull actual ad-hoc request volume from your ticketing system. Calculate your specific payback period and present it to your manager.

2. **Benefit measurement design exercise:** For each of the 5 benefit categories in the worked example, design the specific measurement mechanism: What data will you collect? How will you collect it? What is the baseline? What is the target? Who is responsible for reporting? Document this as a "Benefit Realization Plan" with one page per benefit category.

3. **ROI scorecard exercise:** Design a one-page monthly ROI scorecard for the data platform. Include: cost tracking (actual vs. budget), benefit tracking (actual vs. forecast), key metrics (analyst hours saved, self-service adoption, DQ errors prevented), and a traffic-light status for each benefit line item. Mock up the layout and present it to a colleague for feedback.

---

## Topic 10: Data Governance and Privacy — GDPR, Classification, Retention, Right to Erasure

### What It Is

Data governance in a global payroll platform is the framework of policies, processes, and controls that ensures data is managed responsibly — classified by sensitivity, retained for the appropriate duration, protected from unauthorized access, and erasable when required by law or contract. This is not an abstract compliance exercise — payroll data is among the most sensitive personal data any company holds, and the consequences of governance failures include regulatory fines, lawsuits, loss of client trust, and reputational damage.

The key regulations that drive governance requirements are GDPR (EU), LGPD (Brazil), DPDP Act (India), PIPL (China), and various national data protection laws. While the details vary, all share common principles: lawful basis for processing, data minimization, purpose limitation, retention limits, and data subject rights (access, correction, erasure).

### Why It Matters

**Business impact:**
- GDPR fines can reach up to 4% of global annual revenue or EUR 20 million, whichever is higher — a potentially existential penalty for a mid-stage company
- A single "right to erasure" request that is not properly fulfilled can trigger a complaint to a data protection authority, leading to investigation and potential enforcement action
- Clients conducting due diligence before signing will audit the EOR's data governance practices — weak governance loses enterprise deals
- Worker trust depends on knowing their sensitive data (salary, tax ID, bank account) is properly protected
- An auditor finding unclassified PII in a gold analytics table accessible to 50 users is a material finding that can block a SOC 2 certification

### Process Flow — Data Governance Lifecycle

```
┌───────────────────────────────────────────────────────────────────┐
│                  DATA GOVERNANCE LIFECYCLE                         │
│                                                                    │
│  ┌────────────┐  ┌────────────┐  ┌────────────┐  ┌────────────┐ │
│  │ CLASSIFY    │  │ PROTECT     │  │ RETAIN      │  │ ERASE       │ │
│  │             │  │             │  │             │  │             │ │
│  │ Tag every   │  │ Apply       │  │ Enforce     │  │ Honor       │ │
│  │ column with │  │ access      │  │ retention   │  │ erasure     │ │
│  │ sensitivity │  │ controls,   │  │ policies    │  │ requests    │ │
│  │ level and   │  │ masking,    │  │ per country │  │ per GDPR    │ │
│  │ PII type    │  │ encryption  │  │ and data    │  │ Art. 17     │ │
│  │             │  │             │  │ category    │  │             │ │
│  └─────┬──────┘  └─────┬──────┘  └─────┬──────┘  └─────┬──────┘ │
│        │               │               │               │         │
│        ▼               ▼               ▼               ▼         │
│  ┌─────────────────────────────────────────────────────────────┐ │
│  │                    AUDIT & MONITOR                           │ │
│  │  - Classification coverage audit (quarterly)                 │ │
│  │  - Access control review (monthly)                           │ │
│  │  - Retention policy compliance check (monthly)               │ │
│  │  - Erasure request fulfillment tracking (per request)        │ │
│  │  - PII access logging and anomaly detection (continuous)     │ │
│  └─────────────────────────────────────────────────────────────┘ │
└───────────────────────────────────────────────────────────────────┘
```

### Data Classification Framework

| Classification Level | Definition | Examples in Payroll | Access | Masking |
|---------------------|-----------|-------------------|--------|---------|
| **Restricted** | Highly sensitive PII; unauthorized access causes severe harm | Tax IDs (PAN, Steuer-IdNr, NINO), bank account numbers, salary amounts, social security IDs (Aadhaar, SV-Nummer) | Named individuals with explicit approval only | Full mask for all unauthorized roles; partial mask for ops (last 4 digits visible) |
| **Confidential** | Sensitive personal or business data | Worker names, email addresses, dates of birth, employment contracts, payslip details | Role-based access; need-to-know basis | Pseudonymize for analytics; aggregate for reporting |
| **Internal** | Business data not intended for external sharing | Payroll run summaries, country metrics, pipeline health, internal KPIs | All internal employees with appropriate role | No masking needed internally; mask for external/client views |
| **Public** | Non-sensitive, shareable externally | Aggregate headcount by region (no country detail), published pricing, general compliance certifications | No restrictions | None |

### PII Inventory and Masking Rules

| PII Type | Tables Containing It | Masking for Ops Analyst | Masking for ML Engineer | Masking for Client Portal | Retention |
|----------|---------------------|------------------------|------------------------|--------------------------|-----------|
| **Full name** | worker, payment, payslip | Visible (needed for ops) | Pseudonymized (Worker_ABC) | Visible (their workers only) | Duration of employment + country retention period |
| **Tax ID** | worker | Partial mask (XXXXX1234X) | Hashed (SHA-256) | Not visible | Employment + 7-10 years per country |
| **Bank account** | payment, worker | Partial mask (last 4 digits) | Not available | Not visible | Employment + 7 years |
| **Salary** | contract, payslip, pay_item | Visible (needed for ops) | Available (needed for models) | Visible (their workers only) | Employment + 7-10 years per country |
| **Date of birth** | worker | Visible | Year only (age bucket) | Not visible | Employment + retention period |
| **Email** | worker | Visible | Hashed | Visible (their workers only) | Employment + 1 year post-offboarding |
| **Address** | worker | City/country only | Country only | Not visible | Employment + 1 year post-offboarding |
| **Social security ID** | worker | Partial mask | Hashed | Not visible | Employment + 10 years |

### Retention Policy by Country

| Country | Payroll Records | Tax Records | Employment Contracts | Statutory Filing Records |
|---------|----------------|-------------|---------------------|------------------------|
| **Germany** | 10 years | 10 years | 10 years after termination | 10 years |
| **UK** | 6 years | 6 years | 6 years after termination | 6 years |
| **India** | 8 years | 8 years | 8 years after termination | 8 years |
| **France** | 5 years | 6 years | 5 years after termination | 10 years |
| **Brazil** | 10 years (FGTS) | 5 years | 5 years after termination | 30 years (some categories) |
| **Singapore** | 5 years | 5 years | 5 years after termination | 5 years |
| **UAE** | 5 years | Not applicable (no income tax) | 2 years after termination | 5 years |

### Right to Erasure Process

```
GDPR ARTICLE 17 — RIGHT TO ERASURE ("RIGHT TO BE FORGOTTEN")
═════════════════════════════════════════════════════════════════

Worker (data subject) submits erasure request
    │
    ▼
┌──────────────────────────────────────────────────────┐
│ Step 1: VALIDATE REQUEST                              │
│  - Verify identity of requester                       │
│  - Confirm data subject has right to request erasure  │
│  - Check for exemptions:                              │
│    ├── Legal obligation to retain? (tax records)      │
│    ├── Ongoing legal dispute?                         │
│    └── Public interest or archiving purposes?         │
│                                                       │
│  Timeline: acknowledge within 72 hours                │
└──────────────────────┬───────────────────────────────┘
                       │
    ┌──────────────────┼──────────────────┐
    │                  │                  │
    ▼                  ▼                  ▼
EXEMPT (partial)    FULLY ERASABLE     DENIED (with reason)
    │                  │
    ▼                  ▼
┌──────────────┐  ┌──────────────────────────────────────┐
│ Erase non-   │  │ Step 2: EXECUTE ERASURE               │
│ exempt data; │  │  - Identify ALL systems containing    │
│ retain exempt│  │    this worker's data (PII inventory) │
│ data with    │  │  - Bronze: anonymize (cannot delete   │
│ legal basis  │  │    from immutable store — replace PII │
│ documented   │  │    with anonymized values)             │
│              │  │  - Silver: delete or anonymize records │
└──────────────┘  │  - Gold: re-aggregate without worker  │
                  │  - Event store: anonymize PII in       │
                  │    event payloads                       │
                  │  - Backups: mark for exclusion from     │
                  │    future restores                      │
                  │                                        │
                  │ Timeline: complete within 30 days       │
                  └──────────────────┬─────────────────────┘
                                     │
                                     ▼
                  ┌──────────────────────────────────────┐
                  │ Step 3: VERIFY AND CONFIRM            │
                  │  - Run PII scan across all systems    │
                  │  - Confirm no residual PII remains    │
                  │  - Log erasure completion with audit   │
                  │    trail (what was erased, when, by    │
                  │    whom, legal basis for any retained) │
                  │  - Notify data subject of completion   │
                  └──────────────────────────────────────┘
```

### Data Artifacts

| Entity | Key Fields | Analytics Enabled |
|--------|-----------|-------------------|
| Data classification catalog | table_name, column_name, classification_level, pii_type, owner | Classification coverage audit |
| Retention policy registry | country_code, data_category, retention_years, legal_basis, review_date | Retention compliance monitoring |
| Erasure request log | request_id, worker_id, request_date, status, completion_date, exemptions | Erasure SLA tracking |
| PII access audit log | user_id, table_name, column_name, access_type, timestamp | Access pattern monitoring |
| Data processing record (ROPA) | processing_activity, legal_basis, data_categories, recipients, retention | GDPR Article 30 compliance |

### Controls

| Control | Type | Description |
|---------|------|-------------|
| **Column-level classification tags** | Preventive | Every column in silver/gold tables has a classification tag; unclassified columns trigger an alert |
| **Dynamic data masking** | Preventive | PII columns are automatically masked based on the querying user's role; no manual masking required |
| **Retention enforcement automation** | Preventive | Automated jobs delete or archive data past its retention date per country policy |
| **Erasure verification scan** | Detective | After every erasure execution, an automated scan checks for residual PII across all systems |
| **PII access anomaly detection** | Detective | Alerts when a user accesses PII at unusual volume, frequency, or outside their normal access pattern |
| **Quarterly governance review** | Detective | Quarterly review of classification coverage, retention compliance, erasure SLA adherence, and access audit findings |

### Metrics

| Metric | Definition | Target | Frequency | Owner |
|--------|-----------|--------|-----------|-------|
| Classification coverage | % of columns in silver/gold tables with assigned classification level | 100% | Monthly | Data Governance |
| PII inventory completeness | % of known PII-containing columns documented in the PII inventory | 100% | Monthly | Data Governance |
| Erasure request fulfillment rate | % of valid erasure requests completed within 30 days | 100% | Per request | Data Governance |
| Average erasure fulfillment time | Average days from valid request to completion | < 15 days | Monthly | Data Governance |
| Retention policy compliance | % of data assets compliant with their country-specific retention policy | 100% | Monthly | Data Governance |
| PII access audit log coverage | % of PII-containing tables with access logging enabled | 100% | Monthly | Security |
| Unauthorized PII access incidents | Number of PII access events that violated access policies | 0 | Monthly | Security |
| ROPA (Record of Processing Activities) completeness | % of processing activities documented per GDPR Article 30 | 100% | Quarterly | Legal / Data Governance |
| Data masking effectiveness | % of PII fields correctly masked for unauthorized roles (tested via audit queries) | 100% | Quarterly | Data Engineering |
| Governance training completion | % of data platform users who completed data governance training | 100% | Annually | Data Governance |

### Common Failure Modes

| Failure | Consequence | Real-World Example |
|---------|------------|--------------------|
| **PII in derived/gold tables** | Sensitive data exposed to users who should only see aggregates | A gold table aggregating payroll costs by department includes a `worker_names` column (comma-separated list) for "drill-down convenience." This table is accessible to 50 internal users who should not see individual names. |
| **Retention policy not automated** | Data retained beyond legal requirement; unnecessary liability | German payroll records from 2013 (13 years old) still in silver tables. The 10-year retention period expired 3 years ago. If breached, the company is liable for data it should have deleted. |
| **Erasure request lost in process** | Worker's data not erased within 30 days; regulatory complaint filed | Erasure request arrives via support ticket. Ticket is closed as "resolved" after removing the worker from the platform UI. Nobody checks the analytics platform, event store, or backups. PII persists in 5+ systems. |
| **No classification on new tables** | New tables created without classification; PII exposed by default | Data engineer creates a new staging table with full worker PII for a one-time analysis. Table is never classified, never masked, and persists for 2 years. |

### AI Opportunities

| Opportunity | Inputs | Outputs | Guardrails |
|------------|--------|---------|------------|
| **Automated PII detection** | Column names, sample data values, data types | Classification suggestions (column X looks like a tax_id based on pattern matching) | Suggestions only; require data governance team approval; never auto-classify as non-PII |
| **Erasure completeness verification** | PII inventory, erasure execution log, system scan results | Verification report confirming all instances of the worker's PII have been erased or anonymized | Automated scan; human sign-off required before confirming completion to the data subject |
| **Retention policy alerting** | Data ages across all tables, retention policy registry | Alerts for data approaching or exceeding retention limits, with recommended actions | Alert only; never auto-delete without human approval (retention requirements may have changed) |

### Discovery Questions

1. "Do we have a complete PII inventory — do we know every table and column that contains personally identifiable information?"
2. "How many right-to-erasure requests have we received in the past year? What is our average fulfillment time? Have we ever missed the 30-day deadline?"
3. "Are retention policies automated, or do we rely on manual cleanup? When was the last time old data was actually purged?"
4. "Has data governance ever been raised as a finding in a SOC 2 audit or client security assessment? What was the finding?"
5. "How do we handle PII in our analytics platform — is there dynamic masking, or do we rely on access controls alone?"

### Exercises

1. **PII audit exercise:** Conduct a PII audit of the canonical data model from Topic 2. For every entity, identify every column that contains PII, classify it (Restricted / Confidential / Internal / Public), define the masking rule for each role (Ops Analyst, ML Engineer, Executive, Client Portal), and document the retention period per country.

2. **Erasure implementation exercise:** Design the technical implementation of the right-to-erasure process for the data platform. For each layer (bronze, silver, gold, event store), define: what action is taken (delete, anonymize, exclude from backups), how referential integrity is maintained after erasure, and how you verify completeness. Then simulate the erasure of a single German worker and document every system that was modified.

3. **Analytics-leader exercise — Data governance charter:** Write a Data Governance Charter for the payroll data platform. Include: governance objectives, governance council membership (who has a seat), decision-making process for classification disputes, cadence of governance reviews, escalation path for governance violations, and success metrics. Present this as a document the VP of Compliance and CTO would co-sign.

---

## Topic 11: Technology Stack Recommendations — Tools for Each Layer

### What It Is

This topic provides specific, opinionated technology recommendations for each layer of the data platform — from ingestion through presentation. These are not the only valid choices, but they represent battle-tested options appropriate for a global payroll platform at different maturity stages. The goal is not to prescribe a single stack but to give you a framework for evaluating and selecting tools, with specific recommendations for where to start.

### Why It Matters

**Business impact:**
- Choosing the right tools at each stage avoids painful and expensive migrations later (moving from a warehouse to a lakehouse after 2 years of investment is a 6-12 month project)
- Over-engineering the stack for a 500-worker startup wastes money and engineering time; under-engineering for a 50,000-worker platform creates bottlenecks that block business growth
- Tool choices affect hiring — if you choose a niche tool, you will struggle to hire engineers who know it
- Integration between tools matters as much as individual tool quality — a best-of-breed stack with poor integration is worse than a good-enough integrated stack

### Process Flow — Technology Stack by Layer

```
LAYER              STARTUP (500)         GROWTH (5,000)          SCALE (50,000+)
═══════════════════════════════════════════════════════════════════════════════════

INGESTION          Fivetran / Airbyte    Fivetran + custom       Fivetran + custom +
                   (managed connectors)  Spark Streaming jobs    Kafka for real-time
                                         for CDC                 CDC via Debezium

ORCHESTRATION      Airflow (managed:     Airflow (managed:       Airflow or Dagster
                   MWAA / Astronomer)    MWAA / Astronomer)      (self-hosted for
                   OR Dagster Cloud      + Dagster for data      control) + custom
                                         pipelines               scheduler for SLA

STORAGE            S3 / GCS              S3 / GCS with           S3 / GCS with
                   (Parquet files)       Delta Lake / Iceberg    Delta Lake / Iceberg
                                         table format            + region-specific
                                                                  storage for residency

COMPUTE            Databricks OR         Databricks OR           Databricks OR
(Processing)       Snowflake             Snowflake +             Spark on K8s +
                                         Spark for heavy ETL     Snowflake for serving

TRANSFORMATION     dbt Core or           dbt Core or Cloud       dbt Core + custom
                   dbt Cloud             (500+ models)           Spark jobs for
                                                                  complex transforms

DATA QUALITY       dbt tests +           Great Expectations      Great Expectations +
                   basic assertions      + dbt tests             Monte Carlo / Soda
                                                                  for observability

EVENT BUS          AWS EventBridge       Kafka (managed:         Kafka (self-managed
                   OR simple SQS         Confluent or MSK)       or Confluent) +
                                                                  Schema Registry

SEMANTIC LAYER     dbt metrics +         dbt Semantic Layer      dbt Semantic Layer +
                   direct SQL            or Cube.dev             Cube.dev or AtScale

BI / REPORTING     Preset (Superset)     Looker or Tableau       Tableau / Looker +
                   OR Metabase           + Preset for internal   embedded analytics
                                                                  for client portal

ML PLATFORM        Notebooks             MLflow + Feature        MLflow + Feature
                   (Databricks / Colab)  Store (Databricks       Store + model
                                         or Feast)               serving (SageMaker
                                                                  or Databricks)

DATA CATALOG       dbt docs + wiki       DataHub or Atlan        DataHub or Atlan +
                                                                  custom data product
                                                                  catalog

ACCESS CONTROL     Cloud IAM +           Cloud IAM + Unity       Unity Catalog or
                   manual RBAC           Catalog (Databricks)    Snowflake RBAC +
                                         or Snowflake RBAC       dynamic masking

MONITORING         CloudWatch /          Datadog + PagerDuty     Datadog + PagerDuty
                   basic alerts          for pipeline alerts     + Monte Carlo for
                                                                  data observability
```

### Tool Evaluation Criteria

| Criterion | Weight | Questions to Ask |
|-----------|--------|-----------------|
| **Payroll domain fit** | High | Does it handle PII natively (masking, classification)? Does it support multi-region deployment for data residency? |
| **Scale trajectory** | High | Will this tool still work when we 10x our data volume? What is the migration cost if it does not? |
| **Integration** | High | Does it integrate with our existing stack (cloud provider, orchestrator, BI tool)? |
| **Team expertise** | Medium | Do we have engineers who know this tool? Can we hire for it? |
| **Total cost of ownership** | Medium | License + compute + engineering time to maintain. Beware of tools with low license cost but high operational overhead. |
| **Managed vs. self-hosted** | Medium | At our current team size, can we afford to self-host? When does self-hosting become worth the control? |
| **Vendor stability** | Low-Medium | Is the vendor well-funded? Is there an active open-source community (for OSS tools)? |
| **Compliance certifications** | Medium | SOC 2 Type II? ISO 27001? GDPR-compliant data processing agreement? |

### Recommended Stack: "Start Here" for a Growth-Stage EOR (5,000 Workers)

| Layer | Recommended Tool | Why This One | Approximate Monthly Cost |
|-------|-----------------|-------------|------------------------|
| Cloud | AWS or GCP | Both well-supported; AWS has broader partner ecosystem | Base infrastructure: $5-10K |
| Ingestion | Fivetran (managed) + Debezium for platform CDC | Fivetran handles 80% of sources out of the box; Debezium for real-time on your own DB | $2-5K (Fivetran) + minimal (Debezium OSS) |
| Storage | S3 + Delta Lake (via Databricks) | Immutable bronze storage in S3; Delta for silver/gold with ACID transactions and time travel | $1-3K (storage) |
| Compute | Databricks | Unified analytics platform; supports Spark, SQL, ML; Unity Catalog for governance | $10-20K |
| Orchestration | Dagster Cloud or MWAA (managed Airflow) | Dagster for data-aware orchestration; MWAA if team already knows Airflow | $1-3K |
| Transformation | dbt Cloud | SQL-based transformations; excellent testing; growing semantic layer | $2-5K |
| Data Quality | Great Expectations + dbt tests | GE for complex DQ rules; dbt tests for schema-level checks | OSS (GE) + included in dbt |
| Event Bus | Confluent Cloud (managed Kafka) | Production-grade event streaming with schema registry | $2-5K |
| BI | Preset (managed Superset) + Tableau for exec | Preset for internal self-service (cost-effective); Tableau for polished exec dashboards | $1-2K (Preset) + $5-10K (Tableau) |
| Data Catalog | DataHub (OSS) or Atlan | DataHub if you have eng capacity to self-host; Atlan if you want managed | OSS or $3-5K (Atlan) |
| ML Platform | Databricks ML + MLflow | Already part of Databricks; integrated with compute and storage | Included in Databricks |
| Monitoring | Datadog + PagerDuty | Datadog for pipeline metrics; PagerDuty for on-call alerting | $2-5K |
| **Estimated Total** | | | **$30-70K/month** |

### Data Artifacts

| Entity | Key Fields | Analytics Enabled |
|--------|-----------|-------------------|
| Technology inventory | tool_name, layer, version, license_type, cost, owner, renewal_date | Cost tracking, license management |
| Integration map | source_tool, target_tool, integration_type, data_flow, latency | Dependency mapping, failure impact analysis |
| Tool evaluation log | tool_name, evaluation_date, criteria_scores, decision, rationale | Decision audit trail |
| Migration plan registry | current_tool, target_tool, migration_reason, timeline, status | Migration tracking |
| Cost allocation | tool_name, cost_per_month, allocated_to (team/project), cost_per_worker | Unit economics of data platform |

### Controls

| Control | Type | Description |
|---------|------|-------------|
| **Approved tool list** | Preventive | Only tools on the approved list may be used in production; exceptions require architecture review board approval |
| **Cost monitoring and alerts** | Detective | Monthly cost monitoring per tool; alerts when spend exceeds budget by 20%+ |
| **Vendor security review** | Preventive | Every new tool must pass security review (SOC 2, data processing agreement, encryption standards) before adoption |
| **Architecture review for new tools** | Preventive | Any new tool introduction requires a one-page proposal reviewed by the architecture board |
| **Annual stack review** | Detective | Annual review of the full technology stack — identify tools to consolidate, upgrade, or replace |

### Metrics

| Metric | Definition | Target | Frequency | Owner |
|--------|-----------|--------|-----------|-------|
| Data platform total cost | Sum of all tool costs (license + compute + storage + engineering time) | Track vs. budget | Monthly | Data Engineering |
| Cost per worker per month | Total platform cost / active worker count | < $5 at growth stage; < $2 at scale | Monthly | Data Engineering |
| Tool uptime (per tool) | % of time each production tool is available | > 99.9% | Monthly | Data Engineering |
| Integration failure rate | % of tool-to-tool integrations that experience failures | < 1% | Monthly | Data Engineering |
| Time to onboard new tool | Average time from decision to production deployment for new tools | < 4 weeks | Per instance | Data Engineering |
| Engineering time on maintenance vs. building | % of data engineering time spent maintaining existing tools vs. building new capabilities | < 40% maintenance | Monthly | Data Engineering |
| Vendor lock-in risk score | Qualitative assessment of migration difficulty for each critical tool (1-5 scale) | < 3 average | Annually | Architecture |
| Stack satisfaction (team survey) | Team rating of tool quality and productivity | > 4 out of 5 | Quarterly | Data Engineering |

### Common Failure Modes

| Failure | Consequence | Real-World Example |
|---------|------------|--------------------|
| **Over-engineering at startup stage** | Team of 3 spends 6 months building a Kafka + Spark + Kubernetes stack instead of shipping dashboards | A 500-worker EOR invests in a fully self-hosted Kubernetes-based data platform. The 2 data engineers spend all their time managing infrastructure. After 6 months, there are zero dashboards and the CEO still uses spreadsheets. |
| **Under-engineering at scale** | Platform cannot handle volume; queries time out; pipelines take 8 hours | A 50,000-worker EOR still runs all transformations in a single Snowflake XS warehouse. Monthly payroll aggregation takes 6 hours. Dashboards are stale by the time ops teams check them. |
| **Too many tools** | Integration complexity explodes; team must maintain expertise in 15+ tools | Each data engineer introduced their favorite tool. The stack now includes Airflow, Dagster, Prefect (3 orchestrators), plus Fivetran, Airbyte, and custom Python scripts (3 ingestion approaches). Nobody understands the full stack. |
| **Choosing a tool that does not support PII controls** | Cannot implement data masking or classification; governance blocked | Selected a BI tool that does not support row-level security or column-level masking. Now every analyst can see every worker's salary. Must either replace the tool or build a custom masking layer in front of it. |

### AI Opportunities

| Opportunity | Inputs | Outputs | Guardrails |
|------------|--------|---------|------------|
| **Infrastructure cost optimization** | Cloud spend data, query patterns, resource utilization metrics | Recommendations for right-sizing compute, storage tiering, reserved capacity | Recommendations reviewed by engineering lead before implementation; no auto-scaling changes to production |
| **Tool selection assistant** | Requirements document, team skills, budget constraints, current stack | Ranked tool recommendations with pros/cons, migration effort estimates, and TCO projections | Advisory only; final decision by architecture board |
| **Pipeline performance optimization** | Pipeline execution logs, query plans, resource utilization | Suggested optimizations (partitioning changes, caching strategies, query rewrites) with estimated performance improvement | Test in staging environment before production; no auto-apply |

### Discovery Questions

1. "What is our current data technology stack, and who chose each tool? Was there a formal evaluation process?"
2. "What is our monthly spend on data infrastructure (compute, storage, tooling)? How does it compare to our budget?"
3. "If you could replace one tool in the current stack, which would it be and why?"
4. "How much engineering time is spent maintaining existing infrastructure vs. building new analytical capabilities?"
5. "Are there tools in our stack that only one person knows how to operate? What is the bus factor?"

### Exercises

1. **Technology evaluation exercise:** You are evaluating two options for the semantic layer: dbt Semantic Layer vs. Cube.dev. Create a structured evaluation matrix with 10 criteria (payroll domain fit, PII support, cost, learning curve, integration with existing stack, etc.). Score each option 1-5 per criterion with justification. Make a recommendation.

2. **Stack migration planning:** The company is migrating from a Snowflake-only architecture to a Databricks lakehouse. Design the migration plan: which tables move first (prioritization rationale), how to maintain parallel operation during migration, rollback plan if issues arise, timeline (assume 3 engineers for 6 months), and success criteria.

3. **Analytics-leader exercise — Technology roadmap presentation:** Create a 10-slide presentation for the CTO titled "Data Platform Technology Roadmap: Next 18 Months." Include: current stack assessment (strengths and gaps), proposed target stack with justification, phased implementation plan, cost projections, hiring needs, and risk mitigation. Each slide should have a clear headline, supporting data, and a specific recommendation.

---

## Module Review

### Key Takeaways

1. **The medallion architecture (Bronze, Silver, Gold) is the backbone** of payroll data infrastructure. Bronze preserves raw audit trails, Silver enforces the canonical model and data quality, and Gold serves business-ready analytics. Each layer has a distinct purpose, and skipping layers creates gaps you pay for later.

2. **The canonical data model is not optional.** Without a single, agreed-upon schema for Worker, Contract, PayrollRun, Payslip, PayItem, Payment, StatutoryFiling, and the other core entities, cross-country analytics is impossible. The model must handle multi-country schema diversity (India's CTC components, Germany's church tax, UK's NI categories) through a flexible-but-structured approach.

3. **SCD Type 2 on key dimensions is a hard requirement.** Payroll operates in point-in-time reality: "What was the salary when this payroll was run?" is an auditable question. Without SCD2 on Worker, Contract, and related entities, you cannot answer it.

4. **The event spine is the connective tissue** for audit trails, process mining, real-time monitoring, and ML features. Every operational process (onboarding, payroll processing, payment, filing) should emit a complete sequence of events with standardized schemas.

5. **Data quality in payroll is not a nice-to-have — it is a production-critical capability.** A DQ failure in a gold table can mean wrong executive decisions, missed compliance deadlines, or incorrect client reports. The DQ framework must include automated rules, monitoring, alerting, quarantine, and remediation workflows.

6. **Multi-tenant isolation and data governance are non-negotiable.** Cross-tenant data leaks, PII exposure, and retention policy violations carry regulatory and reputational consequences that can threaten the company's existence. Build governance into the platform from Day 1, not as a retrofit.

7. **Technology choices should match your maturity stage.** A 500-worker startup does not need Kafka and Kubernetes. A 50,000-worker platform cannot survive on spreadsheets and cron jobs. Choose tools that serve you today with a clear migration path for tomorrow.

### Quiz — 10 Questions

**1.** What are the three layers of the medallion architecture, and what is the primary purpose of each? Give a specific payroll example for each layer.
   *Expected answer: The three layers are Bronze, Silver, and Gold. Bronze stores raw, immutable data exactly as received from source systems, serving as the audit trail and replay source — for example, a raw CSV export from the German DATEV payroll engine with original German field names like Personalnummer, Bruttolohn, and Kirchensteuer, appended with ingestion metadata. Silver is the cleaned, standardized layer where data is mapped to the canonical data model, deduplicated, SCD2 history is tracked for dimensions, and DQ rules are applied — for example, a unified worker record with canonical fields like worker_id, full_name, country_code (ISO 3166), annual_salary, salary_currency, annual_salary_usd, valid_from, and valid_to. Gold is the aggregated, business-ready layer optimized for specific analytics use cases and dashboards — for example, a monthly payroll summary table (gold.payroll_run_summary) containing pre-aggregated fields like country, period, total_gross_usd, headcount, error_count, on_time_pay_rate, and avg_processing_days, ready for executive dashboards without any further transformation.*

**2.** Why is SCD Type 2 essential for the Worker entity in a payroll data model? Describe a specific scenario where lack of SCD2 causes an audit failure.
   *Expected answer: SCD Type 2 is essential because payroll operates in point-in-time reality — every payroll calculation depends on the worker's attributes (salary, tax class, employment status) as they were on the specific date the payroll was run, not as they are today. SCD2 tracks history by creating new rows with valid_from and valid_to timestamps and an is_current flag, enabling queries like "What was worker W-042's salary on March 15, 2026?" Without SCD2, a salary change overwrites the historical value, and when an auditor asks for proof that the March payroll run used the correct salary, the silver table only shows the current (post-raise) salary with no audit trail. This is a material audit failure because regulators can request proof of payroll calculations months or years after the fact, and without temporal history the organization cannot demonstrate the calculation was correct. The canonical model applies SCD2 to both the Worker and Contract entities specifically for this reason.*

**3.** What is the difference between CDC and API polling as ingestion patterns? When would you use each, and what are the failure modes specific to each?
   *Expected answer: CDC (Change Data Capture) captures every insert, update, and delete from a source database's transaction log (WAL) in near real-time, with latency measured in seconds to minutes — tools like Debezium stream these changes via Kafka. You use CDC when you own or control the source database, such as the platform core EOR application database, where you need continuous, low-latency synchronization. CDC failure modes include the replication slot breaking during database failover, WAL log accumulation if the consumer falls behind, and missed deletes if the CDC configuration does not capture all operation types. API polling periodically calls a source system's REST API on a schedule to fetch new or changed records, with latency measured in minutes to hours depending on the polling interval. You use API polling when you do not own the source database, such as with external local payroll engines (e.g., the Indian payroll engine polled every 4 hours) or client HRIS systems like Workday. API polling failure modes include API rate limiting, authentication token expiry (e.g., SFTP credentials rotating without notification), pagination bugs that only fetch the first page of results causing missing records, and silent schema changes in the API response that break downstream mappings.*

**4.** Name the five dimensions of data quality and rank them in order of criticality for a payroll analytics platform. Justify your ranking.
   *Expected answer: The five dimensions are Completeness, Accuracy, Validity, Consistency, and Freshness. For a payroll platform, the recommended ranking is: (1) Accuracy — because payroll data directly determines whether people get paid the correct amount; an inaccurate salary or tax calculation results in wrong pay, which costs $50-$500 per incident to remediate. (2) Completeness — because missing records (e.g., 400 active workers dropped from ingestion) mean people do not get paid at all and headcount-based decisions are wrong. (3) Validity — because values violating business rules (net pay > gross pay, negative salary, invalid tax ID formats) indicate calculation errors that produce incorrect payslips. (4) Consistency — because the same fact must agree across tables (worker count in silver.worker must match payslip count for the period and payment count); inconsistency creates "two analysts, two numbers" situations that destroy trust. (5) Freshness — because while stale data causes ops teams to act on outdated information, it is the least likely to cause direct financial harm compared to data that is wrong, missing, invalid, or inconsistent. That said, freshness failures can be critical when filing deadlines are approaching and stale data means missed compliance deadlines.*

**5.** An event with type `payroll_run.variance_flagged` is emitted. What are the required fields in the event envelope? What would the payload contain for a German payroll run where 3 of 142 payslips have net pay variance exceeding 30%?
   *Expected answer: The required event envelope fields are: event_id (UUID, e.g., evt_7f3a2b1c-...), event_type ("payroll_run.variance_flagged"), event_version (e.g., "2.3"), timestamp (ISO 8601 with milliseconds), actor (containing actor_id, actor_type such as "system", and actor_name such as "Automated Variance Engine"), subject (containing entity_type "payroll_run" and entity_id such as "run_de_2026_03_001"), context (containing country_code "DE", entity_id of the legal entity, client_id, and environment), and metadata (containing source_system, correlation_id linking to the payroll run process, trace_id, schema_version, and partition_key "DE"). The payload for this scenario would contain: flagged_payslips: 3, total_payslips: 142, and a flag_reasons array with an entry for each flagged payslip including payslip_id, worker_id, reason ("net_pay_variance_exceeds_threshold"), variance_pct (e.g., -38.2), and a details string explaining the cause (e.g., "Tax class change from I to III detected"). The payload captures what was flagged and why, without embedding the full payslip data, keeping the event lightweight while enabling downstream investigation.*

**6.** You discover that the silver table `silver.worker` has 15,200 records, but the platform core database shows 15,400 active workers. What DQ rules should have caught this? Walk through the investigation process.
   *Expected answer: Two DQ rules should have caught this: DQ-001 (Completeness — every active worker should have a current SCD2 record, comparing silver active count against platform active count) and DQ-012 (Consistency — silver worker count must match the CDC source count, reconciled daily). Both are classified as Critical/High severity and should block gold refresh and alert ops. The investigation process begins by checking the ingestion pipeline run log for the platform core CDC pipeline — look for recent failures, row count drops, or increased latency. Next, compare the 200 missing workers: query the platform core for workers WHERE status='active' and LEFT JOIN against silver.worker to identify the specific worker_ids present in the source but absent in silver. Then determine the root cause by checking common patterns: Were these workers recently onboarded (within the CDC lag window)? Did a schema change break the CDC stream? Is there an API pagination bug only fetching the first page? Did a character encoding issue cause name matching to fail during canonical mapping? Check the quarantine table to see if 200 records were quarantined due to validation failures. Finally, once the root cause is identified, remediate by backfilling the missing records and adding or fixing the DQ rule to prevent recurrence.*

**7.** Describe the difference between logical isolation (RLS) and physical isolation (separate schemas) for multi-tenant data architecture. Which would you recommend for a global payroll platform with 500 clients, and why?
   *Expected answer: Logical isolation uses Row-Level Security (RLS) on shared tables — all clients' data lives in the same silver.worker table, and a client_id filter is automatically applied based on the querying user's role, so Client A's portal query only returns rows WHERE client_id = 'cli_acme'. Physical isolation uses separate database schemas per tenant (schema: cli_acme, schema: cli_beta), where each client has their own copy of every table and access is controlled by schema-level permissions rather than row filters. For 500 clients, the recommended approach is logical isolation (RLS) or a hybrid model, because physical isolation becomes unwieldy beyond 50-100 tenants — every schema change must be applied 500 times, maintenance cost is extremely high, and cross-tenant internal analytics requires UNION across 500 schemas. RLS on shared tables with partitioning by country_code and client_id provides both query performance (via partition pruning) and isolation, while keeping a single schema to maintain. The key risk of RLS is misconfiguration (a missing filter exposes all data), which must be mitigated through quarterly penetration testing, cross-tenant query alerting, and automated RLS policy coverage checks targeting 100% of silver/gold tables. For the most sensitive client-facing data access points, a hybrid approach can layer physical isolation on top for extra protection.*

**8.** What is a metric contract, and why is it necessary? Write a brief metric contract for "on-time pay rate" including formula, exclusions, dimensions, and thresholds.
   *Expected answer: A metric contract is a formal, versioned definition of a business metric that includes its exact formula, dimensions for slicing, inclusions and exclusions, thresholds, data source, owner, and refresh frequency. It is necessary because without it, two analysts will calculate the same metric differently — for example, Ops might include draft payroll runs while Finance excludes them, producing numbers that disagree by 3x in the same board meeting, destroying trust in the analytics function. A metric contract for on-time pay rate: Formula — COUNT(DISTINCT worker_id WHERE payment.settled_at <= payroll_run.pay_date) / COUNT(DISTINCT worker_id WHERE payroll_run.status IN ('paid','closed')); Exclusions — off-cycle correction runs, workers terminated before pay date, payments reversed and retried (count the retry settlement date, not the original); Dimensions — country_code, client_id, pay_frequency (monthly/semi-monthly/bi-weekly), entity_type (owned/partner); Thresholds — green: >= 99%, amber: 97%-99%, red: < 97%; Data source — gold.payment_ledger joined with gold.payroll_run_summary; Owner — Payroll Ops, approved by VP Payroll Ops + VP Analytics; Refresh — per payroll run completion.*

**9.** A German worker submits a GDPR right-to-erasure request. Describe what must happen in each layer of the data platform (bronze, silver, gold, event store) and the key challenge with immutable bronze tables.
   *Expected answer: First, validate the request by verifying the worker's identity and checking for exemptions — Germany requires 10-year retention of payroll and tax records, so payroll calculation data and tax filings are exempt and must be retained with the legal basis documented. For non-exempt PII (name, address, email, bank account, date of birth beyond what tax retention requires), the process by layer is: Bronze — this is the key challenge because bronze tables are append-only and immutable by design (they serve as the audit trail and replay source). You cannot delete rows, so the approach is to anonymize PII fields in place, replacing the worker's name with "ANONYMIZED", tax_id with a hash, and bank account with null, while preserving the non-PII structure for audit continuity. Silver — delete or anonymize the worker's records across silver.worker, silver.contract, silver.payslip, silver.payment, and all related tables, being careful to maintain referential integrity (foreign key references should point to a tombstone record or be nullified). Gold — re-aggregate all gold tables that included this worker's data so that pre-aggregated totals no longer carry any trace of individual PII; since gold tables are aggregated, this typically means re-running the aggregation pipeline. Event store — anonymize PII within event payloads (the JSON payload field) for all events where this worker appears as a subject, while preserving the event structure for process integrity. Additionally, mark backups for exclusion from future restores containing this worker's PII. The entire process must complete within 30 days per GDPR Article 17, and a verification PII scan must confirm no residual PII remains.*

**10.** You are building the data platform for an EOR with 5,000 workers across 20 countries. The engineering team has 5 data engineers. Recommend a technology stack (ingestion, storage, compute, transformation, DQ, BI) with justification for each choice. What would change if the worker count was 50,000?
   *Expected answer: For 5,000 workers (growth stage): Ingestion — Fivetran for managed connectors handling 80% of sources out of the box, plus Debezium (open-source) for CDC on the platform core database, because a 5-engineer team cannot afford to build and maintain custom connectors. Storage — S3/GCS with Delta Lake table format via Databricks, providing immutable bronze storage with ACID transactions, time travel, and schema enforcement for silver/gold layers. Compute — Databricks as the unified analytics platform supporting Spark, SQL, and ML with Unity Catalog for built-in governance and PII controls. Transformation — dbt Cloud for SQL-based transformations with built-in testing, documentation, and a growing semantic layer, allowing analytics engineers to own transformations without Spark expertise. DQ — Great Expectations for complex DQ rules (the 15+ rules across five dimensions) combined with dbt tests for schema-level assertions, both open-source and complementary. BI — Preset (managed Apache Superset) for cost-effective internal self-service analytics plus Tableau for polished executive dashboards. Estimated total cost: $30-70K/month. At 50,000 workers, the key changes would be: ingestion adds Kafka (Confluent Cloud) with Schema Registry for full real-time CDC and event streaming; storage adds region-specific deployments for data residency compliance (e.g., EU region for German worker data per GDPR); compute scales to Spark on Kubernetes plus Snowflake for serving, separating heavy ETL from interactive queries; DQ adds Monte Carlo or Soda for data observability alongside Great Expectations; BI adds embedded analytics for the client portal; and a dedicated data catalog (DataHub or Atlan) with a custom data product catalog becomes essential for managing 50+ data products with SLAs.*

### First 90 Days

**Days 1-30: Assess and Understand**
- [ ] Map all source systems feeding (or should feed) the analytics platform — document: system name, data type, current ingestion method, freshness, known issues
- [ ] Audit the current data model — is there a canonical model? How many "sources of truth" exist for worker count?
- [ ] Assess data quality baseline — pick the top 5 countries by worker count and manually verify: headcount accuracy, payroll cost accuracy, payment success rate accuracy
- [ ] Interview key stakeholders (VP Ops, VP Finance, VP Compliance, VP Product) using the interview questions from each topic
- [ ] Document the current technology stack — every tool, who owns it, what it costs, where the pain points are
- [ ] Shadow the payroll ops team for 3-5 days to understand what data they need and when they need it
- [ ] Identify the top 3 "data trust" incidents from the past 6 months — times when a dashboard showed wrong data or a report was delayed

**Days 31-60: Build Foundations**
- [ ] Implement the canonical data model for the 5 core entities: Worker (SCD2), Contract (SCD2), PayrollRun, Payslip, PayItem
- [ ] Set up the bronze-silver-gold pipeline for the top 3 source systems (platform core DB, top payroll engine, payment system)
- [ ] Define and implement 15-20 automated DQ rules (at least 3 per core entity)
- [ ] Build the data quality dashboard showing: overall DQ score, score by dimension, score by country, active incidents
- [ ] Define metric contracts for the top 10 KPIs (payslip error rate, on-time pay rate, filing on-time rate, cost per payslip, revenue per worker, headcount, etc.)
- [ ] Design the event taxonomy (at least the payroll_run.* and payment.* domains) and propose it to the engineering team
- [ ] Stand up the first version of the semantic layer (dbt metrics or equivalent) with 5 governed metrics

**Days 61-90: Operationalize and Deliver**
- [ ] Launch the Payroll Operations Weekly Dashboard (8 metrics, drill-down by country/client/run)
- [ ] Implement freshness SLAs for all ingestion pipelines with automated alerting
- [ ] Complete PII classification for all silver/gold tables — tag every column, implement masking for non-authorized roles
- [ ] Implement RLS on all client-facing data assets (silver/gold tables accessed by client portal or client-facing reports)
- [ ] Publish the first 3 data products in an internal catalog with: description, schema, DQ score, refresh SLA, owner
- [ ] Present the "State of the Data Platform" to the leadership team: current maturity, gaps identified, 6-month roadmap, resource ask
- [ ] Establish the weekly data quality review cadence with data engineering and domain leads

---

## How This Module Makes You Valuable as an Analytics Leader

This module maps directly to the three pillars of the analytics leader evolution path:

**BI + Lakehouse Leader:** The medallion architecture, canonical data model, technology stack, and data quality framework are the bread and butter of this pillar. You are not learning new concepts here — you are learning how to apply your existing lakehouse expertise to the specific challenges of payroll data (multi-country schema diversity, PII density, temporal precision requirements, regulatory audit trails). Your ability to design and build this platform is the table-stakes qualification for the role.

**Operational Intelligence Architect:** The event spine, process mining capabilities, semantic layer, and metric contracts transform raw data into operational intelligence. You are not just building a data warehouse — you are building the nervous system that tells the organization where payroll operations are working well, where they are failing, and where they are about to fail. The 100+ metrics defined across the 11 topics in this module give you a comprehensive measurement framework that most payroll companies lack.

**AI-Augmented Compliance and Payroll Systems Leader:** The AI opportunities cataloged across every topic — automated schema mapping, anomaly detection on event patterns, predictive process mining, intelligent DQ rules, PII auto-detection, natural language query interfaces — are the specific, practical applications where AI adds value in this domain. You now know the inputs, outputs, and guardrails for each. You can build the business case, scope the project, and define the success criteria — which is exactly what an analytics leader does.

**In interviews and in the role, this module enables you to:**
- Design the data platform architecture from scratch and present it to the CTO
- Define and enforce data quality standards that the ops team trusts
- Build metric contracts that eliminate the "two analysts, two numbers" problem
- Navigate the GDPR/PII/data residency landscape with confidence
- Evaluate and select the right technology stack for the company's stage
- Present a credible 90-day and 18-month roadmap on Day 1

---

## Glossary

| Term | Definition |
|------|-----------|
| **Medallion Architecture** | Three-layer data architecture (Bronze, Silver, Gold) that separates raw data, cleaned/standardized data, and business-ready analytics data |
| **Bronze Layer** | The raw, immutable data layer where source data is stored exactly as received, serving as the audit trail and replay source |
| **Silver Layer** | The cleaned, standardized layer where data is mapped to the canonical model, deduplicated, and quality-checked |
| **Gold Layer** | The aggregated, business-ready layer optimized for specific analytics use cases, dashboards, and ML features |
| **Canonical Data Model** | A single, agreed-upon schema for representing core business entities across all source systems and countries |
| **SCD Type 2 (Slowly Changing Dimension)** | A dimension modeling technique that tracks history by creating new rows with valid_from/valid_to dates, enabling point-in-time queries |
| **Event Spine** | The chronological stream of all business events across the platform, forming the foundation for audit trails, process mining, and real-time monitoring |
| **Event Sourcing** | A pattern where state changes are stored as a sequence of events, enabling state reconstruction by replaying events |
| **CDC (Change Data Capture)** | An ingestion pattern that captures every insert, update, and delete from a source database's transaction log in near real-time |
| **API Polling** | An ingestion pattern that periodically calls a source system's API to fetch new or changed records |
| **Schema-on-Read** | Validating data structure at read time rather than write time; common for bronze layer where raw data is preserved as-is |
| **Schema-on-Write** | Enforcing data structure at write time; applied when promoting data from bronze to silver |
| **Data Quality (DQ)** | The practice of measuring, monitoring, and enforcing the trustworthiness of data across dimensions: completeness, freshness, validity, consistency, accuracy |
| **Quarantine** | Isolating records that fail data quality checks in a separate table for investigation and remediation, preventing bad data from reaching consumers |
| **Row-Level Security (RLS)** | A database access control mechanism that restricts which rows a user can see based on their role or tenant membership |
| **Multi-Tenant Architecture** | A data architecture that serves multiple clients (tenants) from shared infrastructure while maintaining strict data isolation |
| **Data Residency** | The regulatory requirement that personal data of citizens in certain countries must be stored within specific geographic regions |
| **Right to Erasure** | A GDPR right (Article 17) allowing data subjects to request deletion of their personal data, subject to certain exemptions |
| **PII (Personally Identifiable Information)** | Data that can identify a specific individual, such as name, tax ID, bank account, salary, date of birth |
| **Dynamic Data Masking** | Automatically hiding or obfuscating sensitive data based on the querying user's role, without modifying the underlying data |
| **Golden Record** | The single, authoritative version of an entity (especially a worker) assembled from multiple source systems through entity resolution |
| **Entity Resolution** | The process of determining whether records from different systems refer to the same real-world entity, using deterministic or probabilistic matching |
| **Semantic Layer** | An abstraction layer between raw data and consumers that provides business-friendly definitions of metrics, dimensions, and relationships |
| **Metric Contract** | A formal, versioned definition of a business metric including: formula, dimensions, inclusions/exclusions, thresholds, owner, and data source |
| **Data Product** | A curated, documented, quality-assured dataset with defined SLAs, ownership, and consumer documentation — not just a raw table |
| **Dead Letter Queue (DLQ)** | A holding area for events or records that failed processing, awaiting investigation and remediation |
| **Idempotency** | The property that processing the same event or record multiple times produces the same result as processing it once — critical for data pipeline reliability |
| **Partition Pruning** | A query optimization technique where the query engine skips partitions that cannot contain relevant data, improving performance |
| **Delta Lake / Iceberg** | Open-source table formats that add ACID transactions, schema enforcement, and time travel to data lake storage (S3/GCS) |
| **Feature Store** | A centralized repository for ML features that ensures consistent feature computation across training and serving |
| **ROPA (Record of Processing Activities)** | A GDPR Article 30 requirement to maintain documentation of all personal data processing activities, including purposes, categories, and retention |
| **Correlation ID** | A unique identifier that links related events across different systems and domains, enabling end-to-end traceability |
| **Process Mining** | The practice of analyzing event logs to discover, monitor, and improve business processes — identifying bottlenecks, deviations, and optimization opportunities |
| **dbt (data build tool)** | A SQL-based transformation framework that enables analytics engineers to define, test, and document data transformations using version-controlled models |
| **Great Expectations** | An open-source data quality framework that enables defining, running, and monitoring data validation rules (expectations) against datasets |
| **Debezium** | An open-source CDC platform that captures row-level changes from database transaction logs and streams them as events |

---

## CSV Study Plan Tie-In — Week 7: April 13-19, 2026

| Date | Day | Study Plan Output | Domain Connection |
|------|-----|-------------------|-------------------|
| Apr 13 | Monday | **Canonical data model implementation** | Implement the Worker, Contract, PayrollRun, Payslip, and PayItem schemas from Topic 2 in your lakehouse (Databricks or equivalent). Create tables with SCD Type 2 on Worker and Contract. Load synthetic data for 100 workers across 5 countries (IN, DE, UK, BR, SG). Include salary changes for 10 workers to test SCD2. Write the query: "What was worker W-042's salary on March 15, 2026?" |
| Apr 14 | Tuesday | **Event spine and taxonomy** | Build the event store table from Topic 3. Implement the full payroll_run.* event taxonomy. Emit synthetic events for 3 payroll runs (DE, IN, UK) with realistic timestamps at each stage. Write a process mining query that calculates cycle time from payroll_run.created to payroll_run.paid by country. Identify the slowest step. |
| Apr 15 | Wednesday | **Data quality framework** | Implement 15 DQ rules from Topic 5 using dbt tests or Great Expectations. Cover all 5 dimensions (completeness, freshness, validity, consistency, accuracy). Build the DQ observability dashboard showing: overall score, score by dimension, score by country, active incidents. Intentionally introduce 3 data quality issues and verify they are caught. |
| Apr 16 | Thursday | **Metric contracts and semantic layer** | Write formal metric contracts for 5 core metrics (payslip error rate, on-time pay rate, filing on-time rate, cost per payslip, headcount). Implement a basic semantic layer using dbt metrics that enforces these definitions. Build a single dashboard that shows all 5 metrics with country-level drill-down. Verify that two different queries using the semantic layer produce identical results. |
| Apr 17 | Friday | **Multi-tenant isolation and governance** | Implement RLS on the silver.worker and silver.payslip tables to simulate client isolation. Create 3 test roles (client_a_user, client_b_user, internal_analyst). Write 10 test queries to validate that RLS correctly blocks cross-tenant access. Classify all columns in your 5 tables by PII level. Implement dynamic masking on tax_id and salary for an "external_auditor" role. |
| Apr 18 | Saturday | **Week 7 integration demo** | Demo the complete data platform: medallion architecture with bronze/silver/gold tables, canonical data model with SCD2, event spine with process mining, DQ monitoring dashboard, metric layer with governed definitions, and multi-tenant isolation. Key demo query: "Show me the payroll summary for Germany in March, with DQ score, cycle time from events, and confirm that Client A cannot see Client B's data." |
| Apr 19 | Sunday | **Retrospective and planning** | Review: Does your data platform support the analytics use cases from Modules 1-6? Can you answer the questions the VP of Ops, VP of Finance, and VP of Compliance would ask using your platform? Identify gaps. Write the "State of the Data Platform" one-pager from the First 90 Days checklist. Lock Week 8 scope: Applied ML and LangGraph for Operational Intelligence (Module 9). |

---

*Module 7 complete. Continue to Module 8: Master Data Management.*
