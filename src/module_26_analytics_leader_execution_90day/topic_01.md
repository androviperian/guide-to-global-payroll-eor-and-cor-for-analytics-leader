# Topic 1: The First 90 Days — Detailed Week-by-Week Breakdown

## What It Is

Your first 90 days as an analytics leader define how you are perceived for the next two years. This is not an exaggeration — research on executive transitions consistently shows that the impressions formed in the first three months become deeply entrenched and resistant to change. A structured, deliberate approach to your first 90 days is not optional; it is the single highest-leverage investment you will make in this role.

The 90-day plan is divided into four phases: **Listen (Weeks 1-2)**, **Map (Weeks 3-4)**, **Quick Wins (Weeks 5-8)**, and **Strategic Roadmap (Weeks 9-12)**. Each phase has specific objectives, deliverables, meetings, and outputs.

## Why It Matters

- **Credibility is earned in the first 90 days, not assumed from the job title.** You were hired for your potential; you must prove your judgment before people trust your direction.
- **The cost of moving too fast is higher than moving too slow.** Launching a major initiative in week 3 without understanding the political landscape can create enemies who block you for months.
- **Quick wins in weeks 5-8 create the political capital for strategic bets in months 4-12.** Without early demonstrated value, your roadmap will be seen as academic.
- **Global payroll is a "trust business."** People's livelihoods depend on accuracy. You cannot experiment carelessly.

## Process Flow

```
WEEK 1-2: LISTEN                  WEEK 3-4: MAP
┌──────────────────────┐          ┌──────────────────────┐
│ • Meet every leader  │          │ • System landscape   │
│ • Shadow ops team    │          │   audit              │
│ • Observe payroll    │          │ • Data quality        │
│   runs               │────────►│   assessment          │
│ • Collect pain       │          │ • Current-state       │
│   points             │          │   document            │
│ • Understand culture │          │ • Identify quick wins │
└──────────────────────┘          └──────────┬───────────┘
                                             │
                                             ▼
WEEK 9-12: STRATEGIC ROADMAP     WEEK 5-8: QUICK WINS
┌──────────────────────┐          ┌──────────────────────┐
│ • Publish OKRs       │          │ • Executive dashboard │
│ • Present roadmap to │          │ • One data fix        │
│   leadership         │◄────────│ • One automation      │
│ • Launch first AI    │          │ • Data quality        │
│   pilot (shadow)     │          │   baseline            │
│ • Hiring plan live   │          │ • Win visible allies  │
└──────────────────────┘          └──────────────────────┘
```

## Detailed Week-by-Week Plan

**Phase 1: Listen (Weeks 1-2)**

| Week | Day | Activity | Deliverable | Notes |
|------|-----|----------|-------------|-------|
| **W1** | Mon | HR orientation, IT setup, meet your VP/manager | Access to systems, Slack channels, org chart | Ask your manager: "Who are the 3 people whose support I need most?" |
| **W1** | Tue | 1:1 with VP Operations | Notes: ops pain points, current metrics, what they wish they had | This is your most important stakeholder — spend 60 min |
| **W1** | Wed | 1:1 with VP Finance | Notes: financial reporting gaps, billing accuracy concerns, audit readiness | Ask: "What question can you not answer today that you wish you could?" |
| **W1** | Thu | 1:1 with VP Engineering | Notes: data infrastructure, API availability, engineering priorities | Ask: "If I need data from your systems, what is the easiest path?" |
| **W1** | Fri | 1:1 with VP Compliance / Legal | Notes: compliance monitoring gaps, regulatory change tracking | Ask: "What keeps you up at night about compliance exposure?" |
| **W2** | Mon | Shadow the payroll operations team for a full day | Observation notes: manual steps, tool switching, workarounds, pain points | Do NOT suggest improvements yet — just observe and ask questions |
| **W2** | Tue | Observe a live payroll run (ideally for a complex country like India or Brazil) | Process flow notes, data handoff points, error-prone steps | Ask the payroll specialist: "What goes wrong most often?" |
| **W2** | Wed | Review existing dashboards, reports, and analytics artifacts | Inventory: what exists, who uses it, what is missing, what is broken | Most companies have 20+ reports that nobody looks at |
| **W2** | Thu | 1:1 with each direct report (if you inherit a team) | Individual skills assessment, their frustrations, their aspirations | Ask: "What should I know that nobody will tell me in a meeting?" |
| **W2** | Fri | Synthesize week 1-2 findings | Draft: "First Impressions" document (internal, for your eyes only) | Do NOT share this broadly yet — it is your working hypothesis |

**Phase 2: Map (Weeks 3-4)**

| Week | Day | Activity | Deliverable | Notes |
|------|-----|----------|-------------|-------|
| **W3** | Mon-Tue | System landscape audit: map every data system, who owns it, what data flows where | System landscape diagram (see Topic 7 for template) | This is the single most important artifact you will create in month 1 |
| **W3** | Wed-Thu | Data quality spot check: pick 3 countries, pull payroll data, validate against source | Data quality scorecard (5 dimensions: completeness, accuracy, timeliness, consistency, validity) | Focus on the metrics leadership already tracks — are they accurate? |
| **W3** | Fri | Meet with 2-3 client success managers | Client pain points related to data, reporting, visibility | Clients often articulate needs that internal teams have normalized away |
| **W4** | Mon-Tue | Identify 3 quick-win opportunities based on all findings | Quick Win Proposal document (see template below) | Each quick win must be: deliverable in 2-3 weeks, visible to leadership, low risk |
| **W4** | Wed | Draft Current State Assessment (see template below) | Current State Assessment document | This is your first major deliverable — it must be thoughtful and balanced |
| **W4** | Thu | Present Current State Assessment to your VP/manager | Feedback, alignment, permission to proceed with quick wins | Frame as: "Here is what I have found, here is what I recommend, here is what I need" |
| **W4** | Fri | Revise assessment based on feedback; plan quick win execution | Finalized quick win plan with owners, timelines, success criteria | Get explicit permission before starting |

**Phase 3: Quick Wins (Weeks 5-8)**

| Week | Activity | Deliverable | Success Metric |
|------|----------|-------------|----------------|
| **W5** | Build executive KPI dashboard v1 (top 8 metrics, 3-5 countries) | Live dashboard accessible to VP Ops, VP Finance, your VP | Dashboard is functional; at least 3 leaders have seen it |
| **W6** | Deliver data quality baseline report for top 5 countries | Report showing quality scores by dimension, by country, with trends | Report presented at ops review meeting; specific data fixes identified |
| **W7** | Fix one specific, recurring operational problem (e.g., automate a manual report, fix a broken reconciliation, implement a missing validation) | Working solution deployed to production | Ops team confirms the fix works; quantify time saved |
| **W8** | Consolidate quick win results; prepare Phase 4 proposal | Quick Win Impact Summary + Strategic Roadmap Proposal (draft) | Your VP agrees you have earned the right to propose strategic initiatives |

**Phase 4: Strategic Roadmap (Weeks 9-12)**

| Week | Activity | Deliverable | Success Metric |
|------|----------|-------------|----------------|
| **W9** | Draft OKRs for next quarter (see Topic 4) | OKR document aligned with company priorities | VP provides feedback and endorses |
| **W10** | Design analytics roadmap for months 4-12 (see Topic 7) | Roadmap document with prioritized initiatives, resource requirements, expected impact | Roadmap reviewed by VP Ops, VP Finance, VP Engineering |
| **W11** | Launch first AI pilot in shadow mode (e.g., payroll risk scoring for 2 countries) | Pilot running, generating scores, not yet visible to ops team | Model producing outputs; data being collected for validation |
| **W12** | Present 90-day retrospective + strategic plan to leadership team | 90-Day Report: what you found, what you delivered, what you propose, what you need | Leadership approves roadmap and hiring plan; you have a mandate |

## Current State Assessment Template

```
CURRENT STATE ASSESSMENT — BUSINESS ANALYTICS
Prepared by: [Your Name], Analytics Leader, Business Analytics
Date: [Date]
Confidential — Internal Use Only

1. EXECUTIVE SUMMARY
   Three sentences: what you found, what you recommend, what the stakes are.

2. OPERATIONAL HEALTH (based on observation and data review)
   ┌───────────────────────────┬──────────────┬──────────┬───────────┐
   │ Metric                    │ Current      │ Target   │ Gap       │
   ├───────────────────────────┼──────────────┼──────────┼───────────┤
   │ Payslip accuracy rate     │ 99.82%       │ >99.95%  │ -0.13pp   │
   │ On-time pay rate          │ 99.1%        │ >99.5%   │ -0.4pp    │
   │ Filing on-time rate       │ 97.5%        │ 100%     │ -2.5pp    │
   │ Error correction turnaround│ 3.2 days    │ <1 day   │ +2.2 days │
   │ Client-reported issues/mo │ 45           │ <10      │ +35       │
   └───────────────────────────┴──────────────┴──────────┴───────────┘

3. DATA AND ANALYTICS MATURITY
   Maturity level: [1-Reactive / 2-Descriptive / 3-Predictive / 4-Prescriptive]
   Current: Level 1.5 — Some dashboards exist but are manual, incomplete,
   and not trusted. No predictive capabilities.

   Data quality assessment (spot check, 3 countries):
   ┌───────────────┬──────────┬──────────┬──────────┐
   │ Dimension     │ India    │ UK       │ Germany  │
   ├───────────────┼──────────┼──────────┼──────────┤
   │ Completeness  │ 87%      │ 94%      │ 91%      │
   │ Accuracy      │ 91%      │ 96%      │ 93%      │
   │ Timeliness    │ 78%      │ 89%      │ 85%      │
   │ Consistency   │ 72%      │ 88%      │ 80%      │
   │ Validity      │ 85%      │ 93%      │ 90%      │
   └───────────────┴──────────┴──────────┴──────────┘

4. TEAM AND CAPABILITIES
   Current team: [size, composition, skills, gaps]
   Capacity: [what they can deliver vs what is needed]

5. QUICK WIN OPPORTUNITIES (3 specific, achievable in 30 days)
   a. [Description, expected impact, resources needed]
   b. [Description, expected impact, resources needed]
   c. [Description, expected impact, resources needed]

6. STRATEGIC THEMES FOR MONTHS 2-6
   a. [Theme: e.g., "Build operational visibility through real-time dashboards"]
   b. [Theme: e.g., "Establish data quality foundation for AI readiness"]
   c. [Theme: e.g., "Deploy predictive risk scoring for payroll error prevention"]

7. RESOURCE REQUIREMENTS
   [What you need: headcount, tools, budget, executive sponsorship]
```

## Data Artifacts

| Entity | Key Fields | Analytics Enabled |
|--------|-----------|-------------------|
| 90-Day Plan tracker | week_number, phase, activity, deliverable, status, actual_completion_date, blockers | Progress tracking against plan, phase completion rate, blocker analysis |
| Stakeholder meeting log | stakeholder_name, role, date, topics_discussed, action_items, follow_up_date | Relationship coverage heatmap, action item completion rate |
| Current state assessment | metric_name, current_value, target_value, gap, country, assessment_date | Gap analysis trending, improvement tracking over time |
| Quick win register | initiative_name, description, owner, start_date, target_date, actual_date, impact_type, impact_value | Quick win delivery rate, time-to-value, cumulative impact |
| Observation notes | date, context, observation, implication, source_person, category | Pattern identification across observations, theme clustering |

## Controls

| Control | Description | Frequency | Owner |
|---------|-------------|-----------|-------|
| 90-day checkpoint reviews | Formal review with VP at day 30, 60, 90 against plan | 3 times total | You + your VP |
| Quick win gating | Each quick win must have explicit VP approval before launch | Per initiative | You + your VP |
| Stakeholder coverage check | Verify all key stakeholders have been met at least once by day 14 | Weekly (W1-2) | You |
| Data quality validation | Any metric you publish must be validated against source before first use | Per metric | You + analytics team |
| Assessment peer review | Current state assessment reviewed by at least one peer before presenting to VP | Once | You + trusted peer |

## Metrics

| Metric | Definition | Target | Frequency | Owner |
|--------|-----------|--------|-----------|-------|
| Stakeholder meetings completed | Count of unique stakeholders met in first 14 days | >= 10 | Daily (W1-2) | You |
| Systems documented | Count of data systems mapped with data flows | 100% of production systems | End of W3 | You |
| Data quality dimensions assessed | Countries x dimensions assessed vs target | >= 3 countries x 5 dimensions | End of W3 | You |
| Quick wins identified | Number of validated quick-win opportunities | >= 3 | End of W4 | You |
| Quick wins delivered | Quick wins completed on time with measurable impact | 3/3 | End of W8 | You |
| Dashboard adoption | Number of leaders accessing the executive dashboard weekly | >= 5 | Weekly (W6+) | You |
| Current state assessment delivered | Assessment document presented to VP on schedule | Day 28 | Once | You |
| OKR document published | OKRs aligned with company priorities and endorsed by VP | End of W9 | Once | You |
| Roadmap approved | Strategic roadmap reviewed and approved by leadership | End of W12 | Once | You |
| AI pilot launched | First AI pilot running in shadow mode | End of W11 | Once | You + ML engineer |
| 90-day report delivered | Comprehensive report presented to leadership team | Day 90 | Once | You |
| Team satisfaction baseline | Anonymous survey of direct reports on leadership, clarity, support | Baseline captured | End of W12 | You |

## Common Failure Modes

| Failure Mode | Consequence | Real-World Example | Prevention |
|-------------|-------------|-------------------|------------|
| Moving too fast in weeks 1-2 | Ops team feels judged; they become defensive and withhold information | "The new analytics director spent 2 days and already told us our processes are broken" | Listen mode only for 2 weeks; ask questions, do not prescribe |
| Skipping stakeholder meetings | You build something nobody asked for; you miss political dynamics | Building a compliance dashboard when VP Compliance already has a system they trust | Mandatory meetings with every VP-level stakeholder in first 10 days |
| Over-promising in the assessment | You set expectations you cannot meet; credibility erodes when you miss | "I will reduce errors by 50% in 90 days" (realistic target is 15-20% in 6 months) | Under-promise, over-deliver; use ranges, not point estimates |
| Ignoring inherited team dynamics | Team members feel threatened; they sabotage or disengage | Not meeting 1:1 with the senior analyst who was passed over for your role | 1:1 with every team member in week 2; acknowledge their expertise |
| Choosing wrong quick wins | Quick win fails or is invisible to leadership; you lose credibility | Fixing a backend data pipeline that nobody sees vs. fixing the broken weekly report the CEO reads | Quick wins must be visible AND achievable; consult your VP on selection |
| Not documenting findings | You forget what you learned; you cannot reference it in your assessment | Three weeks of meetings with no notes; assessment is vague | Structured notes after every meeting; template for observations |

## AI Opportunities

| AI Application | Inputs | Outputs | Guardrails |
|---------------|--------|---------|------------|
| Automated stakeholder briefing generator | Meeting notes, stakeholder profiles, prior communications | Draft talking points customized per stakeholder before each meeting | Human reviews all outputs; AI does not send communications directly |
| Current state assessment accelerator | System metadata, existing reports, data quality scan results | Draft assessment sections with data-backed findings | All findings validated manually before inclusion in assessment |
| Quick win identification | Observation notes, ops team pain points, data quality scores | Ranked list of potential quick wins with estimated effort and impact | Human selects from ranked list; AI does not unilaterally choose |
| 90-day plan adaptation | Plan template, company-specific context, stakeholder feedback | Adjusted plan with company-specific milestones and dependencies | Original framework maintained; AI adjusts details, not structure |

## Discovery Questions

1. "What does the analytics function do well today, and where does it fall short?" (Assesses current capability perception)
2. "If you could have one report or dashboard that you do not have today, what would it show?" (Identifies highest-value unmet need)
3. "What was the last major decision that was made without adequate data? What happened?" (Reveals data gaps with real consequences)
4. "Who in the organization is most skeptical about investing in analytics? What drives their skepticism?" (Maps political resistance)
5. "What would success look like for me in this role, from your perspective, after 6 months?" (Aligns expectations early)

## Exercises

1. **Design your first-week calendar.** Create an hour-by-hour schedule for your first 5 business days. For each meeting, specify: who you are meeting, what you want to learn, what questions you will ask, and what you will NOT say (boundaries on premature opinions).
2. **Write a Current State Assessment for a hypothetical EOR company.** Based on everything you have learned in Modules 1-9, populate each section of the template with realistic findings. Make the assessment balanced — acknowledge strengths as well as gaps.
3. **Identify 3 quick wins.** For each, specify: the problem, the proposed solution, the expected impact (quantified), the resources required, the timeline, and how you will make the impact visible to leadership.
4. **Write your 90-day retrospective.** Pretend it is day 90. Write the report you would present to leadership, including: what you found, what you delivered, what the measurable impact was, what you propose next, and what you need.
